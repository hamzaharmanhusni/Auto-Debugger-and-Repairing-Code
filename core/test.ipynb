{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Annotated, Any, Dict, Optional, Sequence, TypedDict, List, Tuple\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test_file'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = 'test_repo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PythonLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader('test_repo', glob=\"**/*.py\", loader_cls=PythonLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from pydantic_settings import BaseSettings\\n\\n\\nclass GunicornConfig(BaseSettings):\\n    NUM_WORKERS: int = 1\\n    NUM_THREADS: int = 1\\n    TIMEOUT: int = 30\\n    LOG_LEVEL: str = \"INFO\"\\n    LOG_DEBUG: str = \"DEBUG\"\\n    LOG_ERROR: str = \"ERROR\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_API_GUNICORN_\"\\n\\n\\nGUNICORN_CONFIG = GunicornConfig()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\config.py'}),\n",
       " Document(page_content='import logging\\n\\n\\nclass RoguLogFilter(logging.Filter):\\n    \"\"\"\\n    Custom filter to exclude health check\\n    \"\"\"\\n\\n    def filter(self, record):\\n        try:\\n            uri = str(record.args[2])\\n        except Exception as e:\\n            uri = None\\n        return uri != \"/_health\"\\n\\n\\nclass RoguFormatter(logging.Formatter):\\n    \"\"\"\\n    RG Logger Formatter\\n    \"\"\"\\n\\n    datefmt = \"%Y-%m-%dT%H:%M:%SZ\"\\n    fmt = \\'RG_LOGGER: time=\"{asctime}\" name={name} lineno={lineno} level={levelname} msg=\"{msg}\" {extra}\\'\\n\\n    def format(self, record):\\n\\n        extra = []\\n        if record.exc_info:\\n            extra_str = f\\'exc_info=\"{self.formatException(record.exc_info)}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_body\"):\\n            extra_str = f\\'request_body=\"{record.request_body}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_path\"):\\n            extra_str = f\"request_path={record.request_path}\"\\n            extra.append(extra_str)\\n\\n        extra_str = \" \".join(extra)\\n        message = self.__class__.fmt.format(\\n            asctime=self.formatTime(record, self.__class__.datefmt),\\n            name=record.name,\\n            lineno=record.lineno,\\n            levelname=record.levelname.lower(),\\n            msg=self._one_liner(record.getMessage()),\\n            extra=self._one_liner(extra_str)\\n        )\\n\\n        return message\\n\\n    def formatException(self, exc_info):\\n        result = super().formatException(exc_info)\\n        return self._one_liner(result)\\n\\n    def _one_liner(self, msg):\\n        return msg.replace(\"\\\\n\", \"\\\\t|\\\\t\")\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\custom_logging.py'}),\n",
       " Document(page_content='import threading\\nimport sys\\nimport traceback\\nfrom logging.config import dictConfig\\nfrom config import GUNICORN_CONFIG\\n\\n\\nworkers = GUNICORN_CONFIG.NUM_WORKERS\\nthreads = GUNICORN_CONFIG.NUM_THREADS\\ntimeout = GUNICORN_CONFIG.TIMEOUT\\nworker_class = \"uvicorn.workers.UvicornWorker\"\\nbind = \\'0.0.0.0:80\\'\\n\\n\\nlogconfig_dict = {\\n        \"version\": 1,\\n        \"disable_existing_loggers\": True,\\n        \"formatters\": {\\n                \"custom\": {\\n                    \"()\": \"custom_logging.RoguFormatter\"\\n                }\\n        },\\n        \"filters\": {\\n            \"exclude_health_check\": {\\n                \"()\": \"custom_logging.RoguLogFilter\"\\n            }\\n        },\\n        \"handlers\": {\\n            \"access_console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\",\\n                \"filters\": [\"exclude_health_check\"]\\n            },\\n            \"console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\"\\n            },\\n        },\\n        \"loggers\": {\\n            \"\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"elasticsearch\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"openai\": {\\n                \"level\": GUNICORN_CONFIG.LOG_DEBUG,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"llm\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"gunicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.access\"\\n            },\\n            \"gunicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.error\"\\n            },\\n            \"uvicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"uvicorn.access\"\\n            },\\n            \"uvicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"uvicorn.error\"\\n            }\\n        }\\n    }\\ndictConfig(logconfig_dict)\\n\\n\\ndef post_fork(server, worker):\\n    server.log.info(\"Worker spawned (pid: %s)\", worker.pid)\\n\\n\\ndef pre_fork(server, worker):\\n    pass\\n\\n\\ndef post_worker_init(worker):\\n    pass\\n\\n\\ndef pre_exec(server):\\n    server.log.info(\"Forked child, re-executing.\")\\n\\n\\ndef worker_int(worker):\\n    worker.log.info(\"worker received INT or QUIT signal\")\\n\\n    # get traceback info\\n    id2name = dict([(th.ident, th.name) for th in threading.enumerate()])\\n    code = []\\n    for threadId, stack in sys._current_frames().items():\\n        code.append(\"\\\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\\n        for filename, lineno, name, line in traceback.extract_stack(stack):\\n            code.append(\\'File: \"%s\", line %d, in %s\\' % (filename, lineno, name))\\n            if line:\\n                code.append(\"  %s\" % (line.strip()))\\n    worker.log.debug(\"\\\\n\".join(code))\\n\\n\\ndef worker_abort(worker):\\n    worker.log.info(\"worker received SIGABRT signal\")\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py'}),\n",
       " Document(page_content='import openai\\nimport warnings\\nfrom pydantic_settings import BaseSettings\\nimport os\\n\\n\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n\\nclass ChatCompletionConfig(BaseSettings):\\n    ACTIVE_MODELS: str = \"dummy;openai;vertexai\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_CHAT_COMPLETION_CONFIG_\"\\n\\n\\nclass OpenAIConfig(BaseSettings):\\n    DEFAULT_ORG_KEY: str = \"\"\\n    USE_ALLOCATION: bool = False\\n    ORG_KEY_1: str = \"\"\\n    ORG_KEY_2: str = \"\"\\n    ORG_KEY_3: str = \"\"\\n    ORG_KEY_4: str = \"\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_OPENAI_CONFIG_\"\\n\\n\\nclass VertexAIConfig(BaseSettings):\\n    PROJECT_ID: str = \"silicon-airlock-153323\"\\n    REGION: str = \"asia-southeast1\"\\n    CREDENTIAL_FILE_CONTENT: str = \"\"\\n    CREDENTIAL_FILE_PATH: str = os.path.join(os.getcwd(), \"sa-ai-platform-user.json\")\\n    USE_ALLOCATION: bool = False\\n\\n    def get_secret(self):\\n        if len(self.CREDENTIAL_FILE_CONTENT):\\n            with open(self.CREDENTIAL_FILE_PATH, \"w\") as f:\\n                f.write(self.CREDENTIAL_FILE_CONTENT)\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_VERTEXAI_CONFIG_\"\\n\\n\\nclass EventTrackingConfig(BaseSettings):\\n    ACTIVE: bool = False\\n    ENDPOINT: str = \"http://rg-event-tracking-api.staging.svc.cluster.local/api/v3/event/tracker\"\\n\\n    class Config:\\n        env_prefix = \"RG_DS_CHATBOT_API_TRACKING_\"\\n\\n\\nOPENAI_CONFIG = OpenAIConfig()\\nVERTEXAI_CONFIG = VertexAIConfig()\\nVERTEXAI_CONFIG.get_secret()\\n\\nCHAT_COMPLETION_CONFIG = ChatCompletionConfig()\\nEVENT_TRACKING_CONFIG = EventTrackingConfig()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\config.py'}),\n",
       " Document(page_content='from uuid import uuid4\\n\\n\\ndef generate_uuid():\\n    return str(uuid4())\\n\\n\\ndef pipe(data, *funcs):\\n    \"\"\" Pipe a value through a sequence of functions\\n\\n    I.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\\n\\n    We think of the value as progressing through a pipe of several\\n    transformations, much like pipes in UNIX\\n\\n    ``$ cat data | f | g | h``\\n\\n    >>> double = lambda i: 2 * i\\n    >>> pipe(3, double, str)\\n    \\'6\\'\\n\\n    See Also:\\n        compose\\n        thread_first\\n        thread_last\\n    \"\"\"\\n    for func in funcs:\\n        data = func(data)\\n    return data\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\utils.py'}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\__init__.py'}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.applications import FastAPI\\nfrom fastapi.exceptions import RequestValidationError\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError, AIException\\nfrom llm.api.views import error_response\\nfrom typing import Union\\n\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nasync def request_validation_handler(_: Request, error: RequestValidationError):\\n    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\\n                         for e in error.errors()]\\n    return error_response(error, 400, validation_errors)\\n\\n\\nasync def http_exception_handler(_, error: StarletteHTTPException):\\n    return error_response(error)\\n\\n\\nasync def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\\n    logger.error(f\"Not Found for {error.field}: {error.value}\")\\n    return error_response(error, 400)\\n\\n\\nasync def ai_exception_handler(_, error):\\n    logger.error(f\"LLM error with detail: {error}\")\\n    return error_response(error, 500)\\n\\n\\ndef register_error_handlers(app: FastAPI) -> FastAPI:\\n    app.exception_handler(RequestValidationError)(request_validation_handler)\\n    app.exception_handler(StarletteHTTPException)(http_exception_handler)\\n    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\\n    app.exception_handler(AIException)(ai_exception_handler)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\error_handlers.py'}),\n",
       " Document(page_content='from fastapi.applications import FastAPI\\nfrom llm.client.common.async_http import async_http_open, async_http_close\\n\\n\\nasync def startup() -> None:\\n    await async_http_open()\\n\\n\\nasync def shutdown() -> None:\\n    await async_http_close()\\n\\n\\ndef register_events(app: FastAPI) -> FastAPI:\\n    app.add_event_handler(\"startup\", startup)\\n    app.add_event_handler(\"shutdown\", shutdown)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\events.py'}),\n",
       " Document(page_content='from fastapi.applications import FastAPI\\n\\nfrom llm.api.routers import health\\nfrom llm.api.routers import chat_completion\\nfrom llm.api.error_handlers import register_error_handlers\\nfrom llm.api.events import register_events\\nfrom llm.api.middleware import register_middlewares\\nfrom llm.utils import pipe\\n\\n\\ndef create_instance() -> FastAPI:\\n    return FastAPI()\\n\\n\\ndef init_database(app: FastAPI) -> FastAPI:\\n    return app\\n\\n\\ndef register_routers(app: FastAPI) -> FastAPI:\\n    app.include_router(health.router)\\n    app.include_router(chat_completion.router)\\n    return app\\n\\n\\ndef init_app() -> FastAPI:\\n    app: FastAPI = pipe(\\n        create_instance(),\\n        init_database,\\n        register_events,\\n        register_middlewares,\\n        register_error_handlers,\\n        register_routers\\n    )\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\factory.py'}),\n",
       " Document(page_content='import logging\\nfrom typing import Callable\\n\\nfrom fastapi import Request, Response\\nfrom starlette.middleware.base import BaseHTTPMiddleware\\nfrom fastapi.applications import FastAPI\\nfrom llm.api.views import error_response\\nimport time\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass RoguLoggerMiddleware(BaseHTTPMiddleware):\\n\\n    # ref: https://github.com/encode/starlette/issues/495\\n    async def _reset_receive(self, request, content):\\n        async def receive():\\n            return content\\n        request._receive = receive\\n\\n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\\n        start_time = time.monotonic()\\n\\n        try:\\n            response = await call_next(request)\\n            processing_time = time.monotonic() - start_time\\n            response.headers[\"x-processing-time\"] = str(round(processing_time*1000, 2))\\n            return response\\n        except Exception as e:\\n            # catch any exception during performing request\\n            logger.exception(e)\\n            return error_response(e)\\n\\n\\ndef register_middlewares(app: FastAPI) -> FastAPI:\\n    app.add_middleware(RoguLoggerMiddleware)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\middleware.py'}),\n",
       " Document(page_content='import re\\nimport http\\nimport json\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import AIException, PlatformModelNotFoundError\\nfrom typing import Dict, List, Optional, Type\\nfrom fastapi.responses import JSONResponse\\n\\n\\ndef _camel_to_snake(name):\\n    name = re.sub(\\'(.)([A-Z][a-z]+)\\', r\\'\\\\1_\\\\2\\', name)\\n    return re.sub(\\'([a-z0-9])([A-Z])\\', r\\'\\\\1_\\\\2\\', name).upper()\\n\\n\\ndef chat_completion_response(response):\\n    output = {\\n        \"uuid\": response.uuid,\\n        \"choices\": [choice.dict() for choice in response.choices],\\n        \"usage\": response.usage.dict()\\n    }\\n\\n    return JSONResponse(\\n        dict(\\n            data=output,\\n            status=\"success\",\\n            message=\"success\"\\n        )\\n    )\\n\\n\\nasync def stream_chat_completion_response(response):\\n    async for row in response:\\n        output = {\\n            \"uuid\": row.uuid,\\n            \"choices\": [choice.dict() for choice in row.choices],\\n            \"usage\": row.usage.dict()\\n        }\\n        yield json.dumps(output) + \"\\\\n\"\\n\\n\\ndef error_response(error: Exception, code: int = 500, descriptions: Optional[List] = None):\\n    if descriptions is None:\\n        descriptions = []\\n\\n    status = http.HTTPStatus(code).name\\n\\n    if isinstance(error, StarletteHTTPException):\\n        name = error.detail.replace(\" \", \"\")\\n        code = error.status_code\\n    else:\\n        name = error.__class__.__name__\\n\\n    if isinstance(error, AIException):\\n        if error.args[2] is not None:\\n            code = error.args[2].status_code\\n            message = str(error.args[2].message)\\n            error_field = _camel_to_snake(error.args[2].__class__.__name__)\\n            if code is None:\\n                code = 500\\n            status = http.HTTPStatus(code).name\\n\\n        else:\\n            message = error.args[1]\\n            error_field = _camel_to_snake(error._error_code.name)\\n            if error._error_code == AIException.ErrorCodes.InvalidRequest:\\n                code = 400\\n                status = http.HTTPStatus(code).name\\n                error_field = _camel_to_snake(openai.error.InvalidRequestError.__name__)\\n\\n        error_message = [\\n                {\"field\": error_field, \"message\": message}\\n            ]\\n    elif isinstance(error, PlatformModelNotFoundError):\\n        error_message = [\\n            {\"field\": error.args[0], \"message\": f\"Not found for {error.args[0]}: {error.args[1]}\"}\\n        ]\\n\\n    else:\\n        error_message = [\\n                {\"field\": desc[0], \"message\": desc[1]} for desc in descriptions\\n        ]\\n\\n    return JSONResponse(\\n        dict(\\n            status=status,\\n            error=_camel_to_snake(name),\\n            code=code,\\n            error_code=error_message,\\n            metadata={}\\n        ),\\n        status_code=code,\\n    )\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py'}),\n",
       " Document(page_content='from llm.api.factory import init_app\\n\\n\\napp = init_app()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\__init__.py'}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.routing import APIRouter\\nfrom fastapi.responses import JSONResponse\\nfrom llm.api.views import chat_completion_response, stream_chat_completion_response\\nfrom llm.core.service.dto import ChatCompletionRequest\\nfrom llm.core.service.chat_completion import ChatCompletionService\\nfrom llm.config import CHAT_COMPLETION_CONFIG\\nfrom fastapi.responses import StreamingResponse\\nfrom llm.client.event_tracking.client import EventTrackingApi\\nimport logging\\n\\n\\nrouter = APIRouter()\\nlogger = logging.getLogger(__name__)\\n\\nevent_tracker = EventTrackingApi()\\nactive_models = CHAT_COMPLETION_CONFIG.ACTIVE_MODELS.split(\";\")\\nchat_completion_service = ChatCompletionService(active_models, event_tracker)\\n\\n\\n@router.post(\"/chat/completion\")\\nasync def chat_completion(chat_request: ChatCompletionRequest, request: Request):\\n    raw_response = await chat_completion_service.generate_chat_completion(chat_request)\\n\\n    if not chat_request.stream:\\n        return chat_completion_response(raw_response)\\n    else:\\n        response = stream_chat_completion_response(raw_response)\\n        return StreamingResponse(response, media_type=\"text/event-stream\")\\n\\n\\n@router.post(\"/dummy/completion\")\\nasync def chat_completion_dummy(chat_request: ChatCompletionRequest, request: Request):\\n\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\chat_completion.py'}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.responses import JSONResponse\\nfrom fastapi.routing import APIRouter\\n\\nrouter = APIRouter()\\n\\n\\n@router.get(\"/_health\")\\nasync def health(request: Request):\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\health.py'}),\n",
       " Document(page_content='import logging\\nimport aiohttp\\n\\nfrom typing import Tuple, Optional\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass AsyncHttp:\\n\\n    __slots__ = (\"_client\",)\\n\\n    def __init__(self) -> None:\\n        self._client = None\\n\\n    async def open(self) -> aiohttp.ClientSession:\\n        if not self._client:\\n            self._client = aiohttp.ClientSession()\\n            logger.info(\"Async HTTP client session opened\")\\n        return self._client\\n\\n    async def close(self) -> None:\\n        if self._client:\\n            await self._client.close()\\n            logger.info(\"Async HTTP client session closed\")\\n            self._client = None\\n\\n    async def get(self, url, headers: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def get_params(self, url, headers: Optional[dict] = {}, params: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers, params=params) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post(self, url, data: str, headers: Optional[dict] = None) -> Tuple[bytes, int]:\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post_forget(self, url, data: str, headers: Optional[dict] = None):\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers):\\n            pass\\n\\n\\n_async_http_client = AsyncHttp()\\nasync_http_open = _async_http_client.open\\nasync_http_close = _async_http_client.close\\nasync_get = _async_http_client.get\\nasync_get_params = _async_http_client.get_params\\nasync_post = _async_http_client.post\\nasync_post_forget = _async_http_client.post_forget\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py'}),\n",
       " Document(page_content='import logging\\nimport asyncio\\nimport orjson\\n\\nfrom datetime import datetime, timezone\\nfrom pydantic import BaseModel\\n\\nfrom llm.client.common.async_http import async_post_forget\\nfrom llm.client.event_tracking.entity import (\\n    PostEventTrackingRequestData,\\n    EventLLMChatCompletion\\n)\\nfrom llm.core.service.dto import ChatCompletionRequest, ChatCompletionResponse\\nfrom llm.config import EVENT_TRACKING_CONFIG\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass EventTrackingApi:\\n\\n    def __init__(\\n            self,\\n            endpoint: str = EVENT_TRACKING_CONFIG.ENDPOINT,\\n            active: bool = EVENT_TRACKING_CONFIG.ACTIVE) -> None:\\n\\n        self._endpoint = endpoint\\n        self._active = active\\n\\n    async def log_llm_chat_completion(\\n            self,\\n            chat_completion_request: ChatCompletionRequest,\\n            chat_completion_response: ChatCompletionResponse,\\n            ) -> None:\\n\\n        if len(chat_completion_response.choices) > 0:\\n            if chat_completion_response.choices[0].message.tool_calls is not None:\\n                tool_calls = [x.dict() for x in chat_completion_response.choices[0].message.tool_calls]\\n                completion = orjson.dumps(tool_calls)\\n            else:\\n                completion = chat_completion_response.choices[0].message.content\\n        else:\\n            completion = \"\"\\n\\n        messages = [x.dict() for x in chat_completion_request.messages]\\n\\n        token_usage = {\\n            \"prompt_token\": chat_completion_response.usage.prompt_token,\\n            \"completion_token\": chat_completion_response.usage.completion_token,\\n        }\\n\\n        event = EventLLMChatCompletion(\\n            uuid=chat_completion_response.uuid,\\n            trace_id=chat_completion_request.trace_id,\\n            session_id=chat_completion_request.session_id,\\n            message=orjson.dumps(messages),\\n            task_name=chat_completion_request.task_name,\\n            model=chat_completion_request.model,\\n            platform=chat_completion_request.platform,\\n            completion=completion,\\n            client_name=chat_completion_request.client_name,\\n            token_usage=token_usage,\\n        )\\n\\n        await self._send(\"dsChatbotLlmChatCompletion\", event)\\n\\n    async def _send(self, event_type: str, context: BaseModel, memberId: str = None) -> None:\\n        if self._active:\\n            json_data = PostEventTrackingRequestData(\\n                memberId=memberId,\\n                isLogged=False,\\n                eventType=event_type,\\n                clientTimestamp=datetime.now(timezone.utc).isoformat(),\\n                context=context.model_dump_json()\\n            ).model_dump_json()\\n\\n            headers = {\"content-type\": \"application/json\"}\\n            asyncio.create_task(async_post_forget(self._endpoint, data=json_data, headers=headers))\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py'}),\n",
       " Document(page_content='from typing import List, Optional, Union, Any, Dict\\nfrom pydantic import BaseModel\\n\\n\\nclass EventLLMChatCompletion(BaseModel):\\n    uuid: str\\n    session_id: str\\n    trace_id: str\\n    platform: str\\n    model: str\\n    message: str\\n    completion: Optional[str] = None\\n    task_name: Optional[str]\\n    client_name: str\\n    token_usage: Dict[str, float]\\n\\n\\nclass PostEventTrackingRequestData(BaseModel):\\n    id: Optional[str] = None\\n    sessionId: Optional[str] = None\\n    cookiesId: Optional[str] = None\\n    deviceId: Optional[str] = None\\n    memberId: Optional[str] = None\\n    isLogged: bool = False\\n    eventType: str\\n    clientDevice: Optional[str] = None\\n    clientUA: Optional[str] = None\\n    appVersion: Optional[str] = None\\n    clientOS: Optional[str] = None\\n    clientOSVersion: Optional[str] = None\\n    clientTimestamp: Optional[str] = None\\n    context: Union[str, Any]\\n    connectionType: Optional[str] = None\\n    source: str = \"backend\"\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\entity.py'}),\n",
       " Document(page_content='from openai import AsyncOpenAI\\nfrom llm.core.service.exceptions import AIException\\n\\n\\nclass OpenAIClient:\\n    def __init__(self):\\n        self.client = AsyncOpenAI()\\n\\n    async def send_chat_request(self, messages, model, request_settings, stream, response_format, tools, client_org):\\n        self.client.organization = client_org\\n        if stream:\\n            stream_options = {\"include_usage\": True}\\n        else:\\n            stream_options = None\\n\\n        try:\\n            response = await self.client.chat.completions.create(\\n                model=model,\\n                messages=messages,\\n                temperature=request_settings.temperature,\\n                max_tokens=request_settings.max_tokens,\\n                top_p=request_settings.top_p,\\n                stream=stream,\\n                stream_options=stream_options,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n        except Exception as ex:\\n            raise AIException(\\n                \"openai\",\\n                \"OpenAI service failed to complete the chat\",\\n                ex\\n            )\\n\\n        return response\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\client.py'}),\n",
       " Document(page_content='from pydantic import BaseModel\\n\\n\\nclass OpenAIRequestSettings(BaseModel):\\n    temperature: float\\n    max_tokens: int\\n    top_p: float\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\entity.py'}),\n",
       " Document(page_content='from llm.config import VERTEXAI_CONFIG\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel, GenerationConfig, Image\\nfrom google.oauth2.service_account import Credentials\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nfrom llm.client.vertexai.entity import UserContent, SAFETY_SETTING\\nfrom llm.client.common.async_http import async_get\\n\\n\\nclass VertexAIClient:\\n    def __init__(self, location=None, credential_file=None):\\n        if credential_file is None:\\n            credential_file = VERTEXAI_CONFIG.CREDENTIAL_FILE_PATH\\n        else:\\n            credential_file = credential_file\\n        credentials = Credentials.from_service_account_file(credential_file)\\n\\n        if location is None:\\n            location = VERTEXAI_CONFIG.REGION\\n\\n        vertexai.init(\\n            project=VERTEXAI_CONFIG.PROJECT_ID,\\n            location=location,\\n            credentials=credentials\\n        )\\n\\n        self.model_client = self.__load_model()\\n\\n    def __load_model(self):\\n        model_client = {}\\n        for model_id in CHAT_COMPLETION_MODELS.get(\"vertexai\", []):\\n            model_client[model_id] = GenerativeModel(model_id)\\n        return model_client\\n\\n    async def generate_response(\\n        self,\\n        model: str,\\n        user_content: UserContent,\\n        generation_config: GenerationConfig,\\n        stream: bool,\\n        system_message: str = None\\n    ):\\n        if system_message is not None:\\n            model_client = GenerativeModel(model, system_instruction[system_message])\\n        else:\\n            model_client = self.model_client.get(model)\\n\\n        contents = [user_content.text]\\n        if user_content.images is not None:\\n            images = [await self._load_image_from_url(image) for image in user_content.images]\\n            contents.extend(images)\\n\\n        response = await model_client.generate_content_async(\\n            contents=contents,\\n            generation_config=generation_config,\\n            safety_settings=SAFETY_SETTING,\\n            stream=stream\\n        )\\n\\n        return response\\n\\n    async def _get_image_bytes_from_url(self, image_url: str) -> bytes:\\n        response, status = await async_get(image_url)\\n\\n        return response\\n\\n    async def _load_image_from_url(self, image_url: str) -> Image:\\n        image_bytes = await self._get_image_bytes_from_url(image_url)\\n        return Image.from_bytes(image_bytes)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py'}),\n",
       " Document(page_content='from pydantic import BaseModel\\nfrom vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\\nfrom typing import Optional, List\\n\\n\\nclass UserContent(BaseModel):\\n    text: str\\n    images: Optional[List] = None\\n\\n\\nSAFETY_SETTING = {\\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\entity.py'}),\n",
       " Document(page_content='CHAT_COMPLETION_MODELS = {\\n    \"openai\": [\\n        \"gpt-3.5-turbo\",\\n        \"gpt-3.5-turbo-1106\",\\n        \"gpt-3.5-turbo-0125\",\\n        \"gpt-3.5-turbo-16k\",\\n        \"gpt-4\",\\n        \"gpt-4-turbo\",\\n        \"gpt-4-0125-preview\",\\n        \"gpt-4-1106-preview\",\\n        \"gpt-4o\",\\n        \"ft:gpt-3.5-turbo-0125:ruangguru:intentclass:9Wjgagxi\"\\n    ],\\n    \"vertexai\": [\\n        \"gemini-1.5-flash-001\",\\n        \"gemini-1.5-pro-001\"\\n    ]\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\entity.py'}),\n",
       " Document(page_content='import glob\\nimport yaml\\nimport os\\nimport mmh3\\nfrom llm.utils import generate_uuid\\n\\n\\nSEGMENT_SIZE = 1000\\n\\n\\nclass GeneralAllocation:\\n    def __init__(self, platform_name):\\n        self.config_directory = \"llm/core/allocation/{}/config\".format(platform_name)\\n        self.config = {}\\n        self.__create_config()\\n\\n    def __create_config(self):\\n        directory = glob.glob(self.config_directory + \"/*.yaml\", recursive=True)\\n\\n        for filepath in directory:\\n            config_name = os.path.basename(filepath).split(\".\")[0]\\n            with open(filepath) as f:\\n                config_yaml = yaml.load(f, Loader=yaml.FullLoader)\\n            self.config[config_name] = config_yaml\\n\\n            self.config[config_name] = self.__compute_allocation(config_name)\\n\\n    def get_variant(self, config_name, unit_id=None):\\n        if unit_id is None:\\n            unit_id = generate_uuid()\\n\\n        segment = self.get_segment(unit_id)\\n        if config_name not in self.config.keys():\\n            return None\\n\\n        variants = self.config[config_name][\"variants\"]\\n\\n        for variant in self.config[config_name][\"variants\"]:\\n            if segment < variant[\"offset\"]:\\n                return variant[\"value\"]\\n\\n        return variants[0][\"value\"]\\n\\n    def __compute_allocation(self, config_name):\\n        config = self.config[config_name]\\n        list_variant = []\\n        offset = 0\\n        for row in config[\"variants\"]:\\n            upperbound = int(row[\"weight\"]*SEGMENT_SIZE)\\n            offset = offset + upperbound\\n            list_variant.append(\\n                {\\n                    \"value\": row[\"value\"],\\n                    \"weight\": row[\"weight\"],\\n                    \"offset\": offset\\n                }\\n            )\\n        config[\"variants\"] = list_variant\\n\\n        return config\\n\\n    def get_segment(self, unit_id):\\n        segment = mmh3.hash(unit_id, signed=False) % SEGMENT_SIZE\\n\\n        return segment\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\allocation\\\\__init__.py'}),\n",
       " Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.port import AbstractChatCompletionModel\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = False\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        choices = CompletionChoice(\\n            index=0,\\n            message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n        )\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=[choices],\\n            usage={\\n                \"prompt_token\": 100,\\n                \"completion_token\": 100,\\n                \"total_token\": 200\\n            }\\n        )\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        async def post_process_stream():\\n            for i in range(10):\\n                choices = CompletionChoice(\\n                    index=i,\\n                    message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n                )\\n                response = ChatCompletionResponse(\\n                    uuid=generate_uuid(),\\n                    choices=[choices],\\n                    usage={\\n                        \"prompt_token\": 100,\\n                        \"completion_token\": 100,\\n                        \"total_token\": 200\\n                    }\\n                )\\n                yield response\\n\\n        return (row async for row in post_process_stream())\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\dummy\\\\model.py'}),\n",
       " Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import OPENAI_CONFIG\\nfrom llm.core.platform.openai.entity import MODEL_VARIANT, ORG_KEYS\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_org_key(self, model_id):\\n        if not OPENAI_CONFIG.USE_ALLOCATION:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_model = MODEL_VARIANT.get(model_id)\\n        variant_value = self.get_variant(variant_model)\\n\\n        if variant_value is None:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_org_key = ORG_KEYS.get(variant_value, OPENAI_CONFIG.DEFAULT_ORG_KEY)\\n        logger.info(f\"use openai varian for model_id: {model_id}, with value {variant_value}:{variant_org_key}\")\\n\\n        return variant_org_key\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\allocation.py'}),\n",
       " Document(page_content='from llm.config import OPENAI_CONFIG\\n\\n\\nORG_KEYS = {\\n    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\\n    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\\n    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\\n    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\\n}\\n\\nMODEL_VARIANT = {\\n    \"gpt-3.5-turbo\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\\n    \"gpt-4\": \"gpt-4\",\\n    \"gpt-4-turbo\": \"gpt-4-turbo\",\\n    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\\n    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\\n    \"gpt-4o\": \"gpt-4o\"\\n\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\entity.py'}),\n",
       " Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.client.openai.client import OpenAIClient\\nfrom llm.client.openai.entity import OpenAIRequestSettings\\nfrom llm.core.service.port import AbstractChatCompletionModel\\nfrom llm.core.platform.openai.allocation import Allocation\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.client = OpenAIClient()\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"openai\")\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools,\\n    ):\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n\\n        org_key = self.allocation.get_variant_org_key(model)\\n\\n        openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=False,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )\\n\\n        choices = [choice.dict() for choice in openai_response.choices]\\n        choices = [CompletionChoice(**choice) for choice in choices]\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage={\\n                \"prompt_token\": openai_response.usage.prompt_tokens,\\n                \"completion_token\": openai_response.usage.completion_tokens,\\n                \"total_token\": openai_response.usage.total_tokens\\n            }\\n        )\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n        org_key = self.allocation.get_variant_org_key(model)\\n\\n        openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=True,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )\\n\\n        async def post_process_stream(uuid, openai_response):\\n            async for chunk in openai_response:\\n                choices = []\\n                for choice in chunk.choices:\\n                    message_response = MessageDataResponse(\\n                        content=choice.delta.content,\\n                        role=choice.delta.role\\n                    )\\n                    _choices = CompletionChoice(\\n                        index=choice.index, message=message_response, finish_reason=choice.finish_reason\\n                    )\\n                    choices.append(_choices)\\n\\n                if chunk.usage is not None:\\n                    usage = {\\n                        \"prompt_token\": chunk.usage.prompt_tokens,\\n                        \"completion_token\": chunk.usage.completion_tokens,\\n                        \"total_token\": chunk.usage.total_tokens\\n                    }\\n                else:\\n                    usage = {\\n                        \"prompt_token\": 0,\\n                        \"completion_token\": 0,\\n                        \"total_token\": 0\\n                    }\\n\\n                response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, openai_response))\\n        return response\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py'}),\n",
       " Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import VERTEXAI_CONFIG\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_region(self):\\n        if not VERTEXAI_CONFIG.USE_ALLOCATION:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        variant_value = self.get_variant(\"region\")\\n\\n        if variant_value is None:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        logger.info(f\"use vertex ai region with value {variant_value}\")\\n\\n        return variant_value\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\allocation.py'}),\n",
       " Document(page_content='REGION_VARIANT = [\\n    \"us-central1\",\\n    \"us-east4\",\\n    \"us-west1\",\\n    \"us-west4\",\\n    \"northamerica-northeast1\",\\n    \"europe-west9\",\\n    \"europe-west2\",\\n    \"europe-west3\",\\n    \"europe-west4\",\\n    \"europe-west9\",\\n    \"asia-northeast1\",\\n    \"asia-northeast3\",\\n    \"asia-southeast1\"\\n]\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\entity.py'}),\n",
       " Document(page_content='from llm.core.service.port import AbstractChatCompletionModel\\nfrom vertexai.generative_models import Image, GenerationConfig\\nfrom llm.client.vertexai.client import VertexAIClient\\nfrom llm.client.vertexai.entity import UserContent\\nfrom llm.core.platform.vertexai.allocation import Allocation\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.core.platform.vertexai.entity import REGION_VARIANT\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"vertexai\")\\n        self.clients = self.__load_client()\\n\\n    def __load_client(self):\\n        clients = {}\\n        for region in REGION_VARIANT:\\n            clients[region] = VertexAIClient(location=region)\\n        return clients\\n\\n    def __get_client(self):\\n        varian_region = self.allocation.get_variant_region()\\n        return self.clients.get(varian_region)\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()\\n\\n        # TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=False,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )\\n\\n        choices = self._parse_choices(vertexai_response.candidates)\\n\\n        usage = {\\n            \"prompt_token\": vertexai_response.usage_metadata.prompt_token_count,\\n            \"completion_token\": vertexai_response.usage_metadata.candidates_token_count,\\n            \"total_token\": vertexai_response.usage_metadata.total_token_count\\n        }\\n\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage=usage\\n        )\\n\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()\\n\\n        # TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=True,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )\\n\\n        async def post_process_stream(uuid, vertexai_response):\\n            async for chunk in vertexai_response:\\n                if chunk is None:\\n                    continue\\n                choices = []\\n                index = 0\\n                for row in chunk.candidates:\\n                    if len(row.content.parts) > 0:\\n                        choice = CompletionChoice(\\n                            index=index,\\n                            message=MessageDataResponse(\\n                                role=\"assistant\",\\n                                content=row.content.parts[0].text\\n                            ),\\n                            finish_reason=row.finish_reason.name\\n                        )\\n                        choices.append(choice)\\n                        index += 1\\n                usage = {\\n                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\\n                    \"completion_token\": chunk.usage_metadata.candidates_token_count,\\n                    \"total_token\": chunk.usage_metadata.total_token_count\\n                }\\n\\n                response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, vertexai_response))\\n\\n        return response\\n\\n    def _get_system_message(self, messages):\\n        first_message = messages[0]\\n        if first_message.role == \"system\":\\n            return first_message.content\\n        else:\\n            return None\\n\\n    def _get_user_content(self, messages):\\n        # currently only use last user message as user content\\n        last_message = messages[-1]\\n        text = None\\n        images = []\\n        if last_message.role == \"user\":\\n            content = last_message.content\\n            if isinstance(content, list):\\n                for row in content:\\n                    if row.type == \"text\":\\n                        text = row.text\\n                    elif row.type == \"image_url\":\\n                        image_url = row.image_url.get(\"url\")\\n                        images.append(image_url)\\n            else:\\n                text = content\\n\\n        if len(images) == 0:\\n            images = None\\n\\n        return UserContent(text=text, images=images)\\n\\n    def _construct_generation_config(self, config: ChatCompletionSetting, response_format):\\n        if response_format is not None:\\n            if response_format.type == \"json_object\":\\n                mime_type = \"application/json\"\\n            else:\\n                mime_type = \"text/plain\"\\n        else:\\n            mime_type = \"text/plain\"\\n\\n        generation_config = GenerationConfig(\\n            temperature=config.temperature,\\n            max_output_tokens=config.max_token,\\n            response_mime_type=mime_type\\n        )\\n\\n        return generation_config\\n\\n    def _parse_choices(self, response_candidates):\\n        index = 0\\n        choices = []\\n        for row in response_candidates:\\n            if len(row.content.parts)>0:\\n                messages = MessageDataResponse(\\n                    role=\"assistant\",\\n                    content=row.content.parts[0].text\\n                )\\n            else:\\n                messages = MessageDataResponse()\\n\\n            choice = CompletionChoice(\\n                index=index,\\n                message=messages,\\n                finish_reason=row.finish_reason.name\\n            )\\n            choices.append(choice)\\n            index += 1\\n\\n            return choices\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py'}),\n",
       " Document(page_content='from llm.core.service.dto import ChatCompletionRequest\\nfrom importlib import import_module\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nimport copy\\n\\n\\nclass ChatCompletionService:\\n    def __init__(self, active_models, event_tracker):\\n        self.active_models = active_models\\n        self.event_tracker = event_tracker\\n        self.__models = self.__load_model()\\n\\n    def __load_model(self):\\n        models = {}\\n        for platform in self.active_models:\\n            Model = getattr(\\n                import_module(\"llm.core.platform.{}.model\".format(platform)),\\n                \"Model\",\\n            )\\n            models[platform] = Model()\\n        return models\\n\\n    def __validate_platform_and_model(self, platform: str, model: str):\\n        platform_model = self.__models.get(platform)\\n        if not platform_model:\\n            raise PlatformModelNotFoundError(\"platform\", platform)\\n\\n        if platform_model.require_model_validation:\\n            if model not in CHAT_COMPLETION_MODELS.get(platform, []):\\n                raise PlatformModelNotFoundError(\"model\", model)\\n\\n    async def generate_chat_completion(self, request: ChatCompletionRequest):\\n        model_id = request.model\\n        message_data = request.messages\\n        config = request.config\\n        response_format = request.response_format\\n        tools = request.tools\\n\\n        self.__validate_platform_and_model(request.platform, model_id)\\n\\n        if not request.stream:\\n            completion_response = await self.__models[request.platform].generate_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n            await self.event_tracker.log_llm_chat_completion(request, completion_response)\\n\\n            return completion_response\\n\\n        else:\\n            completion_response = await self.__models[request.platform].generate_stream_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n\\n            return (row async for row in self.__send_tracker_stream(request, completion_response))\\n\\n    async def __send_tracker_stream(self, request: ChatCompletionRequest, response):\\n        full_completion_content = \"\"\\n        full_completion_role = None\\n        full_response = None\\n        full_usage = None\\n        async for row in response:\\n            yield row\\n\\n            if len(row.choices) > 0:\\n                if full_response is None:\\n                    full_response = copy.deepcopy(row)\\n                if row.choices[0].message.content is not None:\\n                    full_completion_content += row.choices[0].message.content\\n                if row.choices[0].message.role is not None:\\n                    full_completion_role = row.choices[0].message.role\\n            if row.usage.prompt_token > 0:\\n                full_usage = row.usage\\n\\n        full_response.choices[0].message.content = full_completion_content\\n        full_response.choices[0].message.role = full_completion_role\\n        full_response.usage = full_usage\\n\\n        await self.event_tracker.log_llm_chat_completion(request, full_response)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py'}),\n",
       " Document(page_content='from pydantic import BaseModel\\nfrom typing import Optional, List, Dict, Union, Any\\n\\n\\nclass ContentData(BaseModel):\\n    type: str\\n    text: Optional[str] = None\\n    image_url: Optional[Dict[str, str]] = None\\n\\n\\nclass FunctionParameterData(BaseModel):\\n    type: str\\n    properties: Dict[Any, Any]\\n    required: List[str]\\n\\n\\nclass FunctionData(BaseModel):\\n    name: str\\n    description: str\\n    parameters: FunctionParameterData\\n\\n\\nclass ToolsDataRequest(BaseModel):\\n    type: str\\n    function: Optional[FunctionData]\\n\\n\\nclass MessageDataRequest(BaseModel):\\n    role: str\\n    content: Union[str, List[ContentData]]\\n\\n\\nclass ChatCompletionSetting(BaseModel):\\n    max_token: Optional[int] = 256\\n    temperature: Optional[float] = 0.7\\n    top_p: Optional[float] = 1\\n    top_k: Optional[float] = 1\\n\\n\\nclass ResponseFormatRequest(BaseModel):\\n    type: Optional[str] = None\\n\\n\\nclass ChatCompletionRequest(BaseModel):\\n    platform: str\\n    model: str\\n    messages: List[MessageDataRequest]\\n    tools: Optional[List[ToolsDataRequest]] = None\\n    config: Optional[ChatCompletionSetting] = {}\\n    stream: bool = False\\n    task_name: Optional[str] = None\\n    client_name: str\\n    trace_id: str\\n    session_id: Optional[str] = \"\"\\n    response_format: Optional[ResponseFormatRequest] = None\\n\\n\\nclass TokenUsage(BaseModel):\\n    prompt_token: Optional[int]\\n    completion_token: Optional[int]\\n    total_token: Optional[int]\\n\\n\\nclass FunctionResponse(BaseModel):\\n    arguments: str\\n    name: str\\n\\n\\nclass ToolCallResponse(BaseModel):\\n    function: Optional[FunctionResponse] = None\\n    type: str\\n\\n\\nclass MessageDataResponse(BaseModel):\\n    role: Optional[str] = None\\n    content: Optional[str] = None\\n    tool_calls: Optional[List[ToolCallResponse]] = None\\n\\n\\nclass CompletionChoice(BaseModel):\\n    index: int\\n    message: MessageDataResponse\\n    finish_reason: Optional[str] = None\\n\\n\\nclass ChatCompletionResponse(BaseModel):\\n    uuid: str\\n    choices: Optional[List[CompletionChoice]] = []\\n    usage: Optional[TokenUsage] = {}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\dto.py'}),\n",
       " Document(page_content='class PlatformModelNotFoundError(Exception):\\n    def __init__(self, field, value):\\n        self.field = field\\n        self.value = value\\n\\n\\nclass AIException(Exception):\\n    def __init__(self, name, message, errors):\\n        self.name = name\\n        self.message = message\\n        self.errors = errors\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\exceptions.py'}),\n",
       " Document(page_content='import abc\\n\\n\\nclass AbstractChatCompletionModel(abc.ABC):\\n    def __init__(self) -> None:\\n        pass\\n\\n    @abc.abstractmethod\\n    def generate_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError\\n\\n    @abc.abstractmethod\\n    def generate_stream_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\port.py'}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\__init__.py'}),\n",
       " Document(page_content='import pytest\\nfrom fastapi.testclient import TestClient\\nfrom pytest import fixture\\n\\n\\n@pytest.fixture(scope=\"package\")\\ndef client():\\n    from llm.api import app\\n    with TestClient(app) as client:\\n        yield client\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\conftest.py'}),\n",
       " Document(page_content='from pprint import pprint\\nimport requests\\nimport json\\n\\n\\ndef test_chat_completion(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"dummy\",\\n        \"model\": \"gpt-4-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n\\n\\ndef test_chat_completion_openai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n\\n\\ndef test_chat_completion_openai_with_tools(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"What\\'s the weather like in San franciso, tokyo and paris\"}\\n        ],\\n        \"tools\": [{\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"get_current_weather\",\\n                \"description\": \"Get the current weather in a given location\",\\n                \"parameters\": {\\n                    \"type\": \"object\",\\n                    \"properties\": {\\n                        \"location\": {\\n                            \"type\": \"string\",\\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\\n                            },\\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\\n                    },\\n                    \"required\": [\"location\"]\\n                }\\n            }\\n        }],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n    assert len(response_data[\"choices\"][0][\"message\"][\"tool_calls\"]) > 0\\n\\n\\ndef test_chat_completion_vertexai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"vertexai\",\\n        \"model\": \"gemini-1.5-flash-001\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py'}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\__init__.py'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader1 = GenericLoader.from_filesystem(path='test_repo',\n",
    "                                        glob = \"**/*\",\n",
    "                                        suffixes=[\".py\"],\n",
    "                                        parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='from pydantic_settings import BaseSettings\\n\\n\\nclass GunicornConfig(BaseSettings):\\n    NUM_WORKERS: int = 1\\n    NUM_THREADS: int = 1\\n    TIMEOUT: int = 30\\n    LOG_LEVEL: str = \"INFO\"\\n    LOG_DEBUG: str = \"DEBUG\"\\n    LOG_ERROR: str = \"ERROR\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_API_GUNICORN_\"\\n\\n\\nGUNICORN_CONFIG = GunicornConfig()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\config.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\n\\n\\nclass RoguLogFilter(logging.Filter):\\n    \"\"\"\\n    Custom filter to exclude health check\\n    \"\"\"\\n\\n    def filter(self, record):\\n        try:\\n            uri = str(record.args[2])\\n        except Exception as e:\\n            uri = None\\n        return uri != \"/_health\"\\n\\n\\nclass RoguFormatter(logging.Formatter):\\n    \"\"\"\\n    RG Logger Formatter\\n    \"\"\"\\n\\n    datefmt = \"%Y-%m-%dT%H:%M:%SZ\"\\n    fmt = \\'RG_LOGGER: time=\"{asctime}\" name={name} lineno={lineno} level={levelname} msg=\"{msg}\" {extra}\\'\\n\\n    def format(self, record):\\n\\n        extra = []\\n        if record.exc_info:\\n            extra_str = f\\'exc_info=\"{self.formatException(record.exc_info)}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_body\"):\\n            extra_str = f\\'request_body=\"{record.request_body}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_path\"):\\n            extra_str = f\"request_path={record.request_path}\"\\n            extra.append(extra_str)\\n\\n        extra_str = \" \".join(extra)\\n        message = self.__class__.fmt.format(\\n            asctime=self.formatTime(record, self.__class__.datefmt),\\n            name=record.name,\\n            lineno=record.lineno,\\n            levelname=record.levelname.lower(),\\n            msg=self._one_liner(record.getMessage()),\\n            extra=self._one_liner(extra_str)\\n        )\\n\\n        return message\\n\\n    def formatException(self, exc_info):\\n        result = super().formatException(exc_info)\\n        return self._one_liner(result)\\n\\n    def _one_liner(self, msg):\\n        return msg.replace(\"\\\\n\", \"\\\\t|\\\\t\")\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\custom_logging.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import threading\\nimport sys\\nimport traceback\\nfrom logging.config import dictConfig\\nfrom config import GUNICORN_CONFIG\\n\\n\\nworkers = GUNICORN_CONFIG.NUM_WORKERS\\nthreads = GUNICORN_CONFIG.NUM_THREADS\\ntimeout = GUNICORN_CONFIG.TIMEOUT\\nworker_class = \"uvicorn.workers.UvicornWorker\"\\nbind = \\'0.0.0.0:80\\'\\n\\n\\nlogconfig_dict = {\\n        \"version\": 1,\\n        \"disable_existing_loggers\": True,\\n        \"formatters\": {\\n                \"custom\": {\\n                    \"()\": \"custom_logging.RoguFormatter\"\\n                }\\n        },\\n        \"filters\": {\\n            \"exclude_health_check\": {\\n                \"()\": \"custom_logging.RoguLogFilter\"\\n            }\\n        },\\n        \"handlers\": {\\n            \"access_console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\",\\n                \"filters\": [\"exclude_health_check\"]\\n            },\\n            \"console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\"\\n            },\\n        },\\n        \"loggers\": {\\n            \"\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"elasticsearch\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"openai\": {\\n                \"level\": GUNICORN_CONFIG.LOG_DEBUG,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"llm\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"gunicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.access\"\\n            },\\n            \"gunicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.error\"\\n            },\\n            \"uvicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"uvicorn.access\"\\n            },\\n            \"uvicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"uvicorn.error\"\\n            }\\n        }\\n    }\\ndictConfig(logconfig_dict)\\n\\n\\ndef post_fork(server, worker):\\n    server.log.info(\"Worker spawned (pid: %s)\", worker.pid)\\n\\n\\ndef pre_fork(server, worker):\\n    pass\\n\\n\\ndef post_worker_init(worker):\\n    pass\\n\\n\\ndef pre_exec(server):\\n    server.log.info(\"Forked child, re-executing.\")\\n\\n\\ndef worker_int(worker):\\n    worker.log.info(\"worker received INT or QUIT signal\")\\n\\n    # get traceback info\\n    id2name = dict([(th.ident, th.name) for th in threading.enumerate()])\\n    code = []\\n    for threadId, stack in sys._current_frames().items():\\n        code.append(\"\\\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\\n        for filename, lineno, name, line in traceback.extract_stack(stack):\\n            code.append(\\'File: \"%s\", line %d, in %s\\' % (filename, lineno, name))\\n            if line:\\n                code.append(\"  %s\" % (line.strip()))\\n    worker.log.debug(\"\\\\n\".join(code))\\n\\n\\ndef worker_abort(worker):\\n    worker.log.info(\"worker received SIGABRT signal\")\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import openai\\nimport warnings\\nfrom pydantic_settings import BaseSettings\\nimport os\\n\\n\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n\\nclass ChatCompletionConfig(BaseSettings):\\n    ACTIVE_MODELS: str = \"dummy;openai;vertexai\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_CHAT_COMPLETION_CONFIG_\"\\n\\n\\nclass OpenAIConfig(BaseSettings):\\n    DEFAULT_ORG_KEY: str = \"\"\\n    USE_ALLOCATION: bool = False\\n    ORG_KEY_1: str = \"\"\\n    ORG_KEY_2: str = \"\"\\n    ORG_KEY_3: str = \"\"\\n    ORG_KEY_4: str = \"\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_OPENAI_CONFIG_\"\\n\\n\\nclass VertexAIConfig(BaseSettings):\\n    PROJECT_ID: str = \"silicon-airlock-153323\"\\n    REGION: str = \"asia-southeast1\"\\n    CREDENTIAL_FILE_CONTENT: str = \"\"\\n    CREDENTIAL_FILE_PATH: str = os.path.join(os.getcwd(), \"sa-ai-platform-user.json\")\\n    USE_ALLOCATION: bool = False\\n\\n    def get_secret(self):\\n        if len(self.CREDENTIAL_FILE_CONTENT):\\n            with open(self.CREDENTIAL_FILE_PATH, \"w\") as f:\\n                f.write(self.CREDENTIAL_FILE_CONTENT)\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_VERTEXAI_CONFIG_\"\\n\\n\\nclass EventTrackingConfig(BaseSettings):\\n    ACTIVE: bool = False\\n    ENDPOINT: str = \"http://rg-event-tracking-api.staging.svc.cluster.local/api/v3/event/tracker\"\\n\\n    class Config:\\n        env_prefix = \"RG_DS_CHATBOT_API_TRACKING_\"\\n\\n\\nOPENAI_CONFIG = OpenAIConfig()\\nVERTEXAI_CONFIG = VertexAIConfig()\\nVERTEXAI_CONFIG.get_secret()\\n\\nCHAT_COMPLETION_CONFIG = ChatCompletionConfig()\\nEVENT_TRACKING_CONFIG = EventTrackingConfig()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\config.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from uuid import uuid4\\n\\n\\ndef generate_uuid():\\n    return str(uuid4())\\n\\n\\ndef pipe(data, *funcs):\\n    \"\"\" Pipe a value through a sequence of functions\\n\\n    I.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\\n\\n    We think of the value as progressing through a pipe of several\\n    transformations, much like pipes in UNIX\\n\\n    ``$ cat data | f | g | h``\\n\\n    >>> double = lambda i: 2 * i\\n    >>> pipe(3, double, str)\\n    \\'6\\'\\n\\n    See Also:\\n        compose\\n        thread_first\\n        thread_last\\n    \"\"\"\\n    for func in funcs:\\n        data = func(data)\\n    return data\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\utils.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.applications import FastAPI\\nfrom fastapi.exceptions import RequestValidationError\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError, AIException\\nfrom llm.api.views import error_response\\nfrom typing import Union\\n\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nasync def request_validation_handler(_: Request, error: RequestValidationError):\\n    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\\n                         for e in error.errors()]\\n    return error_response(error, 400, validation_errors)\\n\\n\\nasync def http_exception_handler(_, error: StarletteHTTPException):\\n    return error_response(error)\\n\\n\\nasync def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\\n    logger.error(f\"Not Found for {error.field}: {error.value}\")\\n    return error_response(error, 400)\\n\\n\\nasync def ai_exception_handler(_, error):\\n    logger.error(f\"LLM error with detail: {error}\")\\n    return error_response(error, 500)\\n\\n\\ndef register_error_handlers(app: FastAPI) -> FastAPI:\\n    app.exception_handler(RequestValidationError)(request_validation_handler)\\n    app.exception_handler(StarletteHTTPException)(http_exception_handler)\\n    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\\n    app.exception_handler(AIException)(ai_exception_handler)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\error_handlers.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from fastapi.applications import FastAPI\\nfrom llm.client.common.async_http import async_http_open, async_http_close\\n\\n\\nasync def startup() -> None:\\n    await async_http_open()\\n\\n\\nasync def shutdown() -> None:\\n    await async_http_close()\\n\\n\\ndef register_events(app: FastAPI) -> FastAPI:\\n    app.add_event_handler(\"startup\", startup)\\n    app.add_event_handler(\"shutdown\", shutdown)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\events.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from fastapi.applications import FastAPI\\n\\nfrom llm.api.routers import health\\nfrom llm.api.routers import chat_completion\\nfrom llm.api.error_handlers import register_error_handlers\\nfrom llm.api.events import register_events\\nfrom llm.api.middleware import register_middlewares\\nfrom llm.utils import pipe\\n\\n\\ndef create_instance() -> FastAPI:\\n    return FastAPI()\\n\\n\\ndef init_database(app: FastAPI) -> FastAPI:\\n    return app\\n\\n\\ndef register_routers(app: FastAPI) -> FastAPI:\\n    app.include_router(health.router)\\n    app.include_router(chat_completion.router)\\n    return app\\n\\n\\ndef init_app() -> FastAPI:\\n    app: FastAPI = pipe(\\n        create_instance(),\\n        init_database,\\n        register_events,\\n        register_middlewares,\\n        register_error_handlers,\\n        register_routers\\n    )\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\factory.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nfrom typing import Callable\\n\\nfrom fastapi import Request, Response\\nfrom starlette.middleware.base import BaseHTTPMiddleware\\nfrom fastapi.applications import FastAPI\\nfrom llm.api.views import error_response\\nimport time\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass RoguLoggerMiddleware(BaseHTTPMiddleware):\\n\\n    # ref: https://github.com/encode/starlette/issues/495\\n    async def _reset_receive(self, request, content):\\n        async def receive():\\n            return content\\n        request._receive = receive\\n\\n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\\n        start_time = time.monotonic()\\n\\n        try:\\n            response = await call_next(request)\\n            processing_time = time.monotonic() - start_time\\n            response.headers[\"x-processing-time\"] = str(round(processing_time*1000, 2))\\n            return response\\n        except Exception as e:\\n            # catch any exception during performing request\\n            logger.exception(e)\\n            return error_response(e)\\n\\n\\ndef register_middlewares(app: FastAPI) -> FastAPI:\\n    app.add_middleware(RoguLoggerMiddleware)\\n    return app\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\middleware.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import re\\nimport http\\nimport json\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import AIException, PlatformModelNotFoundError\\nfrom typing import Dict, List, Optional, Type\\nfrom fastapi.responses import JSONResponse\\n\\n\\ndef _camel_to_snake(name):\\n    name = re.sub(\\'(.)([A-Z][a-z]+)\\', r\\'\\\\1_\\\\2\\', name)\\n    return re.sub(\\'([a-z0-9])([A-Z])\\', r\\'\\\\1_\\\\2\\', name).upper()\\n\\n\\ndef chat_completion_response(response):\\n    output = {\\n        \"uuid\": response.uuid,\\n        \"choices\": [choice.dict() for choice in response.choices],\\n        \"usage\": response.usage.dict()\\n    }\\n\\n    return JSONResponse(\\n        dict(\\n            data=output,\\n            status=\"success\",\\n            message=\"success\"\\n        )\\n    )\\n\\n\\nasync def stream_chat_completion_response(response):\\n    async for row in response:\\n        output = {\\n            \"uuid\": row.uuid,\\n            \"choices\": [choice.dict() for choice in row.choices],\\n            \"usage\": row.usage.dict()\\n        }\\n        yield json.dumps(output) + \"\\\\n\"\\n\\n\\ndef error_response(error: Exception, code: int = 500, descriptions: Optional[List] = None):\\n    if descriptions is None:\\n        descriptions = []\\n\\n    status = http.HTTPStatus(code).name\\n\\n    if isinstance(error, StarletteHTTPException):\\n        name = error.detail.replace(\" \", \"\")\\n        code = error.status_code\\n    else:\\n        name = error.__class__.__name__\\n\\n    if isinstance(error, AIException):\\n        if error.args[2] is not None:\\n            code = error.args[2].status_code\\n            message = str(error.args[2].message)\\n            error_field = _camel_to_snake(error.args[2].__class__.__name__)\\n            if code is None:\\n                code = 500\\n            status = http.HTTPStatus(code).name\\n\\n        else:\\n            message = error.args[1]\\n            error_field = _camel_to_snake(error._error_code.name)\\n            if error._error_code == AIException.ErrorCodes.InvalidRequest:\\n                code = 400\\n                status = http.HTTPStatus(code).name\\n                error_field = _camel_to_snake(openai.error.InvalidRequestError.__name__)\\n\\n        error_message = [\\n                {\"field\": error_field, \"message\": message}\\n            ]\\n    elif isinstance(error, PlatformModelNotFoundError):\\n        error_message = [\\n            {\"field\": error.args[0], \"message\": f\"Not found for {error.args[0]}: {error.args[1]}\"}\\n        ]\\n\\n    else:\\n        error_message = [\\n                {\"field\": desc[0], \"message\": desc[1]} for desc in descriptions\\n        ]\\n\\n    return JSONResponse(\\n        dict(\\n            status=status,\\n            error=_camel_to_snake(name),\\n            code=code,\\n            error_code=error_message,\\n            metadata={}\\n        ),\\n        status_code=code,\\n    )\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.api.factory import init_app\\n\\n\\napp = init_app()\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.routing import APIRouter\\nfrom fastapi.responses import JSONResponse\\nfrom llm.api.views import chat_completion_response, stream_chat_completion_response\\nfrom llm.core.service.dto import ChatCompletionRequest\\nfrom llm.core.service.chat_completion import ChatCompletionService\\nfrom llm.config import CHAT_COMPLETION_CONFIG\\nfrom fastapi.responses import StreamingResponse\\nfrom llm.client.event_tracking.client import EventTrackingApi\\nimport logging\\n\\n\\nrouter = APIRouter()\\nlogger = logging.getLogger(__name__)\\n\\nevent_tracker = EventTrackingApi()\\nactive_models = CHAT_COMPLETION_CONFIG.ACTIVE_MODELS.split(\";\")\\nchat_completion_service = ChatCompletionService(active_models, event_tracker)\\n\\n\\n@router.post(\"/chat/completion\")\\nasync def chat_completion(chat_request: ChatCompletionRequest, request: Request):\\n    raw_response = await chat_completion_service.generate_chat_completion(chat_request)\\n\\n    if not chat_request.stream:\\n        return chat_completion_response(raw_response)\\n    else:\\n        response = stream_chat_completion_response(raw_response)\\n        return StreamingResponse(response, media_type=\"text/event-stream\")\\n\\n\\n@router.post(\"/dummy/completion\")\\nasync def chat_completion_dummy(chat_request: ChatCompletionRequest, request: Request):\\n\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from fastapi import Request\\nfrom fastapi.responses import JSONResponse\\nfrom fastapi.routing import APIRouter\\n\\nrouter = APIRouter()\\n\\n\\n@router.get(\"/_health\")\\nasync def health(request: Request):\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\health.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport aiohttp\\n\\nfrom typing import Tuple, Optional\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass AsyncHttp:\\n\\n    __slots__ = (\"_client\",)\\n\\n    def __init__(self) -> None:\\n        self._client = None\\n\\n    async def open(self) -> aiohttp.ClientSession:\\n        if not self._client:\\n            self._client = aiohttp.ClientSession()\\n            logger.info(\"Async HTTP client session opened\")\\n        return self._client\\n\\n    async def close(self) -> None:\\n        if self._client:\\n            await self._client.close()\\n            logger.info(\"Async HTTP client session closed\")\\n            self._client = None\\n\\n    async def get(self, url, headers: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def get_params(self, url, headers: Optional[dict] = {}, params: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers, params=params) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post(self, url, data: str, headers: Optional[dict] = None) -> Tuple[bytes, int]:\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post_forget(self, url, data: str, headers: Optional[dict] = None):\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers):\\n            pass\\n\\n\\n_async_http_client = AsyncHttp()\\nasync_http_open = _async_http_client.open\\nasync_http_close = _async_http_client.close\\nasync_get = _async_http_client.get\\nasync_get_params = _async_http_client.get_params\\nasync_post = _async_http_client.post\\nasync_post_forget = _async_http_client.post_forget\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import logging\\nimport asyncio\\nimport orjson\\n\\nfrom datetime import datetime, timezone\\nfrom pydantic import BaseModel\\n\\nfrom llm.client.common.async_http import async_post_forget\\nfrom llm.client.event_tracking.entity import (\\n    PostEventTrackingRequestData,\\n    EventLLMChatCompletion\\n)\\nfrom llm.core.service.dto import ChatCompletionRequest, ChatCompletionResponse\\nfrom llm.config import EVENT_TRACKING_CONFIG\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass EventTrackingApi:\\n\\n    def __init__(\\n            self,\\n            endpoint: str = EVENT_TRACKING_CONFIG.ENDPOINT,\\n            active: bool = EVENT_TRACKING_CONFIG.ACTIVE) -> None:\\n\\n        self._endpoint = endpoint\\n        self._active = active\\n\\n    async def log_llm_chat_completion(\\n            self,\\n            chat_completion_request: ChatCompletionRequest,\\n            chat_completion_response: ChatCompletionResponse,\\n            ) -> None:\\n\\n        if len(chat_completion_response.choices) > 0:\\n            if chat_completion_response.choices[0].message.tool_calls is not None:\\n                tool_calls = [x.dict() for x in chat_completion_response.choices[0].message.tool_calls]\\n                completion = orjson.dumps(tool_calls)\\n            else:\\n                completion = chat_completion_response.choices[0].message.content\\n        else:\\n            completion = \"\"\\n\\n        messages = [x.dict() for x in chat_completion_request.messages]\\n\\n        token_usage = {\\n            \"prompt_token\": chat_completion_response.usage.prompt_token,\\n            \"completion_token\": chat_completion_response.usage.completion_token,\\n        }\\n\\n        event = EventLLMChatCompletion(\\n            uuid=chat_completion_response.uuid,\\n            trace_id=chat_completion_request.trace_id,\\n            session_id=chat_completion_request.session_id,\\n            message=orjson.dumps(messages),\\n            task_name=chat_completion_request.task_name,\\n            model=chat_completion_request.model,\\n            platform=chat_completion_request.platform,\\n            completion=completion,\\n            client_name=chat_completion_request.client_name,\\n            token_usage=token_usage,\\n        )\\n\\n        await self._send(\"dsChatbotLlmChatCompletion\", event)\\n\\n    async def _send(self, event_type: str, context: BaseModel, memberId: str = None) -> None:\\n        if self._active:\\n            json_data = PostEventTrackingRequestData(\\n                memberId=memberId,\\n                isLogged=False,\\n                eventType=event_type,\\n                clientTimestamp=datetime.now(timezone.utc).isoformat(),\\n                context=context.model_dump_json()\\n            ).model_dump_json()\\n\\n            headers = {\"content-type\": \"application/json\"}\\n            asyncio.create_task(async_post_forget(self._endpoint, data=json_data, headers=headers))\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from typing import List, Optional, Union, Any, Dict\\nfrom pydantic import BaseModel\\n\\n\\nclass EventLLMChatCompletion(BaseModel):\\n    uuid: str\\n    session_id: str\\n    trace_id: str\\n    platform: str\\n    model: str\\n    message: str\\n    completion: Optional[str] = None\\n    task_name: Optional[str]\\n    client_name: str\\n    token_usage: Dict[str, float]\\n\\n\\nclass PostEventTrackingRequestData(BaseModel):\\n    id: Optional[str] = None\\n    sessionId: Optional[str] = None\\n    cookiesId: Optional[str] = None\\n    deviceId: Optional[str] = None\\n    memberId: Optional[str] = None\\n    isLogged: bool = False\\n    eventType: str\\n    clientDevice: Optional[str] = None\\n    clientUA: Optional[str] = None\\n    appVersion: Optional[str] = None\\n    clientOS: Optional[str] = None\\n    clientOSVersion: Optional[str] = None\\n    clientTimestamp: Optional[str] = None\\n    context: Union[str, Any]\\n    connectionType: Optional[str] = None\\n    source: str = \"backend\"\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from openai import AsyncOpenAI\\nfrom llm.core.service.exceptions import AIException\\n\\n\\nclass OpenAIClient:\\n    def __init__(self):\\n        self.client = AsyncOpenAI()\\n\\n    async def send_chat_request(self, messages, model, request_settings, stream, response_format, tools, client_org):\\n        self.client.organization = client_org\\n        if stream:\\n            stream_options = {\"include_usage\": True}\\n        else:\\n            stream_options = None\\n\\n        try:\\n            response = await self.client.chat.completions.create(\\n                model=model,\\n                messages=messages,\\n                temperature=request_settings.temperature,\\n                max_tokens=request_settings.max_tokens,\\n                top_p=request_settings.top_p,\\n                stream=stream,\\n                stream_options=stream_options,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n        except Exception as ex:\\n            raise AIException(\\n                \"openai\",\\n                \"OpenAI service failed to complete the chat\",\\n                ex\\n            )\\n\\n        return response\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\client.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pydantic import BaseModel\\n\\n\\nclass OpenAIRequestSettings(BaseModel):\\n    temperature: float\\n    max_tokens: int\\n    top_p: float\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.config import VERTEXAI_CONFIG\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel, GenerationConfig, Image\\nfrom google.oauth2.service_account import Credentials\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nfrom llm.client.vertexai.entity import UserContent, SAFETY_SETTING\\nfrom llm.client.common.async_http import async_get\\n\\n\\nclass VertexAIClient:\\n    def __init__(self, location=None, credential_file=None):\\n        if credential_file is None:\\n            credential_file = VERTEXAI_CONFIG.CREDENTIAL_FILE_PATH\\n        else:\\n            credential_file = credential_file\\n        credentials = Credentials.from_service_account_file(credential_file)\\n\\n        if location is None:\\n            location = VERTEXAI_CONFIG.REGION\\n\\n        vertexai.init(\\n            project=VERTEXAI_CONFIG.PROJECT_ID,\\n            location=location,\\n            credentials=credentials\\n        )\\n\\n        self.model_client = self.__load_model()\\n\\n    def __load_model(self):\\n        model_client = {}\\n        for model_id in CHAT_COMPLETION_MODELS.get(\"vertexai\", []):\\n            model_client[model_id] = GenerativeModel(model_id)\\n        return model_client\\n\\n    async def generate_response(\\n        self,\\n        model: str,\\n        user_content: UserContent,\\n        generation_config: GenerationConfig,\\n        stream: bool,\\n        system_message: str = None\\n    ):\\n        if system_message is not None:\\n            model_client = GenerativeModel(model, system_instruction[system_message])\\n        else:\\n            model_client = self.model_client.get(model)\\n\\n        contents = [user_content.text]\\n        if user_content.images is not None:\\n            images = [await self._load_image_from_url(image) for image in user_content.images]\\n            contents.extend(images)\\n\\n        response = await model_client.generate_content_async(\\n            contents=contents,\\n            generation_config=generation_config,\\n            safety_settings=SAFETY_SETTING,\\n            stream=stream\\n        )\\n\\n        return response\\n\\n    async def _get_image_bytes_from_url(self, image_url: str) -> bytes:\\n        response, status = await async_get(image_url)\\n\\n        return response\\n\\n    async def _load_image_from_url(self, image_url: str) -> Image:\\n        image_bytes = await self._get_image_bytes_from_url(image_url)\\n        return Image.from_bytes(image_bytes)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pydantic import BaseModel\\nfrom vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\\nfrom typing import Optional, List\\n\\n\\nclass UserContent(BaseModel):\\n    text: str\\n    images: Optional[List] = None\\n\\n\\nSAFETY_SETTING = {\\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='CHAT_COMPLETION_MODELS = {\\n    \"openai\": [\\n        \"gpt-3.5-turbo\",\\n        \"gpt-3.5-turbo-1106\",\\n        \"gpt-3.5-turbo-0125\",\\n        \"gpt-3.5-turbo-16k\",\\n        \"gpt-4\",\\n        \"gpt-4-turbo\",\\n        \"gpt-4-0125-preview\",\\n        \"gpt-4-1106-preview\",\\n        \"gpt-4o\",\\n        \"ft:gpt-3.5-turbo-0125:ruangguru:intentclass:9Wjgagxi\"\\n    ],\\n    \"vertexai\": [\\n        \"gemini-1.5-flash-001\",\\n        \"gemini-1.5-pro-001\"\\n    ]\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import glob\\nimport yaml\\nimport os\\nimport mmh3\\nfrom llm.utils import generate_uuid\\n\\n\\nSEGMENT_SIZE = 1000\\n\\n\\nclass GeneralAllocation:\\n    def __init__(self, platform_name):\\n        self.config_directory = \"llm/core/allocation/{}/config\".format(platform_name)\\n        self.config = {}\\n        self.__create_config()\\n\\n    def __create_config(self):\\n        directory = glob.glob(self.config_directory + \"/*.yaml\", recursive=True)\\n\\n        for filepath in directory:\\n            config_name = os.path.basename(filepath).split(\".\")[0]\\n            with open(filepath) as f:\\n                config_yaml = yaml.load(f, Loader=yaml.FullLoader)\\n            self.config[config_name] = config_yaml\\n\\n            self.config[config_name] = self.__compute_allocation(config_name)\\n\\n    def get_variant(self, config_name, unit_id=None):\\n        if unit_id is None:\\n            unit_id = generate_uuid()\\n\\n        segment = self.get_segment(unit_id)\\n        if config_name not in self.config.keys():\\n            return None\\n\\n        variants = self.config[config_name][\"variants\"]\\n\\n        for variant in self.config[config_name][\"variants\"]:\\n            if segment < variant[\"offset\"]:\\n                return variant[\"value\"]\\n\\n        return variants[0][\"value\"]\\n\\n    def __compute_allocation(self, config_name):\\n        config = self.config[config_name]\\n        list_variant = []\\n        offset = 0\\n        for row in config[\"variants\"]:\\n            upperbound = int(row[\"weight\"]*SEGMENT_SIZE)\\n            offset = offset + upperbound\\n            list_variant.append(\\n                {\\n                    \"value\": row[\"value\"],\\n                    \"weight\": row[\"weight\"],\\n                    \"offset\": offset\\n                }\\n            )\\n        config[\"variants\"] = list_variant\\n\\n        return config\\n\\n    def get_segment(self, unit_id):\\n        segment = mmh3.hash(unit_id, signed=False) % SEGMENT_SIZE\\n\\n        return segment\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\allocation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.port import AbstractChatCompletionModel\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = False\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        choices = CompletionChoice(\\n            index=0,\\n            message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n        )\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=[choices],\\n            usage={\\n                \"prompt_token\": 100,\\n                \"completion_token\": 100,\\n                \"total_token\": 200\\n            }\\n        )\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        async def post_process_stream():\\n            for i in range(10):\\n                choices = CompletionChoice(\\n                    index=i,\\n                    message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n                )\\n                response = ChatCompletionResponse(\\n                    uuid=generate_uuid(),\\n                    choices=[choices],\\n                    usage={\\n                        \"prompt_token\": 100,\\n                        \"completion_token\": 100,\\n                        \"total_token\": 200\\n                    }\\n                )\\n                yield response\\n\\n        return (row async for row in post_process_stream())\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\dummy\\\\model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import OPENAI_CONFIG\\nfrom llm.core.platform.openai.entity import MODEL_VARIANT, ORG_KEYS\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_org_key(self, model_id):\\n        if not OPENAI_CONFIG.USE_ALLOCATION:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_model = MODEL_VARIANT.get(model_id)\\n        variant_value = self.get_variant(variant_model)\\n\\n        if variant_value is None:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_org_key = ORG_KEYS.get(variant_value, OPENAI_CONFIG.DEFAULT_ORG_KEY)\\n        logger.info(f\"use openai varian for model_id: {model_id}, with value {variant_value}:{variant_org_key}\")\\n\\n        return variant_org_key\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\allocation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.config import OPENAI_CONFIG\\n\\n\\nORG_KEYS = {\\n    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\\n    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\\n    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\\n    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\\n}\\n\\nMODEL_VARIANT = {\\n    \"gpt-3.5-turbo\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\\n    \"gpt-4\": \"gpt-4\",\\n    \"gpt-4-turbo\": \"gpt-4-turbo\",\\n    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\\n    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\\n    \"gpt-4o\": \"gpt-4o\"\\n\\n}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.client.openai.client import OpenAIClient\\nfrom llm.client.openai.entity import OpenAIRequestSettings\\nfrom llm.core.service.port import AbstractChatCompletionModel\\nfrom llm.core.platform.openai.allocation import Allocation\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.client = OpenAIClient()\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"openai\")\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools,\\n    ):\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n\\n        org_key = self.allocation.get_variant_org_key(model)\\n\\n        openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=False,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )\\n\\n        choices = [choice.dict() for choice in openai_response.choices]\\n        choices = [CompletionChoice(**choice) for choice in choices]\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage={\\n                \"prompt_token\": openai_response.usage.prompt_tokens,\\n                \"completion_token\": openai_response.usage.completion_tokens,\\n                \"total_token\": openai_response.usage.total_tokens\\n            }\\n        )\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n        org_key = self.allocation.get_variant_org_key(model)\\n\\n        openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=True,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )\\n\\n        async def post_process_stream(uuid, openai_response):\\n            async for chunk in openai_response:\\n                choices = []\\n                for choice in chunk.choices:\\n                    message_response = MessageDataResponse(\\n                        content=choice.delta.content,\\n                        role=choice.delta.role\\n                    )\\n                    _choices = CompletionChoice(\\n                        index=choice.index, message=message_response, finish_reason=choice.finish_reason\\n                    )\\n                    choices.append(_choices)\\n\\n                if chunk.usage is not None:\\n                    usage = {\\n                        \"prompt_token\": chunk.usage.prompt_tokens,\\n                        \"completion_token\": chunk.usage.completion_tokens,\\n                        \"total_token\": chunk.usage.total_tokens\\n                    }\\n                else:\\n                    usage = {\\n                        \"prompt_token\": 0,\\n                        \"completion_token\": 0,\\n                        \"total_token\": 0\\n                    }\\n\\n                response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, openai_response))\\n        return response\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import VERTEXAI_CONFIG\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_region(self):\\n        if not VERTEXAI_CONFIG.USE_ALLOCATION:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        variant_value = self.get_variant(\"region\")\\n\\n        if variant_value is None:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        logger.info(f\"use vertex ai region with value {variant_value}\")\\n\\n        return variant_value\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\allocation.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='REGION_VARIANT = [\\n    \"us-central1\",\\n    \"us-east4\",\\n    \"us-west1\",\\n    \"us-west4\",\\n    \"northamerica-northeast1\",\\n    \"europe-west9\",\\n    \"europe-west2\",\\n    \"europe-west3\",\\n    \"europe-west4\",\\n    \"europe-west9\",\\n    \"asia-northeast1\",\\n    \"asia-northeast3\",\\n    \"asia-southeast1\"\\n]\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.service.port import AbstractChatCompletionModel\\nfrom vertexai.generative_models import Image, GenerationConfig\\nfrom llm.client.vertexai.client import VertexAIClient\\nfrom llm.client.vertexai.entity import UserContent\\nfrom llm.core.platform.vertexai.allocation import Allocation\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.core.platform.vertexai.entity import REGION_VARIANT\\n\\n\\nclass Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"vertexai\")\\n        self.clients = self.__load_client()\\n\\n    def __load_client(self):\\n        clients = {}\\n        for region in REGION_VARIANT:\\n            clients[region] = VertexAIClient(location=region)\\n        return clients\\n\\n    def __get_client(self):\\n        varian_region = self.allocation.get_variant_region()\\n        return self.clients.get(varian_region)\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()\\n\\n        # TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=False,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )\\n\\n        choices = self._parse_choices(vertexai_response.candidates)\\n\\n        usage = {\\n            \"prompt_token\": vertexai_response.usage_metadata.prompt_token_count,\\n            \"completion_token\": vertexai_response.usage_metadata.candidates_token_count,\\n            \"total_token\": vertexai_response.usage_metadata.total_token_count\\n        }\\n\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage=usage\\n        )\\n\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()\\n\\n        # TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=True,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )\\n\\n        async def post_process_stream(uuid, vertexai_response):\\n            async for chunk in vertexai_response:\\n                if chunk is None:\\n                    continue\\n                choices = []\\n                index = 0\\n                for row in chunk.candidates:\\n                    if len(row.content.parts) > 0:\\n                        choice = CompletionChoice(\\n                            index=index,\\n                            message=MessageDataResponse(\\n                                role=\"assistant\",\\n                                content=row.content.parts[0].text\\n                            ),\\n                            finish_reason=row.finish_reason.name\\n                        )\\n                        choices.append(choice)\\n                        index += 1\\n                usage = {\\n                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\\n                    \"completion_token\": chunk.usage_metadata.candidates_token_count,\\n                    \"total_token\": chunk.usage_metadata.total_token_count\\n                }\\n\\n                response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, vertexai_response))\\n\\n        return response\\n\\n    def _get_system_message(self, messages):\\n        first_message = messages[0]\\n        if first_message.role == \"system\":\\n            return first_message.content\\n        else:\\n            return None\\n\\n    def _get_user_content(self, messages):\\n        # currently only use last user message as user content\\n        last_message = messages[-1]\\n        text = None\\n        images = []\\n        if last_message.role == \"user\":\\n            content = last_message.content\\n            if isinstance(content, list):\\n                for row in content:\\n                    if row.type == \"text\":\\n                        text = row.text\\n                    elif row.type == \"image_url\":\\n                        image_url = row.image_url.get(\"url\")\\n                        images.append(image_url)\\n            else:\\n                text = content\\n\\n        if len(images) == 0:\\n            images = None\\n\\n        return UserContent(text=text, images=images)\\n\\n    def _construct_generation_config(self, config: ChatCompletionSetting, response_format):\\n        if response_format is not None:\\n            if response_format.type == \"json_object\":\\n                mime_type = \"application/json\"\\n            else:\\n                mime_type = \"text/plain\"\\n        else:\\n            mime_type = \"text/plain\"\\n\\n        generation_config = GenerationConfig(\\n            temperature=config.temperature,\\n            max_output_tokens=config.max_token,\\n            response_mime_type=mime_type\\n        )\\n\\n        return generation_config\\n\\n    def _parse_choices(self, response_candidates):\\n        index = 0\\n        choices = []\\n        for row in response_candidates:\\n            if len(row.content.parts)>0:\\n                messages = MessageDataResponse(\\n                    role=\"assistant\",\\n                    content=row.content.parts[0].text\\n                )\\n            else:\\n                messages = MessageDataResponse()\\n\\n            choice = CompletionChoice(\\n                index=index,\\n                message=messages,\\n                finish_reason=row.finish_reason.name\\n            )\\n            choices.append(choice)\\n            index += 1\\n\\n            return choices\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from llm.core.service.dto import ChatCompletionRequest\\nfrom importlib import import_module\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nimport copy\\n\\n\\nclass ChatCompletionService:\\n    def __init__(self, active_models, event_tracker):\\n        self.active_models = active_models\\n        self.event_tracker = event_tracker\\n        self.__models = self.__load_model()\\n\\n    def __load_model(self):\\n        models = {}\\n        for platform in self.active_models:\\n            Model = getattr(\\n                import_module(\"llm.core.platform.{}.model\".format(platform)),\\n                \"Model\",\\n            )\\n            models[platform] = Model()\\n        return models\\n\\n    def __validate_platform_and_model(self, platform: str, model: str):\\n        platform_model = self.__models.get(platform)\\n        if not platform_model:\\n            raise PlatformModelNotFoundError(\"platform\", platform)\\n\\n        if platform_model.require_model_validation:\\n            if model not in CHAT_COMPLETION_MODELS.get(platform, []):\\n                raise PlatformModelNotFoundError(\"model\", model)\\n\\n    async def generate_chat_completion(self, request: ChatCompletionRequest):\\n        model_id = request.model\\n        message_data = request.messages\\n        config = request.config\\n        response_format = request.response_format\\n        tools = request.tools\\n\\n        self.__validate_platform_and_model(request.platform, model_id)\\n\\n        if not request.stream:\\n            completion_response = await self.__models[request.platform].generate_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n            await self.event_tracker.log_llm_chat_completion(request, completion_response)\\n\\n            return completion_response\\n\\n        else:\\n            completion_response = await self.__models[request.platform].generate_stream_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n\\n            return (row async for row in self.__send_tracker_stream(request, completion_response))\\n\\n    async def __send_tracker_stream(self, request: ChatCompletionRequest, response):\\n        full_completion_content = \"\"\\n        full_completion_role = None\\n        full_response = None\\n        full_usage = None\\n        async for row in response:\\n            yield row\\n\\n            if len(row.choices) > 0:\\n                if full_response is None:\\n                    full_response = copy.deepcopy(row)\\n                if row.choices[0].message.content is not None:\\n                    full_completion_content += row.choices[0].message.content\\n                if row.choices[0].message.role is not None:\\n                    full_completion_role = row.choices[0].message.role\\n            if row.usage.prompt_token > 0:\\n                full_usage = row.usage\\n\\n        full_response.choices[0].message.content = full_completion_content\\n        full_response.choices[0].message.role = full_completion_role\\n        full_response.usage = full_usage\\n\\n        await self.event_tracker.log_llm_chat_completion(request, full_response)\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pydantic import BaseModel\\nfrom typing import Optional, List, Dict, Union, Any\\n\\n\\nclass ContentData(BaseModel):\\n    type: str\\n    text: Optional[str] = None\\n    image_url: Optional[Dict[str, str]] = None\\n\\n\\nclass FunctionParameterData(BaseModel):\\n    type: str\\n    properties: Dict[Any, Any]\\n    required: List[str]\\n\\n\\nclass FunctionData(BaseModel):\\n    name: str\\n    description: str\\n    parameters: FunctionParameterData\\n\\n\\nclass ToolsDataRequest(BaseModel):\\n    type: str\\n    function: Optional[FunctionData]\\n\\n\\nclass MessageDataRequest(BaseModel):\\n    role: str\\n    content: Union[str, List[ContentData]]\\n\\n\\nclass ChatCompletionSetting(BaseModel):\\n    max_token: Optional[int] = 256\\n    temperature: Optional[float] = 0.7\\n    top_p: Optional[float] = 1\\n    top_k: Optional[float] = 1\\n\\n\\nclass ResponseFormatRequest(BaseModel):\\n    type: Optional[str] = None\\n\\n\\nclass ChatCompletionRequest(BaseModel):\\n    platform: str\\n    model: str\\n    messages: List[MessageDataRequest]\\n    tools: Optional[List[ToolsDataRequest]] = None\\n    config: Optional[ChatCompletionSetting] = {}\\n    stream: bool = False\\n    task_name: Optional[str] = None\\n    client_name: str\\n    trace_id: str\\n    session_id: Optional[str] = \"\"\\n    response_format: Optional[ResponseFormatRequest] = None\\n\\n\\nclass TokenUsage(BaseModel):\\n    prompt_token: Optional[int]\\n    completion_token: Optional[int]\\n    total_token: Optional[int]\\n\\n\\nclass FunctionResponse(BaseModel):\\n    arguments: str\\n    name: str\\n\\n\\nclass ToolCallResponse(BaseModel):\\n    function: Optional[FunctionResponse] = None\\n    type: str\\n\\n\\nclass MessageDataResponse(BaseModel):\\n    role: Optional[str] = None\\n    content: Optional[str] = None\\n    tool_calls: Optional[List[ToolCallResponse]] = None\\n\\n\\nclass CompletionChoice(BaseModel):\\n    index: int\\n    message: MessageDataResponse\\n    finish_reason: Optional[str] = None\\n\\n\\nclass ChatCompletionResponse(BaseModel):\\n    uuid: str\\n    choices: Optional[List[CompletionChoice]] = []\\n    usage: Optional[TokenUsage] = {}\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\dto.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='class PlatformModelNotFoundError(Exception):\\n    def __init__(self, field, value):\\n        self.field = field\\n        self.value = value\\n\\n\\nclass AIException(Exception):\\n    def __init__(self, name, message, errors):\\n        self.name = name\\n        self.message = message\\n        self.errors = errors\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\exceptions.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import abc\\n\\n\\nclass AbstractChatCompletionModel(abc.ABC):\\n    def __init__(self) -> None:\\n        pass\\n\\n    @abc.abstractmethod\\n    def generate_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError\\n\\n    @abc.abstractmethod\\n    def generate_stream_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\port.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import pytest\\nfrom fastapi.testclient import TestClient\\nfrom pytest import fixture\\n\\n\\n@pytest.fixture(scope=\"package\")\\ndef client():\\n    from llm.api import app\\n    with TestClient(app) as client:\\n        yield client\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\conftest.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='from pprint import pprint\\nimport requests\\nimport json\\n\\n\\ndef test_chat_completion(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"dummy\",\\n        \"model\": \"gpt-4-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n\\n\\ndef test_chat_completion_openai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n\\n\\ndef test_chat_completion_openai_with_tools(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"What\\'s the weather like in San franciso, tokyo and paris\"}\\n        ],\\n        \"tools\": [{\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"get_current_weather\",\\n                \"description\": \"Get the current weather in a given location\",\\n                \"parameters\": {\\n                    \"type\": \"object\",\\n                    \"properties\": {\\n                        \"location\": {\\n                            \"type\": \"string\",\\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\\n                            },\\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\\n                    },\\n                    \"required\": [\"location\"]\\n                }\\n            }\\n        }],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n    assert len(response_data[\"choices\"][0][\"message\"][\"tool_calls\"]) > 0\\n\\n\\ndef test_chat_completion_vertexai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"vertexai\",\\n        \"model\": \"gemini-1.5-flash-001\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\__init__.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1 = loader1.load()\n",
    "docs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                                chunk_size = 1000,\n",
    "                                                                chunk_overlap = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='from pydantic_settings import BaseSettings\\n\\n\\nclass GunicornConfig(BaseSettings):\\n    NUM_WORKERS: int = 1\\n    NUM_THREADS: int = 1\\n    TIMEOUT: int = 30\\n    LOG_LEVEL: str = \"INFO\"\\n    LOG_DEBUG: str = \"DEBUG\"\\n    LOG_ERROR: str = \"ERROR\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_API_GUNICORN_\"\\n\\n\\nGUNICORN_CONFIG = GunicornConfig()', metadata={'source': 'test_repo\\\\rg-llm\\\\config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import logging\\n\\n\\nclass RoguLogFilter(logging.Filter):\\n    \"\"\"\\n    Custom filter to exclude health check\\n    \"\"\"\\n\\n    def filter(self, record):\\n        try:\\n            uri = str(record.args[2])\\n        except Exception as e:\\n            uri = None\\n        return uri != \"/_health\"', metadata={'source': 'test_repo\\\\rg-llm\\\\custom_logging.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class RoguFormatter(logging.Formatter):\\n    \"\"\"\\n    RG Logger Formatter\\n    \"\"\"\\n\\n    datefmt = \"%Y-%m-%dT%H:%M:%SZ\"\\n    fmt = \\'RG_LOGGER: time=\"{asctime}\" name={name} lineno={lineno} level={levelname} msg=\"{msg}\" {extra}\\'\\n\\n    def format(self, record):\\n\\n        extra = []\\n        if record.exc_info:\\n            extra_str = f\\'exc_info=\"{self.formatException(record.exc_info)}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_body\"):\\n            extra_str = f\\'request_body=\"{record.request_body}\"\\'\\n            extra.append(extra_str)\\n\\n        if hasattr(record, \"request_path\"):\\n            extra_str = f\"request_path={record.request_path}\"\\n            extra.append(extra_str)', metadata={'source': 'test_repo\\\\rg-llm\\\\custom_logging.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='extra_str = \" \".join(extra)\\n        message = self.__class__.fmt.format(\\n            asctime=self.formatTime(record, self.__class__.datefmt),\\n            name=record.name,\\n            lineno=record.lineno,\\n            levelname=record.levelname.lower(),\\n            msg=self._one_liner(record.getMessage()),\\n            extra=self._one_liner(extra_str)\\n        )\\n\\n        return message\\n\\n    def formatException(self, exc_info):\\n        result = super().formatException(exc_info)\\n        return self._one_liner(result)\\n\\n    def _one_liner(self, msg):\\n        return msg.replace(\"\\\\n\", \"\\\\t|\\\\t\")', metadata={'source': 'test_repo\\\\rg-llm\\\\custom_logging.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import threading\\nimport sys\\nimport traceback\\nfrom logging.config import dictConfig\\nfrom config import GUNICORN_CONFIG\\n\\n\\nworkers = GUNICORN_CONFIG.NUM_WORKERS\\nthreads = GUNICORN_CONFIG.NUM_THREADS\\ntimeout = GUNICORN_CONFIG.TIMEOUT\\nworker_class = \"uvicorn.workers.UvicornWorker\"\\nbind = \\'0.0.0.0:80\\'', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='logconfig_dict = {\\n        \"version\": 1,\\n        \"disable_existing_loggers\": True,\\n        \"formatters\": {\\n                \"custom\": {\\n                    \"()\": \"custom_logging.RoguFormatter\"\\n                }\\n        },\\n        \"filters\": {\\n            \"exclude_health_check\": {\\n                \"()\": \"custom_logging.RoguLogFilter\"\\n            }\\n        },\\n        \"handlers\": {\\n            \"access_console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\",\\n                \"filters\": [\"exclude_health_check\"]\\n            },\\n            \"console\": {\\n                \"class\": \"logging.StreamHandler\",\\n                \"formatter\": \"custom\"\\n            },\\n        },\\n        \"loggers\": {\\n            \"\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"elasticsearch\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='\"handlers\": [\"console\"]\\n            },\\n            \"openai\": {\\n                \"level\": GUNICORN_CONFIG.LOG_DEBUG,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"llm\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"propagate\": False,\\n                \"handlers\": [\"console\"]\\n            },\\n            \"gunicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.access\"\\n            },\\n            \"gunicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"gunicorn.error\"\\n            },\\n            \"uvicorn.access\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"access_console\"],', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='\"propagate\": False,\\n                \"qualname\": \"uvicorn.access\"\\n            },\\n            \"uvicorn.error\": {\\n                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\\n                \"handlers\": [\"console\"],\\n                \"propagate\": False,\\n                \"qualname\": \"uvicorn.error\"\\n            }\\n        }\\n    }\\ndictConfig(logconfig_dict)', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def post_fork(server, worker):\\n    server.log.info(\"Worker spawned (pid: %s)\", worker.pid)\\n\\n\\ndef pre_fork(server, worker):\\n    pass\\n\\n\\ndef post_worker_init(worker):\\n    pass\\n\\n\\ndef pre_exec(server):\\n    server.log.info(\"Forked child, re-executing.\")\\n\\n\\ndef worker_int(worker):\\n    worker.log.info(\"worker received INT or QUIT signal\")\\n\\n    # get traceback info\\n    id2name = dict([(th.ident, th.name) for th in threading.enumerate()])\\n    code = []\\n    for threadId, stack in sys._current_frames().items():\\n        code.append(\"\\\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\\n        for filename, lineno, name, line in traceback.extract_stack(stack):\\n            code.append(\\'File: \"%s\", line %d, in %s\\' % (filename, lineno, name))\\n            if line:\\n                code.append(\"  %s\" % (line.strip()))\\n    worker.log.debug(\"\\\\n\".join(code))\\n\\n\\ndef worker_abort(worker):\\n    worker.log.info(\"worker received SIGABRT signal\")', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import openai\\nimport warnings\\nfrom pydantic_settings import BaseSettings\\nimport os\\n\\n\\nwarnings.filterwarnings(\\'ignore\\')\\n\\n\\nclass ChatCompletionConfig(BaseSettings):\\n    ACTIVE_MODELS: str = \"dummy;openai;vertexai\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_CHAT_COMPLETION_CONFIG_\"\\n\\n\\nclass OpenAIConfig(BaseSettings):\\n    DEFAULT_ORG_KEY: str = \"\"\\n    USE_ALLOCATION: bool = False\\n    ORG_KEY_1: str = \"\"\\n    ORG_KEY_2: str = \"\"\\n    ORG_KEY_3: str = \"\"\\n    ORG_KEY_4: str = \"\"\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_OPENAI_CONFIG_\"', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class VertexAIConfig(BaseSettings):\\n    PROJECT_ID: str = \"silicon-airlock-153323\"\\n    REGION: str = \"asia-southeast1\"\\n    CREDENTIAL_FILE_CONTENT: str = \"\"\\n    CREDENTIAL_FILE_PATH: str = os.path.join(os.getcwd(), \"sa-ai-platform-user.json\")\\n    USE_ALLOCATION: bool = False\\n\\n    def get_secret(self):\\n        if len(self.CREDENTIAL_FILE_CONTENT):\\n            with open(self.CREDENTIAL_FILE_PATH, \"w\") as f:\\n                f.write(self.CREDENTIAL_FILE_CONTENT)\\n\\n    class Config:\\n        env_prefix = \"RG_LLM_VERTEXAI_CONFIG_\"\\n\\n\\nclass EventTrackingConfig(BaseSettings):\\n    ACTIVE: bool = False\\n    ENDPOINT: str = \"http://rg-event-tracking-api.staging.svc.cluster.local/api/v3/event/tracker\"\\n\\n    class Config:\\n        env_prefix = \"RG_DS_CHATBOT_API_TRACKING_\"\\n\\n\\nOPENAI_CONFIG = OpenAIConfig()\\nVERTEXAI_CONFIG = VertexAIConfig()\\nVERTEXAI_CONFIG.get_secret()\\n\\nCHAT_COMPLETION_CONFIG = ChatCompletionConfig()\\nEVENT_TRACKING_CONFIG = EventTrackingConfig()', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\config.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from uuid import uuid4\\n\\n\\ndef generate_uuid():\\n    return str(uuid4())\\n\\n\\ndef pipe(data, *funcs):\\n    \"\"\" Pipe a value through a sequence of functions\\n\\n    I.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\\n\\n    We think of the value as progressing through a pipe of several\\n    transformations, much like pipes in UNIX\\n\\n    ``$ cat data | f | g | h``\\n\\n    >>> double = lambda i: 2 * i\\n    >>> pipe(3, double, str)\\n    \\'6\\'\\n\\n    See Also:\\n        compose\\n        thread_first\\n        thread_last\\n    \"\"\"\\n    for func in funcs:\\n        data = func(data)\\n    return data', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\utils.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from fastapi import Request\\nfrom fastapi.applications import FastAPI\\nfrom fastapi.exceptions import RequestValidationError\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError, AIException\\nfrom llm.api.views import error_response\\nfrom typing import Union\\n\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nasync def request_validation_handler(_: Request, error: RequestValidationError):\\n    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\\n                         for e in error.errors()]\\n    return error_response(error, 400, validation_errors)\\n\\n\\nasync def http_exception_handler(_, error: StarletteHTTPException):\\n    return error_response(error)\\n\\n\\nasync def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\\n    logger.error(f\"Not Found for {error.field}: {error.value}\")\\n    return error_response(error, 400)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\error_handlers.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def ai_exception_handler(_, error):\\n    logger.error(f\"LLM error with detail: {error}\")\\n    return error_response(error, 500)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\error_handlers.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def register_error_handlers(app: FastAPI) -> FastAPI:\\n    app.exception_handler(RequestValidationError)(request_validation_handler)\\n    app.exception_handler(StarletteHTTPException)(http_exception_handler)\\n    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\\n    app.exception_handler(AIException)(ai_exception_handler)\\n    return app', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\error_handlers.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from fastapi.applications import FastAPI\\nfrom llm.client.common.async_http import async_http_open, async_http_close\\n\\n\\nasync def startup() -> None:\\n    await async_http_open()\\n\\n\\nasync def shutdown() -> None:\\n    await async_http_close()\\n\\n\\ndef register_events(app: FastAPI) -> FastAPI:\\n    app.add_event_handler(\"startup\", startup)\\n    app.add_event_handler(\"shutdown\", shutdown)\\n    return app', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\events.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from fastapi.applications import FastAPI\\n\\nfrom llm.api.routers import health\\nfrom llm.api.routers import chat_completion\\nfrom llm.api.error_handlers import register_error_handlers\\nfrom llm.api.events import register_events\\nfrom llm.api.middleware import register_middlewares\\nfrom llm.utils import pipe\\n\\n\\ndef create_instance() -> FastAPI:\\n    return FastAPI()\\n\\n\\ndef init_database(app: FastAPI) -> FastAPI:\\n    return app\\n\\n\\ndef register_routers(app: FastAPI) -> FastAPI:\\n    app.include_router(health.router)\\n    app.include_router(chat_completion.router)\\n    return app\\n\\n\\ndef init_app() -> FastAPI:\\n    app: FastAPI = pipe(\\n        create_instance(),\\n        init_database,\\n        register_events,\\n        register_middlewares,\\n        register_error_handlers,\\n        register_routers\\n    )\\n    return app', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\factory.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import logging\\nfrom typing import Callable\\n\\nfrom fastapi import Request, Response\\nfrom starlette.middleware.base import BaseHTTPMiddleware\\nfrom fastapi.applications import FastAPI\\nfrom llm.api.views import error_response\\nimport time\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\middleware.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class RoguLoggerMiddleware(BaseHTTPMiddleware):\\n\\n    # ref: https://github.com/encode/starlette/issues/495\\n    async def _reset_receive(self, request, content):\\n        async def receive():\\n            return content\\n        request._receive = receive\\n\\n    async def dispatch(self, request: Request, call_next: Callable) -> Response:\\n        start_time = time.monotonic()\\n\\n        try:\\n            response = await call_next(request)\\n            processing_time = time.monotonic() - start_time\\n            response.headers[\"x-processing-time\"] = str(round(processing_time*1000, 2))\\n            return response\\n        except Exception as e:\\n            # catch any exception during performing request\\n            logger.exception(e)\\n            return error_response(e)\\n\\n\\ndef register_middlewares(app: FastAPI) -> FastAPI:\\n    app.add_middleware(RoguLoggerMiddleware)\\n    return app', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\middleware.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content=\"import re\\nimport http\\nimport json\\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\\nfrom llm.core.service.exceptions import AIException, PlatformModelNotFoundError\\nfrom typing import Dict, List, Optional, Type\\nfrom fastapi.responses import JSONResponse\\n\\n\\ndef _camel_to_snake(name):\\n    name = re.sub('(.)([A-Z][a-z]+)', r'\\\\1_\\\\2', name)\\n    return re.sub('([a-z0-9])([A-Z])', r'\\\\1_\\\\2', name).upper()\", metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def chat_completion_response(response):\\n    output = {\\n        \"uuid\": response.uuid,\\n        \"choices\": [choice.dict() for choice in response.choices],\\n        \"usage\": response.usage.dict()\\n    }\\n\\n    return JSONResponse(\\n        dict(\\n            data=output,\\n            status=\"success\",\\n            message=\"success\"\\n        )\\n    )\\n\\n\\nasync def stream_chat_completion_response(response):\\n    async for row in response:\\n        output = {\\n            \"uuid\": row.uuid,\\n            \"choices\": [choice.dict() for choice in row.choices],\\n            \"usage\": row.usage.dict()\\n        }\\n        yield json.dumps(output) + \"\\\\n\"', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def error_response(error: Exception, code: int = 500, descriptions: Optional[List] = None):\\n    if descriptions is None:\\n        descriptions = []\\n\\n    status = http.HTTPStatus(code).name\\n\\n    if isinstance(error, StarletteHTTPException):\\n        name = error.detail.replace(\" \", \"\")\\n        code = error.status_code\\n    else:\\n        name = error.__class__.__name__\\n\\n    if isinstance(error, AIException):\\n        if error.args[2] is not None:\\n            code = error.args[2].status_code\\n            message = str(error.args[2].message)\\n            error_field = _camel_to_snake(error.args[2].__class__.__name__)\\n            if code is None:\\n                code = 500\\n            status = http.HTTPStatus(code).name', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='else:\\n            message = error.args[1]\\n            error_field = _camel_to_snake(error._error_code.name)\\n            if error._error_code == AIException.ErrorCodes.InvalidRequest:\\n                code = 400\\n                status = http.HTTPStatus(code).name\\n                error_field = _camel_to_snake(openai.error.InvalidRequestError.__name__)\\n\\n        error_message = [\\n                {\"field\": error_field, \"message\": message}\\n            ]\\n    elif isinstance(error, PlatformModelNotFoundError):\\n        error_message = [\\n            {\"field\": error.args[0], \"message\": f\"Not found for {error.args[0]}: {error.args[1]}\"}\\n        ]\\n\\n    else:\\n        error_message = [\\n                {\"field\": desc[0], \"message\": desc[1]} for desc in descriptions\\n        ]', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='return JSONResponse(\\n        dict(\\n            status=status,\\n            error=_camel_to_snake(name),\\n            code=code,\\n            error_code=error_message,\\n            metadata={}\\n        ),\\n        status_code=code,\\n    )', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\views.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.api.factory import init_app\\n\\n\\napp = init_app()', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from fastapi import Request\\nfrom fastapi.routing import APIRouter\\nfrom fastapi.responses import JSONResponse\\nfrom llm.api.views import chat_completion_response, stream_chat_completion_response\\nfrom llm.core.service.dto import ChatCompletionRequest\\nfrom llm.core.service.chat_completion import ChatCompletionService\\nfrom llm.config import CHAT_COMPLETION_CONFIG\\nfrom fastapi.responses import StreamingResponse\\nfrom llm.client.event_tracking.client import EventTrackingApi\\nimport logging\\n\\n\\nrouter = APIRouter()\\nlogger = logging.getLogger(__name__)\\n\\nevent_tracker = EventTrackingApi()\\nactive_models = CHAT_COMPLETION_CONFIG.ACTIVE_MODELS.split(\";\")\\nchat_completion_service = ChatCompletionService(active_models, event_tracker)\\n\\n\\n@router.post(\"/chat/completion\")\\nasync def chat_completion(chat_request: ChatCompletionRequest, request: Request):\\n    raw_response = await chat_completion_service.generate_chat_completion(chat_request)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='if not chat_request.stream:\\n        return chat_completion_response(raw_response)\\n    else:\\n        response = stream_chat_completion_response(raw_response)\\n        return StreamingResponse(response, media_type=\"text/event-stream\")\\n\\n\\n@router.post(\"/dummy/completion\")\\nasync def chat_completion_dummy(chat_request: ChatCompletionRequest, request: Request):\\n\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from fastapi import Request\\nfrom fastapi.responses import JSONResponse\\nfrom fastapi.routing import APIRouter\\n\\nrouter = APIRouter()\\n\\n\\n@router.get(\"/_health\")\\nasync def health(request: Request):\\n    return JSONResponse(dict(status=\"OK\"), status_code=200)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\api\\\\routers\\\\health.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import logging\\nimport aiohttp\\n\\nfrom typing import Tuple, Optional\\n\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class AsyncHttp:\\n\\n    __slots__ = (\"_client\",)\\n\\n    def __init__(self) -> None:\\n        self._client = None\\n\\n    async def open(self) -> aiohttp.ClientSession:\\n        if not self._client:\\n            self._client = aiohttp.ClientSession()\\n            logger.info(\"Async HTTP client session opened\")\\n        return self._client\\n\\n    async def close(self) -> None:\\n        if self._client:\\n            await self._client.close()\\n            logger.info(\"Async HTTP client session closed\")\\n            self._client = None\\n\\n    async def get(self, url, headers: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def get_params(self, url, headers: Optional[dict] = {}, params: Optional[dict] = {}) -> Tuple[bytes, int]:\\n        session = await self.open()\\n        async with session.get(url, headers=headers, params=params) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post(self, url, data: str, headers: Optional[dict] = None) -> Tuple[bytes, int]:\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers) as r:\\n            content = await r.read()\\n            return content, r.status\\n\\n    async def post_forget(self, url, data: str, headers: Optional[dict] = None):\\n        if headers is None:\\n            headers = {\"content-type\": \"application/json\"}\\n\\n        session = await self.open()\\n        async with session.post(url, data=data, headers=headers):\\n            pass', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='_async_http_client = AsyncHttp()\\nasync_http_open = _async_http_client.open\\nasync_http_close = _async_http_client.close\\nasync_get = _async_http_client.get\\nasync_get_params = _async_http_client.get_params\\nasync_post = _async_http_client.post\\nasync_post_forget = _async_http_client.post_forget', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\common\\\\async_http.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import logging\\nimport asyncio\\nimport orjson\\n\\nfrom datetime import datetime, timezone\\nfrom pydantic import BaseModel\\n\\nfrom llm.client.common.async_http import async_post_forget\\nfrom llm.client.event_tracking.entity import (\\n    PostEventTrackingRequestData,\\n    EventLLMChatCompletion\\n)\\nfrom llm.core.service.dto import ChatCompletionRequest, ChatCompletionResponse\\nfrom llm.config import EVENT_TRACKING_CONFIG\\n\\n\\nlogger = logging.getLogger(__name__)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class EventTrackingApi:\\n\\n    def __init__(\\n            self,\\n            endpoint: str = EVENT_TRACKING_CONFIG.ENDPOINT,\\n            active: bool = EVENT_TRACKING_CONFIG.ACTIVE) -> None:\\n\\n        self._endpoint = endpoint\\n        self._active = active\\n\\n    async def log_llm_chat_completion(\\n            self,\\n            chat_completion_request: ChatCompletionRequest,\\n            chat_completion_response: ChatCompletionResponse,\\n            ) -> None:\\n\\n        if len(chat_completion_response.choices) > 0:\\n            if chat_completion_response.choices[0].message.tool_calls is not None:\\n                tool_calls = [x.dict() for x in chat_completion_response.choices[0].message.tool_calls]\\n                completion = orjson.dumps(tool_calls)\\n            else:\\n                completion = chat_completion_response.choices[0].message.content\\n        else:\\n            completion = \"\"\\n\\n        messages = [x.dict() for x in chat_completion_request.messages]', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='token_usage = {\\n            \"prompt_token\": chat_completion_response.usage.prompt_token,\\n            \"completion_token\": chat_completion_response.usage.completion_token,\\n        }\\n\\n        event = EventLLMChatCompletion(\\n            uuid=chat_completion_response.uuid,\\n            trace_id=chat_completion_request.trace_id,\\n            session_id=chat_completion_request.session_id,\\n            message=orjson.dumps(messages),\\n            task_name=chat_completion_request.task_name,\\n            model=chat_completion_request.model,\\n            platform=chat_completion_request.platform,\\n            completion=completion,\\n            client_name=chat_completion_request.client_name,\\n            token_usage=token_usage,\\n        )\\n\\n        await self._send(\"dsChatbotLlmChatCompletion\", event)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def _send(self, event_type: str, context: BaseModel, memberId: str = None) -> None:\\n        if self._active:\\n            json_data = PostEventTrackingRequestData(\\n                memberId=memberId,\\n                isLogged=False,\\n                eventType=event_type,\\n                clientTimestamp=datetime.now(timezone.utc).isoformat(),\\n                context=context.model_dump_json()\\n            ).model_dump_json()\\n\\n            headers = {\"content-type\": \"application/json\"}\\n            asyncio.create_task(async_post_forget(self._endpoint, data=json_data, headers=headers))', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from typing import List, Optional, Union, Any, Dict\\nfrom pydantic import BaseModel\\n\\n\\nclass EventLLMChatCompletion(BaseModel):\\n    uuid: str\\n    session_id: str\\n    trace_id: str\\n    platform: str\\n    model: str\\n    message: str\\n    completion: Optional[str] = None\\n    task_name: Optional[str]\\n    client_name: str\\n    token_usage: Dict[str, float]\\n\\n\\nclass PostEventTrackingRequestData(BaseModel):\\n    id: Optional[str] = None\\n    sessionId: Optional[str] = None\\n    cookiesId: Optional[str] = None\\n    deviceId: Optional[str] = None\\n    memberId: Optional[str] = None\\n    isLogged: bool = False\\n    eventType: str\\n    clientDevice: Optional[str] = None\\n    clientUA: Optional[str] = None\\n    appVersion: Optional[str] = None\\n    clientOS: Optional[str] = None\\n    clientOSVersion: Optional[str] = None\\n    clientTimestamp: Optional[str] = None\\n    context: Union[str, Any]\\n    connectionType: Optional[str] = None\\n    source: str = \"backend\"', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\event_tracking\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from openai import AsyncOpenAI\\nfrom llm.core.service.exceptions import AIException', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class OpenAIClient:\\n    def __init__(self):\\n        self.client = AsyncOpenAI()\\n\\n    async def send_chat_request(self, messages, model, request_settings, stream, response_format, tools, client_org):\\n        self.client.organization = client_org\\n        if stream:\\n            stream_options = {\"include_usage\": True}\\n        else:\\n            stream_options = None', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='try:\\n            response = await self.client.chat.completions.create(\\n                model=model,\\n                messages=messages,\\n                temperature=request_settings.temperature,\\n                max_tokens=request_settings.max_tokens,\\n                top_p=request_settings.top_p,\\n                stream=stream,\\n                stream_options=stream_options,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n        except Exception as ex:\\n            raise AIException(\\n                \"openai\",\\n                \"OpenAI service failed to complete the chat\",\\n                ex\\n            )\\n\\n        return response', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from pydantic import BaseModel\\n\\n\\nclass OpenAIRequestSettings(BaseModel):\\n    temperature: float\\n    max_tokens: int\\n    top_p: float', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\openai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.config import VERTEXAI_CONFIG\\nimport vertexai\\nfrom vertexai.generative_models import GenerativeModel, GenerationConfig, Image\\nfrom google.oauth2.service_account import Credentials\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nfrom llm.client.vertexai.entity import UserContent, SAFETY_SETTING\\nfrom llm.client.common.async_http import async_get', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class VertexAIClient:\\n    def __init__(self, location=None, credential_file=None):\\n        if credential_file is None:\\n            credential_file = VERTEXAI_CONFIG.CREDENTIAL_FILE_PATH\\n        else:\\n            credential_file = credential_file\\n        credentials = Credentials.from_service_account_file(credential_file)\\n\\n        if location is None:\\n            location = VERTEXAI_CONFIG.REGION\\n\\n        vertexai.init(\\n            project=VERTEXAI_CONFIG.PROJECT_ID,\\n            location=location,\\n            credentials=credentials\\n        )\\n\\n        self.model_client = self.__load_model()\\n\\n    def __load_model(self):\\n        model_client = {}\\n        for model_id in CHAT_COMPLETION_MODELS.get(\"vertexai\", []):\\n            model_client[model_id] = GenerativeModel(model_id)\\n        return model_client', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def generate_response(\\n        self,\\n        model: str,\\n        user_content: UserContent,\\n        generation_config: GenerationConfig,\\n        stream: bool,\\n        system_message: str = None\\n    ):\\n        if system_message is not None:\\n            model_client = GenerativeModel(model, system_instruction[system_message])\\n        else:\\n            model_client = self.model_client.get(model)\\n\\n        contents = [user_content.text]\\n        if user_content.images is not None:\\n            images = [await self._load_image_from_url(image) for image in user_content.images]\\n            contents.extend(images)\\n\\n        response = await model_client.generate_content_async(\\n            contents=contents,\\n            generation_config=generation_config,\\n            safety_settings=SAFETY_SETTING,\\n            stream=stream\\n        )\\n\\n        return response\\n\\n    async def _get_image_bytes_from_url(self, image_url: str) -> bytes:\\n        response, status = await async_get(image_url)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='return response\\n\\n    async def _load_image_from_url(self, image_url: str) -> Image:\\n        image_bytes = await self._get_image_bytes_from_url(image_url)\\n        return Image.from_bytes(image_bytes)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from pydantic import BaseModel\\nfrom vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\\nfrom typing import Optional, List\\n\\n\\nclass UserContent(BaseModel):\\n    text: str\\n    images: Optional[List] = None\\n\\n\\nSAFETY_SETTING = {\\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\\n}', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='CHAT_COMPLETION_MODELS = {\\n    \"openai\": [\\n        \"gpt-3.5-turbo\",\\n        \"gpt-3.5-turbo-1106\",\\n        \"gpt-3.5-turbo-0125\",\\n        \"gpt-3.5-turbo-16k\",\\n        \"gpt-4\",\\n        \"gpt-4-turbo\",\\n        \"gpt-4-0125-preview\",\\n        \"gpt-4-1106-preview\",\\n        \"gpt-4o\",\\n        \"ft:gpt-3.5-turbo-0125:ruangguru:intentclass:9Wjgagxi\"\\n    ],\\n    \"vertexai\": [\\n        \"gemini-1.5-flash-001\",\\n        \"gemini-1.5-pro-001\"\\n    ]\\n}', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import glob\\nimport yaml\\nimport os\\nimport mmh3\\nfrom llm.utils import generate_uuid\\n\\n\\nSEGMENT_SIZE = 1000', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\allocation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class GeneralAllocation:\\n    def __init__(self, platform_name):\\n        self.config_directory = \"llm/core/allocation/{}/config\".format(platform_name)\\n        self.config = {}\\n        self.__create_config()\\n\\n    def __create_config(self):\\n        directory = glob.glob(self.config_directory + \"/*.yaml\", recursive=True)\\n\\n        for filepath in directory:\\n            config_name = os.path.basename(filepath).split(\".\")[0]\\n            with open(filepath) as f:\\n                config_yaml = yaml.load(f, Loader=yaml.FullLoader)\\n            self.config[config_name] = config_yaml\\n\\n            self.config[config_name] = self.__compute_allocation(config_name)\\n\\n    def get_variant(self, config_name, unit_id=None):\\n        if unit_id is None:\\n            unit_id = generate_uuid()\\n\\n        segment = self.get_segment(unit_id)\\n        if config_name not in self.config.keys():\\n            return None\\n\\n        variants = self.config[config_name][\"variants\"]', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\allocation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='for variant in self.config[config_name][\"variants\"]:\\n            if segment < variant[\"offset\"]:\\n                return variant[\"value\"]\\n\\n        return variants[0][\"value\"]\\n\\n    def __compute_allocation(self, config_name):\\n        config = self.config[config_name]\\n        list_variant = []\\n        offset = 0\\n        for row in config[\"variants\"]:\\n            upperbound = int(row[\"weight\"]*SEGMENT_SIZE)\\n            offset = offset + upperbound\\n            list_variant.append(\\n                {\\n                    \"value\": row[\"value\"],\\n                    \"weight\": row[\"weight\"],\\n                    \"offset\": offset\\n                }\\n            )\\n        config[\"variants\"] = list_variant\\n\\n        return config\\n\\n    def get_segment(self, unit_id):\\n        segment = mmh3.hash(unit_id, signed=False) % SEGMENT_SIZE\\n\\n        return segment', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\allocation\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.port import AbstractChatCompletionModel', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\dummy\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = False\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        choices = CompletionChoice(\\n            index=0,\\n            message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n        )\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=[choices],\\n            usage={\\n                \"prompt_token\": 100,\\n                \"completion_token\": 100,\\n                \"total_token\": 200\\n            }\\n        )\\n        return response', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\dummy\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        async def post_process_stream():\\n            for i in range(10):\\n                choices = CompletionChoice(\\n                    index=i,\\n                    message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\\n                )\\n                response = ChatCompletionResponse(\\n                    uuid=generate_uuid(),\\n                    choices=[choices],\\n                    usage={\\n                        \"prompt_token\": 100,\\n                        \"completion_token\": 100,\\n                        \"total_token\": 200\\n                    }\\n                )\\n                yield response\\n\\n        return (row async for row in post_process_stream())', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\dummy\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import OPENAI_CONFIG\\nfrom llm.core.platform.openai.entity import MODEL_VARIANT, ORG_KEYS\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_org_key(self, model_id):\\n        if not OPENAI_CONFIG.USE_ALLOCATION:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_model = MODEL_VARIANT.get(model_id)\\n        variant_value = self.get_variant(variant_model)\\n\\n        if variant_value is None:\\n            return OPENAI_CONFIG.DEFAULT_ORG_KEY\\n\\n        variant_org_key = ORG_KEYS.get(variant_value, OPENAI_CONFIG.DEFAULT_ORG_KEY)\\n        logger.info(f\"use openai varian for model_id: {model_id}, with value {variant_value}:{variant_org_key}\")\\n\\n        return variant_org_key', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\allocation.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.config import OPENAI_CONFIG\\n\\n\\nORG_KEYS = {\\n    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\\n    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\\n    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\\n    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\\n}\\n\\nMODEL_VARIANT = {\\n    \"gpt-3.5-turbo\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\\n    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\\n    \"gpt-4\": \"gpt-4\",\\n    \"gpt-4-turbo\": \"gpt-4-turbo\",\\n    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\\n    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\\n    \"gpt-4o\": \"gpt-4o\"\\n\\n}', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.utils import generate_uuid\\nfrom llm.client.openai.client import OpenAIClient\\nfrom llm.client.openai.entity import OpenAIRequestSettings\\nfrom llm.core.service.port import AbstractChatCompletionModel\\nfrom llm.core.platform.openai.allocation import Allocation', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.client = OpenAIClient()\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"openai\")\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools,\\n    ):\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n\\n        org_key = self.allocation.get_variant_org_key(model)\\n\\n        openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=False,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='choices = [choice.dict() for choice in openai_response.choices]\\n        choices = [CompletionChoice(**choice) for choice in choices]\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage={\\n                \"prompt_token\": openai_response.usage.prompt_tokens,\\n                \"completion_token\": openai_response.usage.completion_tokens,\\n                \"total_token\": openai_response.usage.total_tokens\\n            }\\n        )\\n        return response\\n\\n    async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        openai_config = OpenAIRequestSettings(\\n            temperature=config.temperature,\\n            max_tokens=config.max_token,\\n            top_p=config.top_p\\n        )\\n        org_key = self.allocation.get_variant_org_key(model)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='openai_response = await self.client.send_chat_request(\\n            messages=messages,\\n            model=model,\\n            stream=True,\\n            request_settings=openai_config,\\n            response_format=response_format,\\n            tools=tools,\\n            client_org=org_key\\n        )\\n\\n        async def post_process_stream(uuid, openai_response):\\n            async for chunk in openai_response:\\n                choices = []\\n                for choice in chunk.choices:\\n                    message_response = MessageDataResponse(\\n                        content=choice.delta.content,\\n                        role=choice.delta.role\\n                    )\\n                    _choices = CompletionChoice(\\n                        index=choice.index, message=message_response, finish_reason=choice.finish_reason\\n                    )\\n                    choices.append(_choices)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='if chunk.usage is not None:\\n                    usage = {\\n                        \"prompt_token\": chunk.usage.prompt_tokens,\\n                        \"completion_token\": chunk.usage.completion_tokens,\\n                        \"total_token\": chunk.usage.total_tokens\\n                    }\\n                else:\\n                    usage = {\\n                        \"prompt_token\": 0,\\n                        \"completion_token\": 0,\\n                        \"total_token\": 0\\n                    }\\n\\n                response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, openai_response))\\n        return response', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.allocation import GeneralAllocation\\nfrom llm.config import VERTEXAI_CONFIG\\nimport logging\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass Allocation(GeneralAllocation):\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n    def get_variant_region(self):\\n        if not VERTEXAI_CONFIG.USE_ALLOCATION:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        variant_value = self.get_variant(\"region\")\\n\\n        if variant_value is None:\\n            return VERTEXAI_CONFIG.REGION\\n\\n        logger.info(f\"use vertex ai region with value {variant_value}\")\\n\\n        return variant_value', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\allocation.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='REGION_VARIANT = [\\n    \"us-central1\",\\n    \"us-east4\",\\n    \"us-west1\",\\n    \"us-west4\",\\n    \"northamerica-northeast1\",\\n    \"europe-west9\",\\n    \"europe-west2\",\\n    \"europe-west3\",\\n    \"europe-west4\",\\n    \"europe-west9\",\\n    \"asia-northeast1\",\\n    \"asia-northeast3\",\\n    \"asia-southeast1\"\\n]', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\entity.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.service.port import AbstractChatCompletionModel\\nfrom vertexai.generative_models import Image, GenerationConfig\\nfrom llm.client.vertexai.client import VertexAIClient\\nfrom llm.client.vertexai.entity import UserContent\\nfrom llm.core.platform.vertexai.allocation import Allocation\\nfrom llm.utils import generate_uuid\\nfrom llm.core.service.dto import (\\n    MessageDataRequest,\\n    MessageDataResponse,\\n    ChatCompletionSetting,\\n    CompletionChoice,\\n    ChatCompletionResponse\\n)\\nfrom llm.core.platform.vertexai.entity import REGION_VARIANT', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class Model(AbstractChatCompletionModel):\\n    def __init__(self):\\n        self.require_model_validation = True\\n        self.allocation = Allocation(platform_name=\"vertexai\")\\n        self.clients = self.__load_client()\\n\\n    def __load_client(self):\\n        clients = {}\\n        for region in REGION_VARIANT:\\n            clients[region] = VertexAIClient(location=region)\\n        return clients\\n\\n    def __get_client(self):\\n        varian_region = self.allocation.get_variant_region()\\n        return self.clients.get(varian_region)\\n\\n    async def generate_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='# TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=False,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )\\n\\n        choices = self._parse_choices(vertexai_response.candidates)\\n\\n        usage = {\\n            \"prompt_token\": vertexai_response.usage_metadata.prompt_token_count,\\n            \"completion_token\": vertexai_response.usage_metadata.candidates_token_count,\\n            \"total_token\": vertexai_response.usage_metadata.total_token_count\\n        }\\n\\n        response = ChatCompletionResponse(\\n            uuid=generate_uuid(),\\n            choices=choices,\\n            usage=usage\\n        )\\n\\n        return response', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def generate_stream_chat_completion(\\n        self,\\n        model: str,\\n        messages: MessageDataRequest,\\n        config: ChatCompletionSetting,\\n        response_format,\\n        tools\\n    ):\\n        uuid = generate_uuid()\\n        system_message = self._get_system_message(messages)\\n        user_content = self._get_user_content(messages)\\n        generation_config = self._construct_generation_config(config, response_format)\\n\\n        vertexai_client = self.__get_client()\\n\\n        # TODO implement tools in vertexai\\n        vertexai_response = await vertexai_client.generate_response(\\n            model=model,\\n            user_content=user_content,\\n            stream=True,\\n            generation_config=generation_config,\\n            system_message=system_message\\n        )', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def post_process_stream(uuid, vertexai_response):\\n            async for chunk in vertexai_response:\\n                if chunk is None:\\n                    continue\\n                choices = []\\n                index = 0\\n                for row in chunk.candidates:\\n                    if len(row.content.parts) > 0:\\n                        choice = CompletionChoice(\\n                            index=index,\\n                            message=MessageDataResponse(\\n                                role=\"assistant\",\\n                                content=row.content.parts[0].text\\n                            ),\\n                            finish_reason=row.finish_reason.name\\n                        )\\n                        choices.append(choice)\\n                        index += 1\\n                usage = {\\n                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\\n                    \"completion_token\": chunk.usage_metadata.candidates_token_count,', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='\"total_token\": chunk.usage_metadata.total_token_count\\n                }', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='response = ChatCompletionResponse(\\n                    uuid=uuid,\\n                    choices=choices,\\n                    usage=usage\\n\\n                )\\n                yield response\\n\\n        response = (row async for row in post_process_stream(uuid, vertexai_response))\\n\\n        return response\\n\\n    def _get_system_message(self, messages):\\n        first_message = messages[0]\\n        if first_message.role == \"system\":\\n            return first_message.content\\n        else:\\n            return None', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def _get_user_content(self, messages):\\n        # currently only use last user message as user content\\n        last_message = messages[-1]\\n        text = None\\n        images = []\\n        if last_message.role == \"user\":\\n            content = last_message.content\\n            if isinstance(content, list):\\n                for row in content:\\n                    if row.type == \"text\":\\n                        text = row.text\\n                    elif row.type == \"image_url\":\\n                        image_url = row.image_url.get(\"url\")\\n                        images.append(image_url)\\n            else:\\n                text = content\\n\\n        if len(images) == 0:\\n            images = None\\n\\n        return UserContent(text=text, images=images)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def _construct_generation_config(self, config: ChatCompletionSetting, response_format):\\n        if response_format is not None:\\n            if response_format.type == \"json_object\":\\n                mime_type = \"application/json\"\\n            else:\\n                mime_type = \"text/plain\"\\n        else:\\n            mime_type = \"text/plain\"\\n\\n        generation_config = GenerationConfig(\\n            temperature=config.temperature,\\n            max_output_tokens=config.max_token,\\n            response_mime_type=mime_type\\n        )\\n\\n        return generation_config\\n\\n    def _parse_choices(self, response_candidates):\\n        index = 0\\n        choices = []\\n        for row in response_candidates:\\n            if len(row.content.parts)>0:\\n                messages = MessageDataResponse(\\n                    role=\"assistant\",\\n                    content=row.content.parts[0].text\\n                )\\n            else:\\n                messages = MessageDataResponse()', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='choice = CompletionChoice(\\n                index=index,\\n                message=messages,\\n                finish_reason=row.finish_reason.name\\n            )\\n            choices.append(choice)\\n            index += 1\\n\\n            return choices', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from llm.core.service.dto import ChatCompletionRequest\\nfrom importlib import import_module\\nfrom llm.core.service.exceptions import PlatformModelNotFoundError\\nfrom llm.core.entity import CHAT_COMPLETION_MODELS\\nimport copy', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class ChatCompletionService:\\n    def __init__(self, active_models, event_tracker):\\n        self.active_models = active_models\\n        self.event_tracker = event_tracker\\n        self.__models = self.__load_model()\\n\\n    def __load_model(self):\\n        models = {}\\n        for platform in self.active_models:\\n            Model = getattr(\\n                import_module(\"llm.core.platform.{}.model\".format(platform)),\\n                \"Model\",\\n            )\\n            models[platform] = Model()\\n        return models\\n\\n    def __validate_platform_and_model(self, platform: str, model: str):\\n        platform_model = self.__models.get(platform)\\n        if not platform_model:\\n            raise PlatformModelNotFoundError(\"platform\", platform)\\n\\n        if platform_model.require_model_validation:\\n            if model not in CHAT_COMPLETION_MODELS.get(platform, []):\\n                raise PlatformModelNotFoundError(\"model\", model)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='async def generate_chat_completion(self, request: ChatCompletionRequest):\\n        model_id = request.model\\n        message_data = request.messages\\n        config = request.config\\n        response_format = request.response_format\\n        tools = request.tools\\n\\n        self.__validate_platform_and_model(request.platform, model_id)\\n\\n        if not request.stream:\\n            completion_response = await self.__models[request.platform].generate_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n            await self.event_tracker.log_llm_chat_completion(request, completion_response)\\n\\n            return completion_response', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='else:\\n            completion_response = await self.__models[request.platform].generate_stream_chat_completion(\\n                model=model_id,\\n                messages=message_data,\\n                config=config,\\n                response_format=response_format,\\n                tools=tools\\n            )\\n\\n            return (row async for row in self.__send_tracker_stream(request, completion_response))\\n\\n    async def __send_tracker_stream(self, request: ChatCompletionRequest, response):\\n        full_completion_content = \"\"\\n        full_completion_role = None\\n        full_response = None\\n        full_usage = None\\n        async for row in response:\\n            yield row', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='if len(row.choices) > 0:\\n                if full_response is None:\\n                    full_response = copy.deepcopy(row)\\n                if row.choices[0].message.content is not None:\\n                    full_completion_content += row.choices[0].message.content\\n                if row.choices[0].message.role is not None:\\n                    full_completion_role = row.choices[0].message.role\\n            if row.usage.prompt_token > 0:\\n                full_usage = row.usage\\n\\n        full_response.choices[0].message.content = full_completion_content\\n        full_response.choices[0].message.role = full_completion_role\\n        full_response.usage = full_usage\\n\\n        await self.event_tracker.log_llm_chat_completion(request, full_response)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from pydantic import BaseModel\\nfrom typing import Optional, List, Dict, Union, Any\\n\\n\\nclass ContentData(BaseModel):\\n    type: str\\n    text: Optional[str] = None\\n    image_url: Optional[Dict[str, str]] = None\\n\\n\\nclass FunctionParameterData(BaseModel):\\n    type: str\\n    properties: Dict[Any, Any]\\n    required: List[str]\\n\\n\\nclass FunctionData(BaseModel):\\n    name: str\\n    description: str\\n    parameters: FunctionParameterData\\n\\n\\nclass ToolsDataRequest(BaseModel):\\n    type: str\\n    function: Optional[FunctionData]\\n\\n\\nclass MessageDataRequest(BaseModel):\\n    role: str\\n    content: Union[str, List[ContentData]]\\n\\n\\nclass ChatCompletionSetting(BaseModel):\\n    max_token: Optional[int] = 256\\n    temperature: Optional[float] = 0.7\\n    top_p: Optional[float] = 1\\n    top_k: Optional[float] = 1\\n\\n\\nclass ResponseFormatRequest(BaseModel):\\n    type: Optional[str] = None', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\dto.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class ChatCompletionRequest(BaseModel):\\n    platform: str\\n    model: str\\n    messages: List[MessageDataRequest]\\n    tools: Optional[List[ToolsDataRequest]] = None\\n    config: Optional[ChatCompletionSetting] = {}\\n    stream: bool = False\\n    task_name: Optional[str] = None\\n    client_name: str\\n    trace_id: str\\n    session_id: Optional[str] = \"\"\\n    response_format: Optional[ResponseFormatRequest] = None\\n\\n\\nclass TokenUsage(BaseModel):\\n    prompt_token: Optional[int]\\n    completion_token: Optional[int]\\n    total_token: Optional[int]\\n\\n\\nclass FunctionResponse(BaseModel):\\n    arguments: str\\n    name: str\\n\\n\\nclass ToolCallResponse(BaseModel):\\n    function: Optional[FunctionResponse] = None\\n    type: str\\n\\n\\nclass MessageDataResponse(BaseModel):\\n    role: Optional[str] = None\\n    content: Optional[str] = None\\n    tool_calls: Optional[List[ToolCallResponse]] = None\\n\\n\\nclass CompletionChoice(BaseModel):\\n    index: int\\n    message: MessageDataResponse\\n    finish_reason: Optional[str] = None', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\dto.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class ChatCompletionResponse(BaseModel):\\n    uuid: str\\n    choices: Optional[List[CompletionChoice]] = []\\n    usage: Optional[TokenUsage] = {}', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\dto.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='class PlatformModelNotFoundError(Exception):\\n    def __init__(self, field, value):\\n        self.field = field\\n        self.value = value\\n\\n\\nclass AIException(Exception):\\n    def __init__(self, name, message, errors):\\n        self.name = name\\n        self.message = message\\n        self.errors = errors', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\exceptions.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import abc\\n\\n\\nclass AbstractChatCompletionModel(abc.ABC):\\n    def __init__(self) -> None:\\n        pass\\n\\n    @abc.abstractmethod\\n    def generate_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError\\n\\n    @abc.abstractmethod\\n    def generate_stream_chat_completion(self, model, messages, config, response_format, tools):\\n        raise NotImplementedError', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\port.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='import pytest\\nfrom fastapi.testclient import TestClient\\nfrom pytest import fixture\\n\\n\\n@pytest.fixture(scope=\"package\")\\ndef client():\\n    from llm.api import app\\n    with TestClient(app) as client:\\n        yield client', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\conftest.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='from pprint import pprint\\nimport requests\\nimport json\\n\\n\\ndef test_chat_completion(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"dummy\",\\n        \"model\": \"gpt-4-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def test_chat_completion_openai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def test_chat_completion_openai_with_tools(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"What\\'s the weather like in San franciso, tokyo and paris\"}\\n        ],\\n        \"tools\": [{\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"get_current_weather\",\\n                \"description\": \"Get the current weather in a given location\",\\n                \"parameters\": {\\n                    \"type\": \"object\",\\n                    \"properties\": {\\n                        \"location\": {\\n                            \"type\": \"string\",\\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\\n                            },\\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\\n                    },\\n                    \"required\": [\"location\"]\\n                }\\n            }\\n        }],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='\"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\\n    assert len(response_data[\"choices\"][0][\"message\"][\"tool_calls\"]) > 0', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>}), Document(page_content='def test_chat_completion_vertexai(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"halo kak\"}\\n        ],\\n        \"platform\": \"vertexai\",\\n        \"model\": \"gemini-1.5-flash-001\",\\n        \"config\": {\\n            \"max_token\": 1000,\\n            \"temperature\": 0.5,\\n            \"top_p\": 1,\\n            \"top_k\": 1\\n        },\\n        \"client_name\": \"salesbot\",\\n        \"task_name\": \"saya\",\\n        \"stream\": False,\\n        \"session_id\": \"session_abc\"\\n    }\\n\\n    response = client.post(\"chat/completion\", json=data)\\n    response_data = response.json().get(\"data\", {})\\n\\n    assert response.status_code == 200\\n    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"', metadata={'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py', 'language': <Language.PYTHON: 'python'>})]\n"
     ]
    }
   ],
   "source": [
    "texts = documents_splitter.split_documents(docs1)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source : test_repo\\rg-llm\\config.py\n",
      "\n",
      "from pydantic_settings import BaseSettings\n",
      "\n",
      "\n",
      "class GunicornConfig(BaseSettings):\n",
      "    NUM_WORKERS: int = 1\n",
      "    NUM_THREADS: int = 1\n",
      "    TIMEOUT: int = 30\n",
      "    LOG_LEVEL: str = \"INFO\"\n",
      "    LOG_DEBUG: str = \"DEBUG\"\n",
      "    LOG_ERROR: str = \"ERROR\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_API_GUNICORN_\"\n",
      "\n",
      "\n",
      "GUNICORN_CONFIG = GunicornConfig()\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "class RoguLogFilter(logging.Filter):\n",
      "    \"\"\"\n",
      "    Custom filter to exclude health check\n",
      "    \"\"\"\n",
      "\n",
      "    def filter(self, record):\n",
      "        try:\n",
      "            uri = str(record.args[2])\n",
      "        except Exception as e:\n",
      "            uri = None\n",
      "        return uri != \"/_health\"\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "class RoguFormatter(logging.Formatter):\n",
      "    \"\"\"\n",
      "    RG Logger Formatter\n",
      "    \"\"\"\n",
      "\n",
      "    datefmt = \"%Y-%m-%dT%H:%M:%SZ\"\n",
      "    fmt = 'RG_LOGGER: time=\"{asctime}\" name={name} lineno={lineno} level={levelname} msg=\"{msg}\" {extra}'\n",
      "\n",
      "    def format(self, record):\n",
      "\n",
      "        extra = []\n",
      "        if record.exc_info:\n",
      "            extra_str = f'exc_info=\"{self.formatException(record.exc_info)}\"'\n",
      "            extra.append(extra_str)\n",
      "\n",
      "        if hasattr(record, \"request_body\"):\n",
      "            extra_str = f'request_body=\"{record.request_body}\"'\n",
      "            extra.append(extra_str)\n",
      "\n",
      "        if hasattr(record, \"request_path\"):\n",
      "            extra_str = f\"request_path={record.request_path}\"\n",
      "            extra.append(extra_str)\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "extra_str = \" \".join(extra)\n",
      "        message = self.__class__.fmt.format(\n",
      "            asctime=self.formatTime(record, self.__class__.datefmt),\n",
      "            name=record.name,\n",
      "            lineno=record.lineno,\n",
      "            levelname=record.levelname.lower(),\n",
      "            msg=self._one_liner(record.getMessage()),\n",
      "            extra=self._one_liner(extra_str)\n",
      "        )\n",
      "\n",
      "        return message\n",
      "\n",
      "    def formatException(self, exc_info):\n",
      "        result = super().formatException(exc_info)\n",
      "        return self._one_liner(result)\n",
      "\n",
      "    def _one_liner(self, msg):\n",
      "        return msg.replace(\"\\n\", \"\\t|\\t\")\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "import threading\n",
      "import sys\n",
      "import traceback\n",
      "from logging.config import dictConfig\n",
      "from config import GUNICORN_CONFIG\n",
      "\n",
      "\n",
      "workers = GUNICORN_CONFIG.NUM_WORKERS\n",
      "threads = GUNICORN_CONFIG.NUM_THREADS\n",
      "timeout = GUNICORN_CONFIG.TIMEOUT\n",
      "worker_class = \"uvicorn.workers.UvicornWorker\"\n",
      "bind = '0.0.0.0:80'\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "logconfig_dict = {\n",
      "        \"version\": 1,\n",
      "        \"disable_existing_loggers\": True,\n",
      "        \"formatters\": {\n",
      "                \"custom\": {\n",
      "                    \"()\": \"custom_logging.RoguFormatter\"\n",
      "                }\n",
      "        },\n",
      "        \"filters\": {\n",
      "            \"exclude_health_check\": {\n",
      "                \"()\": \"custom_logging.RoguLogFilter\"\n",
      "            }\n",
      "        },\n",
      "        \"handlers\": {\n",
      "            \"access_console\": {\n",
      "                \"class\": \"logging.StreamHandler\",\n",
      "                \"formatter\": \"custom\",\n",
      "                \"filters\": [\"exclude_health_check\"]\n",
      "            },\n",
      "            \"console\": {\n",
      "                \"class\": \"logging.StreamHandler\",\n",
      "                \"formatter\": \"custom\"\n",
      "            },\n",
      "        },\n",
      "        \"loggers\": {\n",
      "            \"\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"elasticsearch\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"propagate\": False,\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "\"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"openai\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_DEBUG,\n",
      "                \"propagate\": False,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"llm\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"propagate\": False,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"gunicorn.access\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"access_console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"gunicorn.access\"\n",
      "            },\n",
      "            \"gunicorn.error\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"gunicorn.error\"\n",
      "            },\n",
      "            \"uvicorn.access\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"access_console\"],\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "\"propagate\": False,\n",
      "                \"qualname\": \"uvicorn.access\"\n",
      "            },\n",
      "            \"uvicorn.error\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"uvicorn.error\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "dictConfig(logconfig_dict)\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "def post_fork(server, worker):\n",
      "    server.log.info(\"Worker spawned (pid: %s)\", worker.pid)\n",
      "\n",
      "\n",
      "def pre_fork(server, worker):\n",
      "    pass\n",
      "\n",
      "\n",
      "def post_worker_init(worker):\n",
      "    pass\n",
      "\n",
      "\n",
      "def pre_exec(server):\n",
      "    server.log.info(\"Forked child, re-executing.\")\n",
      "\n",
      "\n",
      "def worker_int(worker):\n",
      "    worker.log.info(\"worker received INT or QUIT signal\")\n",
      "\n",
      "    # get traceback info\n",
      "    id2name = dict([(th.ident, th.name) for th in threading.enumerate()])\n",
      "    code = []\n",
      "    for threadId, stack in sys._current_frames().items():\n",
      "        code.append(\"\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\n",
      "        for filename, lineno, name, line in traceback.extract_stack(stack):\n",
      "            code.append('File: \"%s\", line %d, in %s' % (filename, lineno, name))\n",
      "            if line:\n",
      "                code.append(\"  %s\" % (line.strip()))\n",
      "    worker.log.debug(\"\\n\".join(code))\n",
      "\n",
      "\n",
      "def worker_abort(worker):\n",
      "    worker.log.info(\"worker received SIGABRT signal\")\n",
      "Source : test_repo\\rg-llm\\llm\\config.py\n",
      "\n",
      "import openai\n",
      "import warnings\n",
      "from pydantic_settings import BaseSettings\n",
      "import os\n",
      "\n",
      "\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "\n",
      "class ChatCompletionConfig(BaseSettings):\n",
      "    ACTIVE_MODELS: str = \"dummy;openai;vertexai\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_CHAT_COMPLETION_CONFIG_\"\n",
      "\n",
      "\n",
      "class OpenAIConfig(BaseSettings):\n",
      "    DEFAULT_ORG_KEY: str = \"\"\n",
      "    USE_ALLOCATION: bool = False\n",
      "    ORG_KEY_1: str = \"\"\n",
      "    ORG_KEY_2: str = \"\"\n",
      "    ORG_KEY_3: str = \"\"\n",
      "    ORG_KEY_4: str = \"\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_OPENAI_CONFIG_\"\n",
      "Source : test_repo\\rg-llm\\llm\\config.py\n",
      "\n",
      "class VertexAIConfig(BaseSettings):\n",
      "    PROJECT_ID: str = \"silicon-airlock-153323\"\n",
      "    REGION: str = \"asia-southeast1\"\n",
      "    CREDENTIAL_FILE_CONTENT: str = \"\"\n",
      "    CREDENTIAL_FILE_PATH: str = os.path.join(os.getcwd(), \"sa-ai-platform-user.json\")\n",
      "    USE_ALLOCATION: bool = False\n",
      "\n",
      "    def get_secret(self):\n",
      "        if len(self.CREDENTIAL_FILE_CONTENT):\n",
      "            with open(self.CREDENTIAL_FILE_PATH, \"w\") as f:\n",
      "                f.write(self.CREDENTIAL_FILE_CONTENT)\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_VERTEXAI_CONFIG_\"\n",
      "\n",
      "\n",
      "class EventTrackingConfig(BaseSettings):\n",
      "    ACTIVE: bool = False\n",
      "    ENDPOINT: str = \"http://rg-event-tracking-api.staging.svc.cluster.local/api/v3/event/tracker\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_DS_CHATBOT_API_TRACKING_\"\n",
      "\n",
      "\n",
      "OPENAI_CONFIG = OpenAIConfig()\n",
      "VERTEXAI_CONFIG = VertexAIConfig()\n",
      "VERTEXAI_CONFIG.get_secret()\n",
      "\n",
      "CHAT_COMPLETION_CONFIG = ChatCompletionConfig()\n",
      "EVENT_TRACKING_CONFIG = EventTrackingConfig()\n",
      "Source : test_repo\\rg-llm\\llm\\utils.py\n",
      "\n",
      "from uuid import uuid4\n",
      "\n",
      "\n",
      "def generate_uuid():\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "def pipe(data, *funcs):\n",
      "    \"\"\" Pipe a value through a sequence of functions\n",
      "\n",
      "    I.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\n",
      "\n",
      "    We think of the value as progressing through a pipe of several\n",
      "    transformations, much like pipes in UNIX\n",
      "\n",
      "    ``$ cat data | f | g | h``\n",
      "\n",
      "    >>> double = lambda i: 2 * i\n",
      "    >>> pipe(3, double, str)\n",
      "    '6'\n",
      "\n",
      "    See Also:\n",
      "        compose\n",
      "        thread_first\n",
      "        thread_last\n",
      "    \"\"\"\n",
      "    for func in funcs:\n",
      "        data = func(data)\n",
      "    return data\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.applications import FastAPI\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError, AIException\n",
      "from llm.api.views import error_response\n",
      "from typing import Union\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "async def request_validation_handler(_: Request, error: RequestValidationError):\n",
      "    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\n",
      "                         for e in error.errors()]\n",
      "    return error_response(error, 400, validation_errors)\n",
      "\n",
      "\n",
      "async def http_exception_handler(_, error: StarletteHTTPException):\n",
      "    return error_response(error)\n",
      "\n",
      "\n",
      "async def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\n",
      "    logger.error(f\"Not Found for {error.field}: {error.value}\")\n",
      "    return error_response(error, 400)\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "async def ai_exception_handler(_, error):\n",
      "    logger.error(f\"LLM error with detail: {error}\")\n",
      "    return error_response(error, 500)\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "def register_error_handlers(app: FastAPI) -> FastAPI:\n",
      "    app.exception_handler(RequestValidationError)(request_validation_handler)\n",
      "    app.exception_handler(StarletteHTTPException)(http_exception_handler)\n",
      "    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\n",
      "    app.exception_handler(AIException)(ai_exception_handler)\n",
      "    return app\n",
      "Source : test_repo\\rg-llm\\llm\\api\\events.py\n",
      "\n",
      "from fastapi.applications import FastAPI\n",
      "from llm.client.common.async_http import async_http_open, async_http_close\n",
      "\n",
      "\n",
      "async def startup() -> None:\n",
      "    await async_http_open()\n",
      "\n",
      "\n",
      "async def shutdown() -> None:\n",
      "    await async_http_close()\n",
      "\n",
      "\n",
      "def register_events(app: FastAPI) -> FastAPI:\n",
      "    app.add_event_handler(\"startup\", startup)\n",
      "    app.add_event_handler(\"shutdown\", shutdown)\n",
      "    return app\n",
      "Source : test_repo\\rg-llm\\llm\\api\\factory.py\n",
      "\n",
      "from fastapi.applications import FastAPI\n",
      "\n",
      "from llm.api.routers import health\n",
      "from llm.api.routers import chat_completion\n",
      "from llm.api.error_handlers import register_error_handlers\n",
      "from llm.api.events import register_events\n",
      "from llm.api.middleware import register_middlewares\n",
      "from llm.utils import pipe\n",
      "\n",
      "\n",
      "def create_instance() -> FastAPI:\n",
      "    return FastAPI()\n",
      "\n",
      "\n",
      "def init_database(app: FastAPI) -> FastAPI:\n",
      "    return app\n",
      "\n",
      "\n",
      "def register_routers(app: FastAPI) -> FastAPI:\n",
      "    app.include_router(health.router)\n",
      "    app.include_router(chat_completion.router)\n",
      "    return app\n",
      "\n",
      "\n",
      "def init_app() -> FastAPI:\n",
      "    app: FastAPI = pipe(\n",
      "        create_instance(),\n",
      "        init_database,\n",
      "        register_events,\n",
      "        register_middlewares,\n",
      "        register_error_handlers,\n",
      "        register_routers\n",
      "    )\n",
      "    return app\n",
      "Source : test_repo\\rg-llm\\llm\\api\\middleware.py\n",
      "\n",
      "import logging\n",
      "from typing import Callable\n",
      "\n",
      "from fastapi import Request, Response\n",
      "from starlette.middleware.base import BaseHTTPMiddleware\n",
      "from fastapi.applications import FastAPI\n",
      "from llm.api.views import error_response\n",
      "import time\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "Source : test_repo\\rg-llm\\llm\\api\\middleware.py\n",
      "\n",
      "class RoguLoggerMiddleware(BaseHTTPMiddleware):\n",
      "\n",
      "    # ref: https://github.com/encode/starlette/issues/495\n",
      "    async def _reset_receive(self, request, content):\n",
      "        async def receive():\n",
      "            return content\n",
      "        request._receive = receive\n",
      "\n",
      "    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n",
      "        start_time = time.monotonic()\n",
      "\n",
      "        try:\n",
      "            response = await call_next(request)\n",
      "            processing_time = time.monotonic() - start_time\n",
      "            response.headers[\"x-processing-time\"] = str(round(processing_time*1000, 2))\n",
      "            return response\n",
      "        except Exception as e:\n",
      "            # catch any exception during performing request\n",
      "            logger.exception(e)\n",
      "            return error_response(e)\n",
      "\n",
      "\n",
      "def register_middlewares(app: FastAPI) -> FastAPI:\n",
      "    app.add_middleware(RoguLoggerMiddleware)\n",
      "    return app\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "import re\n",
      "import http\n",
      "import json\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import AIException, PlatformModelNotFoundError\n",
      "from typing import Dict, List, Optional, Type\n",
      "from fastapi.responses import JSONResponse\n",
      "\n",
      "\n",
      "def _camel_to_snake(name):\n",
      "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
      "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).upper()\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "def chat_completion_response(response):\n",
      "    output = {\n",
      "        \"uuid\": response.uuid,\n",
      "        \"choices\": [choice.dict() for choice in response.choices],\n",
      "        \"usage\": response.usage.dict()\n",
      "    }\n",
      "\n",
      "    return JSONResponse(\n",
      "        dict(\n",
      "            data=output,\n",
      "            status=\"success\",\n",
      "            message=\"success\"\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "async def stream_chat_completion_response(response):\n",
      "    async for row in response:\n",
      "        output = {\n",
      "            \"uuid\": row.uuid,\n",
      "            \"choices\": [choice.dict() for choice in row.choices],\n",
      "            \"usage\": row.usage.dict()\n",
      "        }\n",
      "        yield json.dumps(output) + \"\\n\"\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "def error_response(error: Exception, code: int = 500, descriptions: Optional[List] = None):\n",
      "    if descriptions is None:\n",
      "        descriptions = []\n",
      "\n",
      "    status = http.HTTPStatus(code).name\n",
      "\n",
      "    if isinstance(error, StarletteHTTPException):\n",
      "        name = error.detail.replace(\" \", \"\")\n",
      "        code = error.status_code\n",
      "    else:\n",
      "        name = error.__class__.__name__\n",
      "\n",
      "    if isinstance(error, AIException):\n",
      "        if error.args[2] is not None:\n",
      "            code = error.args[2].status_code\n",
      "            message = str(error.args[2].message)\n",
      "            error_field = _camel_to_snake(error.args[2].__class__.__name__)\n",
      "            if code is None:\n",
      "                code = 500\n",
      "            status = http.HTTPStatus(code).name\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "else:\n",
      "            message = error.args[1]\n",
      "            error_field = _camel_to_snake(error._error_code.name)\n",
      "            if error._error_code == AIException.ErrorCodes.InvalidRequest:\n",
      "                code = 400\n",
      "                status = http.HTTPStatus(code).name\n",
      "                error_field = _camel_to_snake(openai.error.InvalidRequestError.__name__)\n",
      "\n",
      "        error_message = [\n",
      "                {\"field\": error_field, \"message\": message}\n",
      "            ]\n",
      "    elif isinstance(error, PlatformModelNotFoundError):\n",
      "        error_message = [\n",
      "            {\"field\": error.args[0], \"message\": f\"Not found for {error.args[0]}: {error.args[1]}\"}\n",
      "        ]\n",
      "\n",
      "    else:\n",
      "        error_message = [\n",
      "                {\"field\": desc[0], \"message\": desc[1]} for desc in descriptions\n",
      "        ]\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "return JSONResponse(\n",
      "        dict(\n",
      "            status=status,\n",
      "            error=_camel_to_snake(name),\n",
      "            code=code,\n",
      "            error_code=error_message,\n",
      "            metadata={}\n",
      "        ),\n",
      "        status_code=code,\n",
      "    )\n",
      "Source : test_repo\\rg-llm\\llm\\api\\__init__.py\n",
      "\n",
      "from llm.api.factory import init_app\n",
      "\n",
      "\n",
      "app = init_app()\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\chat_completion.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.routing import APIRouter\n",
      "from fastapi.responses import JSONResponse\n",
      "from llm.api.views import chat_completion_response, stream_chat_completion_response\n",
      "from llm.core.service.dto import ChatCompletionRequest\n",
      "from llm.core.service.chat_completion import ChatCompletionService\n",
      "from llm.config import CHAT_COMPLETION_CONFIG\n",
      "from fastapi.responses import StreamingResponse\n",
      "from llm.client.event_tracking.client import EventTrackingApi\n",
      "import logging\n",
      "\n",
      "\n",
      "router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "event_tracker = EventTrackingApi()\n",
      "active_models = CHAT_COMPLETION_CONFIG.ACTIVE_MODELS.split(\";\")\n",
      "chat_completion_service = ChatCompletionService(active_models, event_tracker)\n",
      "\n",
      "\n",
      "@router.post(\"/chat/completion\")\n",
      "async def chat_completion(chat_request: ChatCompletionRequest, request: Request):\n",
      "    raw_response = await chat_completion_service.generate_chat_completion(chat_request)\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\chat_completion.py\n",
      "\n",
      "if not chat_request.stream:\n",
      "        return chat_completion_response(raw_response)\n",
      "    else:\n",
      "        response = stream_chat_completion_response(raw_response)\n",
      "        return StreamingResponse(response, media_type=\"text/event-stream\")\n",
      "\n",
      "\n",
      "@router.post(\"/dummy/completion\")\n",
      "async def chat_completion_dummy(chat_request: ChatCompletionRequest, request: Request):\n",
      "\n",
      "    return JSONResponse(dict(status=\"OK\"), status_code=200)\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\health.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.responses import JSONResponse\n",
      "from fastapi.routing import APIRouter\n",
      "\n",
      "router = APIRouter()\n",
      "\n",
      "\n",
      "@router.get(\"/_health\")\n",
      "async def health(request: Request):\n",
      "    return JSONResponse(dict(status=\"OK\"), status_code=200)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "import logging\n",
      "import aiohttp\n",
      "\n",
      "from typing import Tuple, Optional\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "class AsyncHttp:\n",
      "\n",
      "    __slots__ = (\"_client\",)\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        self._client = None\n",
      "\n",
      "    async def open(self) -> aiohttp.ClientSession:\n",
      "        if not self._client:\n",
      "            self._client = aiohttp.ClientSession()\n",
      "            logger.info(\"Async HTTP client session opened\")\n",
      "        return self._client\n",
      "\n",
      "    async def close(self) -> None:\n",
      "        if self._client:\n",
      "            await self._client.close()\n",
      "            logger.info(\"Async HTTP client session closed\")\n",
      "            self._client = None\n",
      "\n",
      "    async def get(self, url, headers: Optional[dict] = {}) -> Tuple[bytes, int]:\n",
      "        session = await self.open()\n",
      "        async with session.get(url, headers=headers) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "async def get_params(self, url, headers: Optional[dict] = {}, params: Optional[dict] = {}) -> Tuple[bytes, int]:\n",
      "        session = await self.open()\n",
      "        async with session.get(url, headers=headers, params=params) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "\n",
      "    async def post(self, url, data: str, headers: Optional[dict] = None) -> Tuple[bytes, int]:\n",
      "        if headers is None:\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "\n",
      "        session = await self.open()\n",
      "        async with session.post(url, data=data, headers=headers) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "\n",
      "    async def post_forget(self, url, data: str, headers: Optional[dict] = None):\n",
      "        if headers is None:\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "\n",
      "        session = await self.open()\n",
      "        async with session.post(url, data=data, headers=headers):\n",
      "            pass\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "_async_http_client = AsyncHttp()\n",
      "async_http_open = _async_http_client.open\n",
      "async_http_close = _async_http_client.close\n",
      "async_get = _async_http_client.get\n",
      "async_get_params = _async_http_client.get_params\n",
      "async_post = _async_http_client.post\n",
      "async_post_forget = _async_http_client.post_forget\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "import logging\n",
      "import asyncio\n",
      "import orjson\n",
      "\n",
      "from datetime import datetime, timezone\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from llm.client.common.async_http import async_post_forget\n",
      "from llm.client.event_tracking.entity import (\n",
      "    PostEventTrackingRequestData,\n",
      "    EventLLMChatCompletion\n",
      ")\n",
      "from llm.core.service.dto import ChatCompletionRequest, ChatCompletionResponse\n",
      "from llm.config import EVENT_TRACKING_CONFIG\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "class EventTrackingApi:\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            endpoint: str = EVENT_TRACKING_CONFIG.ENDPOINT,\n",
      "            active: bool = EVENT_TRACKING_CONFIG.ACTIVE) -> None:\n",
      "\n",
      "        self._endpoint = endpoint\n",
      "        self._active = active\n",
      "\n",
      "    async def log_llm_chat_completion(\n",
      "            self,\n",
      "            chat_completion_request: ChatCompletionRequest,\n",
      "            chat_completion_response: ChatCompletionResponse,\n",
      "            ) -> None:\n",
      "\n",
      "        if len(chat_completion_response.choices) > 0:\n",
      "            if chat_completion_response.choices[0].message.tool_calls is not None:\n",
      "                tool_calls = [x.dict() for x in chat_completion_response.choices[0].message.tool_calls]\n",
      "                completion = orjson.dumps(tool_calls)\n",
      "            else:\n",
      "                completion = chat_completion_response.choices[0].message.content\n",
      "        else:\n",
      "            completion = \"\"\n",
      "\n",
      "        messages = [x.dict() for x in chat_completion_request.messages]\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "token_usage = {\n",
      "            \"prompt_token\": chat_completion_response.usage.prompt_token,\n",
      "            \"completion_token\": chat_completion_response.usage.completion_token,\n",
      "        }\n",
      "\n",
      "        event = EventLLMChatCompletion(\n",
      "            uuid=chat_completion_response.uuid,\n",
      "            trace_id=chat_completion_request.trace_id,\n",
      "            session_id=chat_completion_request.session_id,\n",
      "            message=orjson.dumps(messages),\n",
      "            task_name=chat_completion_request.task_name,\n",
      "            model=chat_completion_request.model,\n",
      "            platform=chat_completion_request.platform,\n",
      "            completion=completion,\n",
      "            client_name=chat_completion_request.client_name,\n",
      "            token_usage=token_usage,\n",
      "        )\n",
      "\n",
      "        await self._send(\"dsChatbotLlmChatCompletion\", event)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "async def _send(self, event_type: str, context: BaseModel, memberId: str = None) -> None:\n",
      "        if self._active:\n",
      "            json_data = PostEventTrackingRequestData(\n",
      "                memberId=memberId,\n",
      "                isLogged=False,\n",
      "                eventType=event_type,\n",
      "                clientTimestamp=datetime.now(timezone.utc).isoformat(),\n",
      "                context=context.model_dump_json()\n",
      "            ).model_dump_json()\n",
      "\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "            asyncio.create_task(async_post_forget(self._endpoint, data=json_data, headers=headers))\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\entity.py\n",
      "\n",
      "from typing import List, Optional, Union, Any, Dict\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class EventLLMChatCompletion(BaseModel):\n",
      "    uuid: str\n",
      "    session_id: str\n",
      "    trace_id: str\n",
      "    platform: str\n",
      "    model: str\n",
      "    message: str\n",
      "    completion: Optional[str] = None\n",
      "    task_name: Optional[str]\n",
      "    client_name: str\n",
      "    token_usage: Dict[str, float]\n",
      "\n",
      "\n",
      "class PostEventTrackingRequestData(BaseModel):\n",
      "    id: Optional[str] = None\n",
      "    sessionId: Optional[str] = None\n",
      "    cookiesId: Optional[str] = None\n",
      "    deviceId: Optional[str] = None\n",
      "    memberId: Optional[str] = None\n",
      "    isLogged: bool = False\n",
      "    eventType: str\n",
      "    clientDevice: Optional[str] = None\n",
      "    clientUA: Optional[str] = None\n",
      "    appVersion: Optional[str] = None\n",
      "    clientOS: Optional[str] = None\n",
      "    clientOSVersion: Optional[str] = None\n",
      "    clientTimestamp: Optional[str] = None\n",
      "    context: Union[str, Any]\n",
      "    connectionType: Optional[str] = None\n",
      "    source: str = \"backend\"\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "from openai import AsyncOpenAI\n",
      "from llm.core.service.exceptions import AIException\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "class OpenAIClient:\n",
      "    def __init__(self):\n",
      "        self.client = AsyncOpenAI()\n",
      "\n",
      "    async def send_chat_request(self, messages, model, request_settings, stream, response_format, tools, client_org):\n",
      "        self.client.organization = client_org\n",
      "        if stream:\n",
      "            stream_options = {\"include_usage\": True}\n",
      "        else:\n",
      "            stream_options = None\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "try:\n",
      "            response = await self.client.chat.completions.create(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                temperature=request_settings.temperature,\n",
      "                max_tokens=request_settings.max_tokens,\n",
      "                top_p=request_settings.top_p,\n",
      "                stream=stream,\n",
      "                stream_options=stream_options,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "        except Exception as ex:\n",
      "            raise AIException(\n",
      "                \"openai\",\n",
      "                \"OpenAI service failed to complete the chat\",\n",
      "                ex\n",
      "            )\n",
      "\n",
      "        return response\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\entity.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class OpenAIRequestSettings(BaseModel):\n",
      "    temperature: float\n",
      "    max_tokens: int\n",
      "    top_p: float\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "from llm.config import VERTEXAI_CONFIG\n",
      "import vertexai\n",
      "from vertexai.generative_models import GenerativeModel, GenerationConfig, Image\n",
      "from google.oauth2.service_account import Credentials\n",
      "from llm.core.entity import CHAT_COMPLETION_MODELS\n",
      "from llm.client.vertexai.entity import UserContent, SAFETY_SETTING\n",
      "from llm.client.common.async_http import async_get\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "class VertexAIClient:\n",
      "    def __init__(self, location=None, credential_file=None):\n",
      "        if credential_file is None:\n",
      "            credential_file = VERTEXAI_CONFIG.CREDENTIAL_FILE_PATH\n",
      "        else:\n",
      "            credential_file = credential_file\n",
      "        credentials = Credentials.from_service_account_file(credential_file)\n",
      "\n",
      "        if location is None:\n",
      "            location = VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        vertexai.init(\n",
      "            project=VERTEXAI_CONFIG.PROJECT_ID,\n",
      "            location=location,\n",
      "            credentials=credentials\n",
      "        )\n",
      "\n",
      "        self.model_client = self.__load_model()\n",
      "\n",
      "    def __load_model(self):\n",
      "        model_client = {}\n",
      "        for model_id in CHAT_COMPLETION_MODELS.get(\"vertexai\", []):\n",
      "            model_client[model_id] = GenerativeModel(model_id)\n",
      "        return model_client\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "async def generate_response(\n",
      "        self,\n",
      "        model: str,\n",
      "        user_content: UserContent,\n",
      "        generation_config: GenerationConfig,\n",
      "        stream: bool,\n",
      "        system_message: str = None\n",
      "    ):\n",
      "        if system_message is not None:\n",
      "            model_client = GenerativeModel(model, system_instruction[system_message])\n",
      "        else:\n",
      "            model_client = self.model_client.get(model)\n",
      "\n",
      "        contents = [user_content.text]\n",
      "        if user_content.images is not None:\n",
      "            images = [await self._load_image_from_url(image) for image in user_content.images]\n",
      "            contents.extend(images)\n",
      "\n",
      "        response = await model_client.generate_content_async(\n",
      "            contents=contents,\n",
      "            generation_config=generation_config,\n",
      "            safety_settings=SAFETY_SETTING,\n",
      "            stream=stream\n",
      "        )\n",
      "\n",
      "        return response\n",
      "\n",
      "    async def _get_image_bytes_from_url(self, image_url: str) -> bytes:\n",
      "        response, status = await async_get(image_url)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "return response\n",
      "\n",
      "    async def _load_image_from_url(self, image_url: str) -> Image:\n",
      "        image_bytes = await self._get_image_bytes_from_url(image_url)\n",
      "        return Image.from_bytes(image_bytes)\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\entity.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "from vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\n",
      "from typing import Optional, List\n",
      "\n",
      "\n",
      "class UserContent(BaseModel):\n",
      "    text: str\n",
      "    images: Optional[List] = None\n",
      "\n",
      "\n",
      "SAFETY_SETTING = {\n",
      "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
      "}\n",
      "Source : test_repo\\rg-llm\\llm\\core\\entity.py\n",
      "\n",
      "CHAT_COMPLETION_MODELS = {\n",
      "    \"openai\": [\n",
      "        \"gpt-3.5-turbo\",\n",
      "        \"gpt-3.5-turbo-1106\",\n",
      "        \"gpt-3.5-turbo-0125\",\n",
      "        \"gpt-3.5-turbo-16k\",\n",
      "        \"gpt-4\",\n",
      "        \"gpt-4-turbo\",\n",
      "        \"gpt-4-0125-preview\",\n",
      "        \"gpt-4-1106-preview\",\n",
      "        \"gpt-4o\",\n",
      "        \"ft:gpt-3.5-turbo-0125:ruangguru:intentclass:9Wjgagxi\"\n",
      "    ],\n",
      "    \"vertexai\": [\n",
      "        \"gemini-1.5-flash-001\",\n",
      "        \"gemini-1.5-pro-001\"\n",
      "    ]\n",
      "}\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "import glob\n",
      "import yaml\n",
      "import os\n",
      "import mmh3\n",
      "from llm.utils import generate_uuid\n",
      "\n",
      "\n",
      "SEGMENT_SIZE = 1000\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "class GeneralAllocation:\n",
      "    def __init__(self, platform_name):\n",
      "        self.config_directory = \"llm/core/allocation/{}/config\".format(platform_name)\n",
      "        self.config = {}\n",
      "        self.__create_config()\n",
      "\n",
      "    def __create_config(self):\n",
      "        directory = glob.glob(self.config_directory + \"/*.yaml\", recursive=True)\n",
      "\n",
      "        for filepath in directory:\n",
      "            config_name = os.path.basename(filepath).split(\".\")[0]\n",
      "            with open(filepath) as f:\n",
      "                config_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
      "            self.config[config_name] = config_yaml\n",
      "\n",
      "            self.config[config_name] = self.__compute_allocation(config_name)\n",
      "\n",
      "    def get_variant(self, config_name, unit_id=None):\n",
      "        if unit_id is None:\n",
      "            unit_id = generate_uuid()\n",
      "\n",
      "        segment = self.get_segment(unit_id)\n",
      "        if config_name not in self.config.keys():\n",
      "            return None\n",
      "\n",
      "        variants = self.config[config_name][\"variants\"]\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "for variant in self.config[config_name][\"variants\"]:\n",
      "            if segment < variant[\"offset\"]:\n",
      "                return variant[\"value\"]\n",
      "\n",
      "        return variants[0][\"value\"]\n",
      "\n",
      "    def __compute_allocation(self, config_name):\n",
      "        config = self.config[config_name]\n",
      "        list_variant = []\n",
      "        offset = 0\n",
      "        for row in config[\"variants\"]:\n",
      "            upperbound = int(row[\"weight\"]*SEGMENT_SIZE)\n",
      "            offset = offset + upperbound\n",
      "            list_variant.append(\n",
      "                {\n",
      "                    \"value\": row[\"value\"],\n",
      "                    \"weight\": row[\"weight\"],\n",
      "                    \"offset\": offset\n",
      "                }\n",
      "            )\n",
      "        config[\"variants\"] = list_variant\n",
      "\n",
      "        return config\n",
      "\n",
      "    def get_segment(self, unit_id):\n",
      "        segment = mmh3.hash(unit_id, signed=False) % SEGMENT_SIZE\n",
      "\n",
      "        return segment\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.utils import generate_uuid\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.require_model_validation = False\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        choices = CompletionChoice(\n",
      "            index=0,\n",
      "            message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\n",
      "        )\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=[choices],\n",
      "            usage={\n",
      "                \"prompt_token\": 100,\n",
      "                \"completion_token\": 100,\n",
      "                \"total_token\": 200\n",
      "            }\n",
      "        )\n",
      "        return response\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        async def post_process_stream():\n",
      "            for i in range(10):\n",
      "                choices = CompletionChoice(\n",
      "                    index=i,\n",
      "                    message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\n",
      "                )\n",
      "                response = ChatCompletionResponse(\n",
      "                    uuid=generate_uuid(),\n",
      "                    choices=[choices],\n",
      "                    usage={\n",
      "                        \"prompt_token\": 100,\n",
      "                        \"completion_token\": 100,\n",
      "                        \"total_token\": 200\n",
      "                    }\n",
      "                )\n",
      "                yield response\n",
      "\n",
      "        return (row async for row in post_process_stream())\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\allocation.py\n",
      "\n",
      "from llm.core.allocation import GeneralAllocation\n",
      "from llm.config import OPENAI_CONFIG\n",
      "from llm.core.platform.openai.entity import MODEL_VARIANT, ORG_KEYS\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class Allocation(GeneralAllocation):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def get_variant_org_key(self, model_id):\n",
      "        if not OPENAI_CONFIG.USE_ALLOCATION:\n",
      "            return OPENAI_CONFIG.DEFAULT_ORG_KEY\n",
      "\n",
      "        variant_model = MODEL_VARIANT.get(model_id)\n",
      "        variant_value = self.get_variant(variant_model)\n",
      "\n",
      "        if variant_value is None:\n",
      "            return OPENAI_CONFIG.DEFAULT_ORG_KEY\n",
      "\n",
      "        variant_org_key = ORG_KEYS.get(variant_value, OPENAI_CONFIG.DEFAULT_ORG_KEY)\n",
      "        logger.info(f\"use openai varian for model_id: {model_id}, with value {variant_value}:{variant_org_key}\")\n",
      "\n",
      "        return variant_org_key\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\entity.py\n",
      "\n",
      "from llm.config import OPENAI_CONFIG\n",
      "\n",
      "\n",
      "ORG_KEYS = {\n",
      "    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\n",
      "    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\n",
      "    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\n",
      "    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\n",
      "}\n",
      "\n",
      "MODEL_VARIANT = {\n",
      "    \"gpt-3.5-turbo\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\n",
      "    \"gpt-4\": \"gpt-4\",\n",
      "    \"gpt-4-turbo\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4o\": \"gpt-4o\"\n",
      "\n",
      "}\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.utils import generate_uuid\n",
      "from llm.client.openai.client import OpenAIClient\n",
      "from llm.client.openai.entity import OpenAIRequestSettings\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "from llm.core.platform.openai.allocation import Allocation\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.client = OpenAIClient()\n",
      "        self.require_model_validation = True\n",
      "        self.allocation = Allocation(platform_name=\"openai\")\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools,\n",
      "    ):\n",
      "        openai_config = OpenAIRequestSettings(\n",
      "            temperature=config.temperature,\n",
      "            max_tokens=config.max_token,\n",
      "            top_p=config.top_p\n",
      "        )\n",
      "\n",
      "        org_key = self.allocation.get_variant_org_key(model)\n",
      "\n",
      "        openai_response = await self.client.send_chat_request(\n",
      "            messages=messages,\n",
      "            model=model,\n",
      "            stream=False,\n",
      "            request_settings=openai_config,\n",
      "            response_format=response_format,\n",
      "            tools=tools,\n",
      "            client_org=org_key\n",
      "        )\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "choices = [choice.dict() for choice in openai_response.choices]\n",
      "        choices = [CompletionChoice(**choice) for choice in choices]\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=choices,\n",
      "            usage={\n",
      "                \"prompt_token\": openai_response.usage.prompt_tokens,\n",
      "                \"completion_token\": openai_response.usage.completion_tokens,\n",
      "                \"total_token\": openai_response.usage.total_tokens\n",
      "            }\n",
      "        )\n",
      "        return response\n",
      "\n",
      "    async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        uuid = generate_uuid()\n",
      "        openai_config = OpenAIRequestSettings(\n",
      "            temperature=config.temperature,\n",
      "            max_tokens=config.max_token,\n",
      "            top_p=config.top_p\n",
      "        )\n",
      "        org_key = self.allocation.get_variant_org_key(model)\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "openai_response = await self.client.send_chat_request(\n",
      "            messages=messages,\n",
      "            model=model,\n",
      "            stream=True,\n",
      "            request_settings=openai_config,\n",
      "            response_format=response_format,\n",
      "            tools=tools,\n",
      "            client_org=org_key\n",
      "        )\n",
      "\n",
      "        async def post_process_stream(uuid, openai_response):\n",
      "            async for chunk in openai_response:\n",
      "                choices = []\n",
      "                for choice in chunk.choices:\n",
      "                    message_response = MessageDataResponse(\n",
      "                        content=choice.delta.content,\n",
      "                        role=choice.delta.role\n",
      "                    )\n",
      "                    _choices = CompletionChoice(\n",
      "                        index=choice.index, message=message_response, finish_reason=choice.finish_reason\n",
      "                    )\n",
      "                    choices.append(_choices)\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "if chunk.usage is not None:\n",
      "                    usage = {\n",
      "                        \"prompt_token\": chunk.usage.prompt_tokens,\n",
      "                        \"completion_token\": chunk.usage.completion_tokens,\n",
      "                        \"total_token\": chunk.usage.total_tokens\n",
      "                    }\n",
      "                else:\n",
      "                    usage = {\n",
      "                        \"prompt_token\": 0,\n",
      "                        \"completion_token\": 0,\n",
      "                        \"total_token\": 0\n",
      "                    }\n",
      "\n",
      "                response = ChatCompletionResponse(\n",
      "                    uuid=uuid,\n",
      "                    choices=choices,\n",
      "                    usage=usage\n",
      "\n",
      "                )\n",
      "\n",
      "                yield response\n",
      "\n",
      "        response = (row async for row in post_process_stream(uuid, openai_response))\n",
      "        return response\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\allocation.py\n",
      "\n",
      "from llm.core.allocation import GeneralAllocation\n",
      "from llm.config import VERTEXAI_CONFIG\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class Allocation(GeneralAllocation):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def get_variant_region(self):\n",
      "        if not VERTEXAI_CONFIG.USE_ALLOCATION:\n",
      "            return VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        variant_value = self.get_variant(\"region\")\n",
      "\n",
      "        if variant_value is None:\n",
      "            return VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        logger.info(f\"use vertex ai region with value {variant_value}\")\n",
      "\n",
      "        return variant_value\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\entity.py\n",
      "\n",
      "REGION_VARIANT = [\n",
      "    \"us-central1\",\n",
      "    \"us-east4\",\n",
      "    \"us-west1\",\n",
      "    \"us-west4\",\n",
      "    \"northamerica-northeast1\",\n",
      "    \"europe-west9\",\n",
      "    \"europe-west2\",\n",
      "    \"europe-west3\",\n",
      "    \"europe-west4\",\n",
      "    \"europe-west9\",\n",
      "    \"asia-northeast1\",\n",
      "    \"asia-northeast3\",\n",
      "    \"asia-southeast1\"\n",
      "]\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "from vertexai.generative_models import Image, GenerationConfig\n",
      "from llm.client.vertexai.client import VertexAIClient\n",
      "from llm.client.vertexai.entity import UserContent\n",
      "from llm.core.platform.vertexai.allocation import Allocation\n",
      "from llm.utils import generate_uuid\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.core.platform.vertexai.entity import REGION_VARIANT\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.require_model_validation = True\n",
      "        self.allocation = Allocation(platform_name=\"vertexai\")\n",
      "        self.clients = self.__load_client()\n",
      "\n",
      "    def __load_client(self):\n",
      "        clients = {}\n",
      "        for region in REGION_VARIANT:\n",
      "            clients[region] = VertexAIClient(location=region)\n",
      "        return clients\n",
      "\n",
      "    def __get_client(self):\n",
      "        varian_region = self.allocation.get_variant_region()\n",
      "        return self.clients.get(varian_region)\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        system_message = self._get_system_message(messages)\n",
      "        user_content = self._get_user_content(messages)\n",
      "        generation_config = self._construct_generation_config(config, response_format)\n",
      "\n",
      "        vertexai_client = self.__get_client()\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "# TODO implement tools in vertexai\n",
      "        vertexai_response = await vertexai_client.generate_response(\n",
      "            model=model,\n",
      "            user_content=user_content,\n",
      "            stream=False,\n",
      "            generation_config=generation_config,\n",
      "            system_message=system_message\n",
      "        )\n",
      "\n",
      "        choices = self._parse_choices(vertexai_response.candidates)\n",
      "\n",
      "        usage = {\n",
      "            \"prompt_token\": vertexai_response.usage_metadata.prompt_token_count,\n",
      "            \"completion_token\": vertexai_response.usage_metadata.candidates_token_count,\n",
      "            \"total_token\": vertexai_response.usage_metadata.total_token_count\n",
      "        }\n",
      "\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=choices,\n",
      "            usage=usage\n",
      "        )\n",
      "\n",
      "        return response\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        uuid = generate_uuid()\n",
      "        system_message = self._get_system_message(messages)\n",
      "        user_content = self._get_user_content(messages)\n",
      "        generation_config = self._construct_generation_config(config, response_format)\n",
      "\n",
      "        vertexai_client = self.__get_client()\n",
      "\n",
      "        # TODO implement tools in vertexai\n",
      "        vertexai_response = await vertexai_client.generate_response(\n",
      "            model=model,\n",
      "            user_content=user_content,\n",
      "            stream=True,\n",
      "            generation_config=generation_config,\n",
      "            system_message=system_message\n",
      "        )\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "async def post_process_stream(uuid, vertexai_response):\n",
      "            async for chunk in vertexai_response:\n",
      "                if chunk is None:\n",
      "                    continue\n",
      "                choices = []\n",
      "                index = 0\n",
      "                for row in chunk.candidates:\n",
      "                    if len(row.content.parts) > 0:\n",
      "                        choice = CompletionChoice(\n",
      "                            index=index,\n",
      "                            message=MessageDataResponse(\n",
      "                                role=\"assistant\",\n",
      "                                content=row.content.parts[0].text\n",
      "                            ),\n",
      "                            finish_reason=row.finish_reason.name\n",
      "                        )\n",
      "                        choices.append(choice)\n",
      "                        index += 1\n",
      "                usage = {\n",
      "                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\n",
      "                    \"completion_token\": chunk.usage_metadata.candidates_token_count,\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "\"total_token\": chunk.usage_metadata.total_token_count\n",
      "                }\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "response = ChatCompletionResponse(\n",
      "                    uuid=uuid,\n",
      "                    choices=choices,\n",
      "                    usage=usage\n",
      "\n",
      "                )\n",
      "                yield response\n",
      "\n",
      "        response = (row async for row in post_process_stream(uuid, vertexai_response))\n",
      "\n",
      "        return response\n",
      "\n",
      "    def _get_system_message(self, messages):\n",
      "        first_message = messages[0]\n",
      "        if first_message.role == \"system\":\n",
      "            return first_message.content\n",
      "        else:\n",
      "            return None\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "def _get_user_content(self, messages):\n",
      "        # currently only use last user message as user content\n",
      "        last_message = messages[-1]\n",
      "        text = None\n",
      "        images = []\n",
      "        if last_message.role == \"user\":\n",
      "            content = last_message.content\n",
      "            if isinstance(content, list):\n",
      "                for row in content:\n",
      "                    if row.type == \"text\":\n",
      "                        text = row.text\n",
      "                    elif row.type == \"image_url\":\n",
      "                        image_url = row.image_url.get(\"url\")\n",
      "                        images.append(image_url)\n",
      "            else:\n",
      "                text = content\n",
      "\n",
      "        if len(images) == 0:\n",
      "            images = None\n",
      "\n",
      "        return UserContent(text=text, images=images)\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "def _construct_generation_config(self, config: ChatCompletionSetting, response_format):\n",
      "        if response_format is not None:\n",
      "            if response_format.type == \"json_object\":\n",
      "                mime_type = \"application/json\"\n",
      "            else:\n",
      "                mime_type = \"text/plain\"\n",
      "        else:\n",
      "            mime_type = \"text/plain\"\n",
      "\n",
      "        generation_config = GenerationConfig(\n",
      "            temperature=config.temperature,\n",
      "            max_output_tokens=config.max_token,\n",
      "            response_mime_type=mime_type\n",
      "        )\n",
      "\n",
      "        return generation_config\n",
      "\n",
      "    def _parse_choices(self, response_candidates):\n",
      "        index = 0\n",
      "        choices = []\n",
      "        for row in response_candidates:\n",
      "            if len(row.content.parts)>0:\n",
      "                messages = MessageDataResponse(\n",
      "                    role=\"assistant\",\n",
      "                    content=row.content.parts[0].text\n",
      "                )\n",
      "            else:\n",
      "                messages = MessageDataResponse()\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "choice = CompletionChoice(\n",
      "                index=index,\n",
      "                message=messages,\n",
      "                finish_reason=row.finish_reason.name\n",
      "            )\n",
      "            choices.append(choice)\n",
      "            index += 1\n",
      "\n",
      "            return choices\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "from llm.core.service.dto import ChatCompletionRequest\n",
      "from importlib import import_module\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError\n",
      "from llm.core.entity import CHAT_COMPLETION_MODELS\n",
      "import copy\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "class ChatCompletionService:\n",
      "    def __init__(self, active_models, event_tracker):\n",
      "        self.active_models = active_models\n",
      "        self.event_tracker = event_tracker\n",
      "        self.__models = self.__load_model()\n",
      "\n",
      "    def __load_model(self):\n",
      "        models = {}\n",
      "        for platform in self.active_models:\n",
      "            Model = getattr(\n",
      "                import_module(\"llm.core.platform.{}.model\".format(platform)),\n",
      "                \"Model\",\n",
      "            )\n",
      "            models[platform] = Model()\n",
      "        return models\n",
      "\n",
      "    def __validate_platform_and_model(self, platform: str, model: str):\n",
      "        platform_model = self.__models.get(platform)\n",
      "        if not platform_model:\n",
      "            raise PlatformModelNotFoundError(\"platform\", platform)\n",
      "\n",
      "        if platform_model.require_model_validation:\n",
      "            if model not in CHAT_COMPLETION_MODELS.get(platform, []):\n",
      "                raise PlatformModelNotFoundError(\"model\", model)\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "async def generate_chat_completion(self, request: ChatCompletionRequest):\n",
      "        model_id = request.model\n",
      "        message_data = request.messages\n",
      "        config = request.config\n",
      "        response_format = request.response_format\n",
      "        tools = request.tools\n",
      "\n",
      "        self.__validate_platform_and_model(request.platform, model_id)\n",
      "\n",
      "        if not request.stream:\n",
      "            completion_response = await self.__models[request.platform].generate_chat_completion(\n",
      "                model=model_id,\n",
      "                messages=message_data,\n",
      "                config=config,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "            await self.event_tracker.log_llm_chat_completion(request, completion_response)\n",
      "\n",
      "            return completion_response\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "else:\n",
      "            completion_response = await self.__models[request.platform].generate_stream_chat_completion(\n",
      "                model=model_id,\n",
      "                messages=message_data,\n",
      "                config=config,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "\n",
      "            return (row async for row in self.__send_tracker_stream(request, completion_response))\n",
      "\n",
      "    async def __send_tracker_stream(self, request: ChatCompletionRequest, response):\n",
      "        full_completion_content = \"\"\n",
      "        full_completion_role = None\n",
      "        full_response = None\n",
      "        full_usage = None\n",
      "        async for row in response:\n",
      "            yield row\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "if len(row.choices) > 0:\n",
      "                if full_response is None:\n",
      "                    full_response = copy.deepcopy(row)\n",
      "                if row.choices[0].message.content is not None:\n",
      "                    full_completion_content += row.choices[0].message.content\n",
      "                if row.choices[0].message.role is not None:\n",
      "                    full_completion_role = row.choices[0].message.role\n",
      "            if row.usage.prompt_token > 0:\n",
      "                full_usage = row.usage\n",
      "\n",
      "        full_response.choices[0].message.content = full_completion_content\n",
      "        full_response.choices[0].message.role = full_completion_role\n",
      "        full_response.usage = full_usage\n",
      "\n",
      "        await self.event_tracker.log_llm_chat_completion(request, full_response)\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "from typing import Optional, List, Dict, Union, Any\n",
      "\n",
      "\n",
      "class ContentData(BaseModel):\n",
      "    type: str\n",
      "    text: Optional[str] = None\n",
      "    image_url: Optional[Dict[str, str]] = None\n",
      "\n",
      "\n",
      "class FunctionParameterData(BaseModel):\n",
      "    type: str\n",
      "    properties: Dict[Any, Any]\n",
      "    required: List[str]\n",
      "\n",
      "\n",
      "class FunctionData(BaseModel):\n",
      "    name: str\n",
      "    description: str\n",
      "    parameters: FunctionParameterData\n",
      "\n",
      "\n",
      "class ToolsDataRequest(BaseModel):\n",
      "    type: str\n",
      "    function: Optional[FunctionData]\n",
      "\n",
      "\n",
      "class MessageDataRequest(BaseModel):\n",
      "    role: str\n",
      "    content: Union[str, List[ContentData]]\n",
      "\n",
      "\n",
      "class ChatCompletionSetting(BaseModel):\n",
      "    max_token: Optional[int] = 256\n",
      "    temperature: Optional[float] = 0.7\n",
      "    top_p: Optional[float] = 1\n",
      "    top_k: Optional[float] = 1\n",
      "\n",
      "\n",
      "class ResponseFormatRequest(BaseModel):\n",
      "    type: Optional[str] = None\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "class ChatCompletionRequest(BaseModel):\n",
      "    platform: str\n",
      "    model: str\n",
      "    messages: List[MessageDataRequest]\n",
      "    tools: Optional[List[ToolsDataRequest]] = None\n",
      "    config: Optional[ChatCompletionSetting] = {}\n",
      "    stream: bool = False\n",
      "    task_name: Optional[str] = None\n",
      "    client_name: str\n",
      "    trace_id: str\n",
      "    session_id: Optional[str] = \"\"\n",
      "    response_format: Optional[ResponseFormatRequest] = None\n",
      "\n",
      "\n",
      "class TokenUsage(BaseModel):\n",
      "    prompt_token: Optional[int]\n",
      "    completion_token: Optional[int]\n",
      "    total_token: Optional[int]\n",
      "\n",
      "\n",
      "class FunctionResponse(BaseModel):\n",
      "    arguments: str\n",
      "    name: str\n",
      "\n",
      "\n",
      "class ToolCallResponse(BaseModel):\n",
      "    function: Optional[FunctionResponse] = None\n",
      "    type: str\n",
      "\n",
      "\n",
      "class MessageDataResponse(BaseModel):\n",
      "    role: Optional[str] = None\n",
      "    content: Optional[str] = None\n",
      "    tool_calls: Optional[List[ToolCallResponse]] = None\n",
      "\n",
      "\n",
      "class CompletionChoice(BaseModel):\n",
      "    index: int\n",
      "    message: MessageDataResponse\n",
      "    finish_reason: Optional[str] = None\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "class ChatCompletionResponse(BaseModel):\n",
      "    uuid: str\n",
      "    choices: Optional[List[CompletionChoice]] = []\n",
      "    usage: Optional[TokenUsage] = {}\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\exceptions.py\n",
      "\n",
      "class PlatformModelNotFoundError(Exception):\n",
      "    def __init__(self, field, value):\n",
      "        self.field = field\n",
      "        self.value = value\n",
      "\n",
      "\n",
      "class AIException(Exception):\n",
      "    def __init__(self, name, message, errors):\n",
      "        self.name = name\n",
      "        self.message = message\n",
      "        self.errors = errors\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\port.py\n",
      "\n",
      "import abc\n",
      "\n",
      "\n",
      "class AbstractChatCompletionModel(abc.ABC):\n",
      "    def __init__(self) -> None:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def generate_chat_completion(self, model, messages, config, response_format, tools):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def generate_stream_chat_completion(self, model, messages, config, response_format, tools):\n",
      "        raise NotImplementedError\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\conftest.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from pytest import fixture\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"package\")\n",
      "def client():\n",
      "    from llm.api import app\n",
      "    with TestClient(app) as client:\n",
      "        yield client\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "from pprint import pprint\n",
      "import requests\n",
      "import json\n",
      "\n",
      "\n",
      "def test_chat_completion(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"dummy\",\n",
      "        \"model\": \"gpt-4-turbo\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_openai(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"openai\",\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_openai_with_tools(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"What's the weather like in San franciso, tokyo and paris\"}\n",
      "        ],\n",
      "        \"tools\": [{\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "                \"name\": \"get_current_weather\",\n",
      "                \"description\": \"Get the current weather in a given location\",\n",
      "                \"parameters\": {\n",
      "                    \"type\": \"object\",\n",
      "                    \"properties\": {\n",
      "                        \"location\": {\n",
      "                            \"type\": \"string\",\n",
      "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                            },\n",
      "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
      "                    },\n",
      "                    \"required\": [\"location\"]\n",
      "                }\n",
      "            }\n",
      "        }],\n",
      "        \"platform\": \"openai\",\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "\"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "    assert len(response_data[\"choices\"][0][\"message\"][\"tool_calls\"]) > 0\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_vertexai(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"vertexai\",\n",
      "        \"model\": \"gemini-1.5-flash-001\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n"
     ]
    }
   ],
   "source": [
    "for doc in texts:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata[\"source\"]\n",
    "    print(\"Source : \" + metadata + \"\\n\\n\" + page_content  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source : test_repo\\rg-llm\\config.py\n",
      "\n",
      "from pydantic_settings import BaseSettings\n",
      "\n",
      "\n",
      "class GunicornConfig(BaseSettings):\n",
      "    NUM_WORKERS: int = 1\n",
      "    NUM_THREADS: int = 1\n",
      "    TIMEOUT: int = 30\n",
      "    LOG_LEVEL: str = \"INFO\"\n",
      "    LOG_DEBUG: str = \"DEBUG\"\n",
      "    LOG_ERROR: str = \"ERROR\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_API_GUNICORN_\"\n",
      "\n",
      "\n",
      "GUNICORN_CONFIG = GunicornConfig()\n",
      "\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "class RoguLogFilter(logging.Filter):\n",
      "    \"\"\"\n",
      "    Custom filter to exclude health check\n",
      "    \"\"\"\n",
      "\n",
      "    def filter(self, record):\n",
      "        try:\n",
      "            uri = str(record.args[2])\n",
      "        except Exception as e:\n",
      "            uri = None\n",
      "        return uri != \"/_health\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "class RoguFormatter(logging.Formatter):\n",
      "    \"\"\"\n",
      "    RG Logger Formatter\n",
      "    \"\"\"\n",
      "\n",
      "    datefmt = \"%Y-%m-%dT%H:%M:%SZ\"\n",
      "    fmt = 'RG_LOGGER: time=\"{asctime}\" name={name} lineno={lineno} level={levelname} msg=\"{msg}\" {extra}'\n",
      "\n",
      "    def format(self, record):\n",
      "\n",
      "        extra = []\n",
      "        if record.exc_info:\n",
      "            extra_str = f'exc_info=\"{self.formatException(record.exc_info)}\"'\n",
      "            extra.append(extra_str)\n",
      "\n",
      "        if hasattr(record, \"request_body\"):\n",
      "            extra_str = f'request_body=\"{record.request_body}\"'\n",
      "            extra.append(extra_str)\n",
      "\n",
      "        if hasattr(record, \"request_path\"):\n",
      "            extra_str = f\"request_path={record.request_path}\"\n",
      "            extra.append(extra_str)\n",
      "\n",
      "Source : test_repo\\rg-llm\\custom_logging.py\n",
      "\n",
      "extra_str = \" \".join(extra)\n",
      "        message = self.__class__.fmt.format(\n",
      "            asctime=self.formatTime(record, self.__class__.datefmt),\n",
      "            name=record.name,\n",
      "            lineno=record.lineno,\n",
      "            levelname=record.levelname.lower(),\n",
      "            msg=self._one_liner(record.getMessage()),\n",
      "            extra=self._one_liner(extra_str)\n",
      "        )\n",
      "\n",
      "        return message\n",
      "\n",
      "    def formatException(self, exc_info):\n",
      "        result = super().formatException(exc_info)\n",
      "        return self._one_liner(result)\n",
      "\n",
      "    def _one_liner(self, msg):\n",
      "        return msg.replace(\"\\n\", \"\\t|\\t\")\n",
      "\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "import threading\n",
      "import sys\n",
      "import traceback\n",
      "from logging.config import dictConfig\n",
      "from config import GUNICORN_CONFIG\n",
      "\n",
      "\n",
      "workers = GUNICORN_CONFIG.NUM_WORKERS\n",
      "threads = GUNICORN_CONFIG.NUM_THREADS\n",
      "timeout = GUNICORN_CONFIG.TIMEOUT\n",
      "worker_class = \"uvicorn.workers.UvicornWorker\"\n",
      "bind = '0.0.0.0:80'\n",
      "\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "logconfig_dict = {\n",
      "        \"version\": 1,\n",
      "        \"disable_existing_loggers\": True,\n",
      "        \"formatters\": {\n",
      "                \"custom\": {\n",
      "                    \"()\": \"custom_logging.RoguFormatter\"\n",
      "                }\n",
      "        },\n",
      "        \"filters\": {\n",
      "            \"exclude_health_check\": {\n",
      "                \"()\": \"custom_logging.RoguLogFilter\"\n",
      "            }\n",
      "        },\n",
      "        \"handlers\": {\n",
      "            \"access_console\": {\n",
      "                \"class\": \"logging.StreamHandler\",\n",
      "                \"formatter\": \"custom\",\n",
      "                \"filters\": [\"exclude_health_check\"]\n",
      "            },\n",
      "            \"console\": {\n",
      "                \"class\": \"logging.StreamHandler\",\n",
      "                \"formatter\": \"custom\"\n",
      "            },\n",
      "        },\n",
      "        \"loggers\": {\n",
      "            \"\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"elasticsearch\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"propagate\": False,\n",
      "\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "\"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"openai\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_DEBUG,\n",
      "                \"propagate\": False,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"llm\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"propagate\": False,\n",
      "                \"handlers\": [\"console\"]\n",
      "            },\n",
      "            \"gunicorn.access\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"access_console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"gunicorn.access\"\n",
      "            },\n",
      "            \"gunicorn.error\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"gunicorn.error\"\n",
      "            },\n",
      "            \"uvicorn.access\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"access_console\"],\n",
      "\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "\"propagate\": False,\n",
      "                \"qualname\": \"uvicorn.access\"\n",
      "            },\n",
      "            \"uvicorn.error\": {\n",
      "                \"level\": GUNICORN_CONFIG.LOG_LEVEL,\n",
      "                \"handlers\": [\"console\"],\n",
      "                \"propagate\": False,\n",
      "                \"qualname\": \"uvicorn.error\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "dictConfig(logconfig_dict)\n",
      "\n",
      "Source : test_repo\\rg-llm\\gunicorn.config.py\n",
      "\n",
      "def post_fork(server, worker):\n",
      "    server.log.info(\"Worker spawned (pid: %s)\", worker.pid)\n",
      "\n",
      "\n",
      "def pre_fork(server, worker):\n",
      "    pass\n",
      "\n",
      "\n",
      "def post_worker_init(worker):\n",
      "    pass\n",
      "\n",
      "\n",
      "def pre_exec(server):\n",
      "    server.log.info(\"Forked child, re-executing.\")\n",
      "\n",
      "\n",
      "def worker_int(worker):\n",
      "    worker.log.info(\"worker received INT or QUIT signal\")\n",
      "\n",
      "    # get traceback info\n",
      "    id2name = dict([(th.ident, th.name) for th in threading.enumerate()])\n",
      "    code = []\n",
      "    for threadId, stack in sys._current_frames().items():\n",
      "        code.append(\"\\n# Thread: %s(%d)\" % (id2name.get(threadId, \"\"), threadId))\n",
      "        for filename, lineno, name, line in traceback.extract_stack(stack):\n",
      "            code.append('File: \"%s\", line %d, in %s' % (filename, lineno, name))\n",
      "            if line:\n",
      "                code.append(\"  %s\" % (line.strip()))\n",
      "    worker.log.debug(\"\\n\".join(code))\n",
      "\n",
      "\n",
      "def worker_abort(worker):\n",
      "    worker.log.info(\"worker received SIGABRT signal\")\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\config.py\n",
      "\n",
      "import openai\n",
      "import warnings\n",
      "from pydantic_settings import BaseSettings\n",
      "import os\n",
      "\n",
      "\n",
      "warnings.filterwarnings('ignore')\n",
      "\n",
      "\n",
      "class ChatCompletionConfig(BaseSettings):\n",
      "    ACTIVE_MODELS: str = \"dummy;openai;vertexai\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_CHAT_COMPLETION_CONFIG_\"\n",
      "\n",
      "\n",
      "class OpenAIConfig(BaseSettings):\n",
      "    DEFAULT_ORG_KEY: str = \"\"\n",
      "    USE_ALLOCATION: bool = False\n",
      "    ORG_KEY_1: str = \"\"\n",
      "    ORG_KEY_2: str = \"\"\n",
      "    ORG_KEY_3: str = \"\"\n",
      "    ORG_KEY_4: str = \"\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_OPENAI_CONFIG_\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\config.py\n",
      "\n",
      "class VertexAIConfig(BaseSettings):\n",
      "    PROJECT_ID: str = \"silicon-airlock-153323\"\n",
      "    REGION: str = \"asia-southeast1\"\n",
      "    CREDENTIAL_FILE_CONTENT: str = \"\"\n",
      "    CREDENTIAL_FILE_PATH: str = os.path.join(os.getcwd(), \"sa-ai-platform-user.json\")\n",
      "    USE_ALLOCATION: bool = False\n",
      "\n",
      "    def get_secret(self):\n",
      "        if len(self.CREDENTIAL_FILE_CONTENT):\n",
      "            with open(self.CREDENTIAL_FILE_PATH, \"w\") as f:\n",
      "                f.write(self.CREDENTIAL_FILE_CONTENT)\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_LLM_VERTEXAI_CONFIG_\"\n",
      "\n",
      "\n",
      "class EventTrackingConfig(BaseSettings):\n",
      "    ACTIVE: bool = False\n",
      "    ENDPOINT: str = \"http://rg-event-tracking-api.staging.svc.cluster.local/api/v3/event/tracker\"\n",
      "\n",
      "    class Config:\n",
      "        env_prefix = \"RG_DS_CHATBOT_API_TRACKING_\"\n",
      "\n",
      "\n",
      "OPENAI_CONFIG = OpenAIConfig()\n",
      "VERTEXAI_CONFIG = VertexAIConfig()\n",
      "VERTEXAI_CONFIG.get_secret()\n",
      "\n",
      "CHAT_COMPLETION_CONFIG = ChatCompletionConfig()\n",
      "EVENT_TRACKING_CONFIG = EventTrackingConfig()\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\utils.py\n",
      "\n",
      "from uuid import uuid4\n",
      "\n",
      "\n",
      "def generate_uuid():\n",
      "    return str(uuid4())\n",
      "\n",
      "\n",
      "def pipe(data, *funcs):\n",
      "    \"\"\" Pipe a value through a sequence of functions\n",
      "\n",
      "    I.e. ``pipe(data, f, g, h)`` is equivalent to ``h(g(f(data)))``\n",
      "\n",
      "    We think of the value as progressing through a pipe of several\n",
      "    transformations, much like pipes in UNIX\n",
      "\n",
      "    ``$ cat data | f | g | h``\n",
      "\n",
      "    >>> double = lambda i: 2 * i\n",
      "    >>> pipe(3, double, str)\n",
      "    '6'\n",
      "\n",
      "    See Also:\n",
      "        compose\n",
      "        thread_first\n",
      "        thread_last\n",
      "    \"\"\"\n",
      "    for func in funcs:\n",
      "        data = func(data)\n",
      "    return data\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.applications import FastAPI\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError, AIException\n",
      "from llm.api.views import error_response\n",
      "from typing import Union\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "async def request_validation_handler(_: Request, error: RequestValidationError):\n",
      "    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\n",
      "                         for e in error.errors()]\n",
      "    return error_response(error, 400, validation_errors)\n",
      "\n",
      "\n",
      "async def http_exception_handler(_, error: StarletteHTTPException):\n",
      "    return error_response(error)\n",
      "\n",
      "\n",
      "async def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\n",
      "    logger.error(f\"Not Found for {error.field}: {error.value}\")\n",
      "    return error_response(error, 400)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "async def ai_exception_handler(_, error):\n",
      "    logger.error(f\"LLM error with detail: {error}\")\n",
      "    return error_response(error, 500)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "def register_error_handlers(app: FastAPI) -> FastAPI:\n",
      "    app.exception_handler(RequestValidationError)(request_validation_handler)\n",
      "    app.exception_handler(StarletteHTTPException)(http_exception_handler)\n",
      "    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\n",
      "    app.exception_handler(AIException)(ai_exception_handler)\n",
      "    return app\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\events.py\n",
      "\n",
      "from fastapi.applications import FastAPI\n",
      "from llm.client.common.async_http import async_http_open, async_http_close\n",
      "\n",
      "\n",
      "async def startup() -> None:\n",
      "    await async_http_open()\n",
      "\n",
      "\n",
      "async def shutdown() -> None:\n",
      "    await async_http_close()\n",
      "\n",
      "\n",
      "def register_events(app: FastAPI) -> FastAPI:\n",
      "    app.add_event_handler(\"startup\", startup)\n",
      "    app.add_event_handler(\"shutdown\", shutdown)\n",
      "    return app\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\factory.py\n",
      "\n",
      "from fastapi.applications import FastAPI\n",
      "\n",
      "from llm.api.routers import health\n",
      "from llm.api.routers import chat_completion\n",
      "from llm.api.error_handlers import register_error_handlers\n",
      "from llm.api.events import register_events\n",
      "from llm.api.middleware import register_middlewares\n",
      "from llm.utils import pipe\n",
      "\n",
      "\n",
      "def create_instance() -> FastAPI:\n",
      "    return FastAPI()\n",
      "\n",
      "\n",
      "def init_database(app: FastAPI) -> FastAPI:\n",
      "    return app\n",
      "\n",
      "\n",
      "def register_routers(app: FastAPI) -> FastAPI:\n",
      "    app.include_router(health.router)\n",
      "    app.include_router(chat_completion.router)\n",
      "    return app\n",
      "\n",
      "\n",
      "def init_app() -> FastAPI:\n",
      "    app: FastAPI = pipe(\n",
      "        create_instance(),\n",
      "        init_database,\n",
      "        register_events,\n",
      "        register_middlewares,\n",
      "        register_error_handlers,\n",
      "        register_routers\n",
      "    )\n",
      "    return app\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\middleware.py\n",
      "\n",
      "import logging\n",
      "from typing import Callable\n",
      "\n",
      "from fastapi import Request, Response\n",
      "from starlette.middleware.base import BaseHTTPMiddleware\n",
      "from fastapi.applications import FastAPI\n",
      "from llm.api.views import error_response\n",
      "import time\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\middleware.py\n",
      "\n",
      "class RoguLoggerMiddleware(BaseHTTPMiddleware):\n",
      "\n",
      "    # ref: https://github.com/encode/starlette/issues/495\n",
      "    async def _reset_receive(self, request, content):\n",
      "        async def receive():\n",
      "            return content\n",
      "        request._receive = receive\n",
      "\n",
      "    async def dispatch(self, request: Request, call_next: Callable) -> Response:\n",
      "        start_time = time.monotonic()\n",
      "\n",
      "        try:\n",
      "            response = await call_next(request)\n",
      "            processing_time = time.monotonic() - start_time\n",
      "            response.headers[\"x-processing-time\"] = str(round(processing_time*1000, 2))\n",
      "            return response\n",
      "        except Exception as e:\n",
      "            # catch any exception during performing request\n",
      "            logger.exception(e)\n",
      "            return error_response(e)\n",
      "\n",
      "\n",
      "def register_middlewares(app: FastAPI) -> FastAPI:\n",
      "    app.add_middleware(RoguLoggerMiddleware)\n",
      "    return app\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "import re\n",
      "import http\n",
      "import json\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import AIException, PlatformModelNotFoundError\n",
      "from typing import Dict, List, Optional, Type\n",
      "from fastapi.responses import JSONResponse\n",
      "\n",
      "\n",
      "def _camel_to_snake(name):\n",
      "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
      "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).upper()\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "def chat_completion_response(response):\n",
      "    output = {\n",
      "        \"uuid\": response.uuid,\n",
      "        \"choices\": [choice.dict() for choice in response.choices],\n",
      "        \"usage\": response.usage.dict()\n",
      "    }\n",
      "\n",
      "    return JSONResponse(\n",
      "        dict(\n",
      "            data=output,\n",
      "            status=\"success\",\n",
      "            message=\"success\"\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "async def stream_chat_completion_response(response):\n",
      "    async for row in response:\n",
      "        output = {\n",
      "            \"uuid\": row.uuid,\n",
      "            \"choices\": [choice.dict() for choice in row.choices],\n",
      "            \"usage\": row.usage.dict()\n",
      "        }\n",
      "        yield json.dumps(output) + \"\\n\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "def error_response(error: Exception, code: int = 500, descriptions: Optional[List] = None):\n",
      "    if descriptions is None:\n",
      "        descriptions = []\n",
      "\n",
      "    status = http.HTTPStatus(code).name\n",
      "\n",
      "    if isinstance(error, StarletteHTTPException):\n",
      "        name = error.detail.replace(\" \", \"\")\n",
      "        code = error.status_code\n",
      "    else:\n",
      "        name = error.__class__.__name__\n",
      "\n",
      "    if isinstance(error, AIException):\n",
      "        if error.args[2] is not None:\n",
      "            code = error.args[2].status_code\n",
      "            message = str(error.args[2].message)\n",
      "            error_field = _camel_to_snake(error.args[2].__class__.__name__)\n",
      "            if code is None:\n",
      "                code = 500\n",
      "            status = http.HTTPStatus(code).name\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "else:\n",
      "            message = error.args[1]\n",
      "            error_field = _camel_to_snake(error._error_code.name)\n",
      "            if error._error_code == AIException.ErrorCodes.InvalidRequest:\n",
      "                code = 400\n",
      "                status = http.HTTPStatus(code).name\n",
      "                error_field = _camel_to_snake(openai.error.InvalidRequestError.__name__)\n",
      "\n",
      "        error_message = [\n",
      "                {\"field\": error_field, \"message\": message}\n",
      "            ]\n",
      "    elif isinstance(error, PlatformModelNotFoundError):\n",
      "        error_message = [\n",
      "            {\"field\": error.args[0], \"message\": f\"Not found for {error.args[0]}: {error.args[1]}\"}\n",
      "        ]\n",
      "\n",
      "    else:\n",
      "        error_message = [\n",
      "                {\"field\": desc[0], \"message\": desc[1]} for desc in descriptions\n",
      "        ]\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "return JSONResponse(\n",
      "        dict(\n",
      "            status=status,\n",
      "            error=_camel_to_snake(name),\n",
      "            code=code,\n",
      "            error_code=error_message,\n",
      "            metadata={}\n",
      "        ),\n",
      "        status_code=code,\n",
      "    )\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\__init__.py\n",
      "\n",
      "from llm.api.factory import init_app\n",
      "\n",
      "\n",
      "app = init_app()\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\chat_completion.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.routing import APIRouter\n",
      "from fastapi.responses import JSONResponse\n",
      "from llm.api.views import chat_completion_response, stream_chat_completion_response\n",
      "from llm.core.service.dto import ChatCompletionRequest\n",
      "from llm.core.service.chat_completion import ChatCompletionService\n",
      "from llm.config import CHAT_COMPLETION_CONFIG\n",
      "from fastapi.responses import StreamingResponse\n",
      "from llm.client.event_tracking.client import EventTrackingApi\n",
      "import logging\n",
      "\n",
      "\n",
      "router = APIRouter()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "event_tracker = EventTrackingApi()\n",
      "active_models = CHAT_COMPLETION_CONFIG.ACTIVE_MODELS.split(\";\")\n",
      "chat_completion_service = ChatCompletionService(active_models, event_tracker)\n",
      "\n",
      "\n",
      "@router.post(\"/chat/completion\")\n",
      "async def chat_completion(chat_request: ChatCompletionRequest, request: Request):\n",
      "    raw_response = await chat_completion_service.generate_chat_completion(chat_request)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\chat_completion.py\n",
      "\n",
      "if not chat_request.stream:\n",
      "        return chat_completion_response(raw_response)\n",
      "    else:\n",
      "        response = stream_chat_completion_response(raw_response)\n",
      "        return StreamingResponse(response, media_type=\"text/event-stream\")\n",
      "\n",
      "\n",
      "@router.post(\"/dummy/completion\")\n",
      "async def chat_completion_dummy(chat_request: ChatCompletionRequest, request: Request):\n",
      "\n",
      "    return JSONResponse(dict(status=\"OK\"), status_code=200)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\routers\\health.py\n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.responses import JSONResponse\n",
      "from fastapi.routing import APIRouter\n",
      "\n",
      "router = APIRouter()\n",
      "\n",
      "\n",
      "@router.get(\"/_health\")\n",
      "async def health(request: Request):\n",
      "    return JSONResponse(dict(status=\"OK\"), status_code=200)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "import logging\n",
      "import aiohttp\n",
      "\n",
      "from typing import Tuple, Optional\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "class AsyncHttp:\n",
      "\n",
      "    __slots__ = (\"_client\",)\n",
      "\n",
      "    def __init__(self) -> None:\n",
      "        self._client = None\n",
      "\n",
      "    async def open(self) -> aiohttp.ClientSession:\n",
      "        if not self._client:\n",
      "            self._client = aiohttp.ClientSession()\n",
      "            logger.info(\"Async HTTP client session opened\")\n",
      "        return self._client\n",
      "\n",
      "    async def close(self) -> None:\n",
      "        if self._client:\n",
      "            await self._client.close()\n",
      "            logger.info(\"Async HTTP client session closed\")\n",
      "            self._client = None\n",
      "\n",
      "    async def get(self, url, headers: Optional[dict] = {}) -> Tuple[bytes, int]:\n",
      "        session = await self.open()\n",
      "        async with session.get(url, headers=headers) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "async def get_params(self, url, headers: Optional[dict] = {}, params: Optional[dict] = {}) -> Tuple[bytes, int]:\n",
      "        session = await self.open()\n",
      "        async with session.get(url, headers=headers, params=params) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "\n",
      "    async def post(self, url, data: str, headers: Optional[dict] = None) -> Tuple[bytes, int]:\n",
      "        if headers is None:\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "\n",
      "        session = await self.open()\n",
      "        async with session.post(url, data=data, headers=headers) as r:\n",
      "            content = await r.read()\n",
      "            return content, r.status\n",
      "\n",
      "    async def post_forget(self, url, data: str, headers: Optional[dict] = None):\n",
      "        if headers is None:\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "\n",
      "        session = await self.open()\n",
      "        async with session.post(url, data=data, headers=headers):\n",
      "            pass\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "_async_http_client = AsyncHttp()\n",
      "async_http_open = _async_http_client.open\n",
      "async_http_close = _async_http_client.close\n",
      "async_get = _async_http_client.get\n",
      "async_get_params = _async_http_client.get_params\n",
      "async_post = _async_http_client.post\n",
      "async_post_forget = _async_http_client.post_forget\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "import logging\n",
      "import asyncio\n",
      "import orjson\n",
      "\n",
      "from datetime import datetime, timezone\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from llm.client.common.async_http import async_post_forget\n",
      "from llm.client.event_tracking.entity import (\n",
      "    PostEventTrackingRequestData,\n",
      "    EventLLMChatCompletion\n",
      ")\n",
      "from llm.core.service.dto import ChatCompletionRequest, ChatCompletionResponse\n",
      "from llm.config import EVENT_TRACKING_CONFIG\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "class EventTrackingApi:\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            endpoint: str = EVENT_TRACKING_CONFIG.ENDPOINT,\n",
      "            active: bool = EVENT_TRACKING_CONFIG.ACTIVE) -> None:\n",
      "\n",
      "        self._endpoint = endpoint\n",
      "        self._active = active\n",
      "\n",
      "    async def log_llm_chat_completion(\n",
      "            self,\n",
      "            chat_completion_request: ChatCompletionRequest,\n",
      "            chat_completion_response: ChatCompletionResponse,\n",
      "            ) -> None:\n",
      "\n",
      "        if len(chat_completion_response.choices) > 0:\n",
      "            if chat_completion_response.choices[0].message.tool_calls is not None:\n",
      "                tool_calls = [x.dict() for x in chat_completion_response.choices[0].message.tool_calls]\n",
      "                completion = orjson.dumps(tool_calls)\n",
      "            else:\n",
      "                completion = chat_completion_response.choices[0].message.content\n",
      "        else:\n",
      "            completion = \"\"\n",
      "\n",
      "        messages = [x.dict() for x in chat_completion_request.messages]\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "token_usage = {\n",
      "            \"prompt_token\": chat_completion_response.usage.prompt_token,\n",
      "            \"completion_token\": chat_completion_response.usage.completion_token,\n",
      "        }\n",
      "\n",
      "        event = EventLLMChatCompletion(\n",
      "            uuid=chat_completion_response.uuid,\n",
      "            trace_id=chat_completion_request.trace_id,\n",
      "            session_id=chat_completion_request.session_id,\n",
      "            message=orjson.dumps(messages),\n",
      "            task_name=chat_completion_request.task_name,\n",
      "            model=chat_completion_request.model,\n",
      "            platform=chat_completion_request.platform,\n",
      "            completion=completion,\n",
      "            client_name=chat_completion_request.client_name,\n",
      "            token_usage=token_usage,\n",
      "        )\n",
      "\n",
      "        await self._send(\"dsChatbotLlmChatCompletion\", event)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\client.py\n",
      "\n",
      "async def _send(self, event_type: str, context: BaseModel, memberId: str = None) -> None:\n",
      "        if self._active:\n",
      "            json_data = PostEventTrackingRequestData(\n",
      "                memberId=memberId,\n",
      "                isLogged=False,\n",
      "                eventType=event_type,\n",
      "                clientTimestamp=datetime.now(timezone.utc).isoformat(),\n",
      "                context=context.model_dump_json()\n",
      "            ).model_dump_json()\n",
      "\n",
      "            headers = {\"content-type\": \"application/json\"}\n",
      "            asyncio.create_task(async_post_forget(self._endpoint, data=json_data, headers=headers))\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\event_tracking\\entity.py\n",
      "\n",
      "from typing import List, Optional, Union, Any, Dict\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class EventLLMChatCompletion(BaseModel):\n",
      "    uuid: str\n",
      "    session_id: str\n",
      "    trace_id: str\n",
      "    platform: str\n",
      "    model: str\n",
      "    message: str\n",
      "    completion: Optional[str] = None\n",
      "    task_name: Optional[str]\n",
      "    client_name: str\n",
      "    token_usage: Dict[str, float]\n",
      "\n",
      "\n",
      "class PostEventTrackingRequestData(BaseModel):\n",
      "    id: Optional[str] = None\n",
      "    sessionId: Optional[str] = None\n",
      "    cookiesId: Optional[str] = None\n",
      "    deviceId: Optional[str] = None\n",
      "    memberId: Optional[str] = None\n",
      "    isLogged: bool = False\n",
      "    eventType: str\n",
      "    clientDevice: Optional[str] = None\n",
      "    clientUA: Optional[str] = None\n",
      "    appVersion: Optional[str] = None\n",
      "    clientOS: Optional[str] = None\n",
      "    clientOSVersion: Optional[str] = None\n",
      "    clientTimestamp: Optional[str] = None\n",
      "    context: Union[str, Any]\n",
      "    connectionType: Optional[str] = None\n",
      "    source: str = \"backend\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "from openai import AsyncOpenAI\n",
      "from llm.core.service.exceptions import AIException\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "class OpenAIClient:\n",
      "    def __init__(self):\n",
      "        self.client = AsyncOpenAI()\n",
      "\n",
      "    async def send_chat_request(self, messages, model, request_settings, stream, response_format, tools, client_org):\n",
      "        self.client.organization = client_org\n",
      "        if stream:\n",
      "            stream_options = {\"include_usage\": True}\n",
      "        else:\n",
      "            stream_options = None\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\client.py\n",
      "\n",
      "try:\n",
      "            response = await self.client.chat.completions.create(\n",
      "                model=model,\n",
      "                messages=messages,\n",
      "                temperature=request_settings.temperature,\n",
      "                max_tokens=request_settings.max_tokens,\n",
      "                top_p=request_settings.top_p,\n",
      "                stream=stream,\n",
      "                stream_options=stream_options,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "        except Exception as ex:\n",
      "            raise AIException(\n",
      "                \"openai\",\n",
      "                \"OpenAI service failed to complete the chat\",\n",
      "                ex\n",
      "            )\n",
      "\n",
      "        return response\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\openai\\entity.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "\n",
      "class OpenAIRequestSettings(BaseModel):\n",
      "    temperature: float\n",
      "    max_tokens: int\n",
      "    top_p: float\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "from llm.config import VERTEXAI_CONFIG\n",
      "import vertexai\n",
      "from vertexai.generative_models import GenerativeModel, GenerationConfig, Image\n",
      "from google.oauth2.service_account import Credentials\n",
      "from llm.core.entity import CHAT_COMPLETION_MODELS\n",
      "from llm.client.vertexai.entity import UserContent, SAFETY_SETTING\n",
      "from llm.client.common.async_http import async_get\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "class VertexAIClient:\n",
      "    def __init__(self, location=None, credential_file=None):\n",
      "        if credential_file is None:\n",
      "            credential_file = VERTEXAI_CONFIG.CREDENTIAL_FILE_PATH\n",
      "        else:\n",
      "            credential_file = credential_file\n",
      "        credentials = Credentials.from_service_account_file(credential_file)\n",
      "\n",
      "        if location is None:\n",
      "            location = VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        vertexai.init(\n",
      "            project=VERTEXAI_CONFIG.PROJECT_ID,\n",
      "            location=location,\n",
      "            credentials=credentials\n",
      "        )\n",
      "\n",
      "        self.model_client = self.__load_model()\n",
      "\n",
      "    def __load_model(self):\n",
      "        model_client = {}\n",
      "        for model_id in CHAT_COMPLETION_MODELS.get(\"vertexai\", []):\n",
      "            model_client[model_id] = GenerativeModel(model_id)\n",
      "        return model_client\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "async def generate_response(\n",
      "        self,\n",
      "        model: str,\n",
      "        user_content: UserContent,\n",
      "        generation_config: GenerationConfig,\n",
      "        stream: bool,\n",
      "        system_message: str = None\n",
      "    ):\n",
      "        if system_message is not None:\n",
      "            model_client = GenerativeModel(model, system_instruction[system_message])\n",
      "        else:\n",
      "            model_client = self.model_client.get(model)\n",
      "\n",
      "        contents = [user_content.text]\n",
      "        if user_content.images is not None:\n",
      "            images = [await self._load_image_from_url(image) for image in user_content.images]\n",
      "            contents.extend(images)\n",
      "\n",
      "        response = await model_client.generate_content_async(\n",
      "            contents=contents,\n",
      "            generation_config=generation_config,\n",
      "            safety_settings=SAFETY_SETTING,\n",
      "            stream=stream\n",
      "        )\n",
      "\n",
      "        return response\n",
      "\n",
      "    async def _get_image_bytes_from_url(self, image_url: str) -> bytes:\n",
      "        response, status = await async_get(image_url)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\client.py\n",
      "\n",
      "return response\n",
      "\n",
      "    async def _load_image_from_url(self, image_url: str) -> Image:\n",
      "        image_bytes = await self._get_image_bytes_from_url(image_url)\n",
      "        return Image.from_bytes(image_bytes)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\vertexai\\entity.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "from vertexai.preview.generative_models import HarmCategory, HarmBlockThreshold\n",
      "from typing import Optional, List\n",
      "\n",
      "\n",
      "class UserContent(BaseModel):\n",
      "    text: str\n",
      "    images: Optional[List] = None\n",
      "\n",
      "\n",
      "SAFETY_SETTING = {\n",
      "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
      "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
      "}\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\entity.py\n",
      "\n",
      "CHAT_COMPLETION_MODELS = {\n",
      "    \"openai\": [\n",
      "        \"gpt-3.5-turbo\",\n",
      "        \"gpt-3.5-turbo-1106\",\n",
      "        \"gpt-3.5-turbo-0125\",\n",
      "        \"gpt-3.5-turbo-16k\",\n",
      "        \"gpt-4\",\n",
      "        \"gpt-4-turbo\",\n",
      "        \"gpt-4-0125-preview\",\n",
      "        \"gpt-4-1106-preview\",\n",
      "        \"gpt-4o\",\n",
      "        \"ft:gpt-3.5-turbo-0125:ruangguru:intentclass:9Wjgagxi\"\n",
      "    ],\n",
      "    \"vertexai\": [\n",
      "        \"gemini-1.5-flash-001\",\n",
      "        \"gemini-1.5-pro-001\"\n",
      "    ]\n",
      "}\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "import glob\n",
      "import yaml\n",
      "import os\n",
      "import mmh3\n",
      "from llm.utils import generate_uuid\n",
      "\n",
      "\n",
      "SEGMENT_SIZE = 1000\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "class GeneralAllocation:\n",
      "    def __init__(self, platform_name):\n",
      "        self.config_directory = \"llm/core/allocation/{}/config\".format(platform_name)\n",
      "        self.config = {}\n",
      "        self.__create_config()\n",
      "\n",
      "    def __create_config(self):\n",
      "        directory = glob.glob(self.config_directory + \"/*.yaml\", recursive=True)\n",
      "\n",
      "        for filepath in directory:\n",
      "            config_name = os.path.basename(filepath).split(\".\")[0]\n",
      "            with open(filepath) as f:\n",
      "                config_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
      "            self.config[config_name] = config_yaml\n",
      "\n",
      "            self.config[config_name] = self.__compute_allocation(config_name)\n",
      "\n",
      "    def get_variant(self, config_name, unit_id=None):\n",
      "        if unit_id is None:\n",
      "            unit_id = generate_uuid()\n",
      "\n",
      "        segment = self.get_segment(unit_id)\n",
      "        if config_name not in self.config.keys():\n",
      "            return None\n",
      "\n",
      "        variants = self.config[config_name][\"variants\"]\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\allocation\\__init__.py\n",
      "\n",
      "for variant in self.config[config_name][\"variants\"]:\n",
      "            if segment < variant[\"offset\"]:\n",
      "                return variant[\"value\"]\n",
      "\n",
      "        return variants[0][\"value\"]\n",
      "\n",
      "    def __compute_allocation(self, config_name):\n",
      "        config = self.config[config_name]\n",
      "        list_variant = []\n",
      "        offset = 0\n",
      "        for row in config[\"variants\"]:\n",
      "            upperbound = int(row[\"weight\"]*SEGMENT_SIZE)\n",
      "            offset = offset + upperbound\n",
      "            list_variant.append(\n",
      "                {\n",
      "                    \"value\": row[\"value\"],\n",
      "                    \"weight\": row[\"weight\"],\n",
      "                    \"offset\": offset\n",
      "                }\n",
      "            )\n",
      "        config[\"variants\"] = list_variant\n",
      "\n",
      "        return config\n",
      "\n",
      "    def get_segment(self, unit_id):\n",
      "        segment = mmh3.hash(unit_id, signed=False) % SEGMENT_SIZE\n",
      "\n",
      "        return segment\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.utils import generate_uuid\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.require_model_validation = False\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        choices = CompletionChoice(\n",
      "            index=0,\n",
      "            message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\n",
      "        )\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=[choices],\n",
      "            usage={\n",
      "                \"prompt_token\": 100,\n",
      "                \"completion_token\": 100,\n",
      "                \"total_token\": 200\n",
      "            }\n",
      "        )\n",
      "        return response\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\dummy\\model.py\n",
      "\n",
      "async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        async def post_process_stream():\n",
      "            for i in range(10):\n",
      "                choices = CompletionChoice(\n",
      "                    index=i,\n",
      "                    message=MessageDataResponse(role=\"assistant\", content=\"halo, saya siap membantu\")\n",
      "                )\n",
      "                response = ChatCompletionResponse(\n",
      "                    uuid=generate_uuid(),\n",
      "                    choices=[choices],\n",
      "                    usage={\n",
      "                        \"prompt_token\": 100,\n",
      "                        \"completion_token\": 100,\n",
      "                        \"total_token\": 200\n",
      "                    }\n",
      "                )\n",
      "                yield response\n",
      "\n",
      "        return (row async for row in post_process_stream())\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\allocation.py\n",
      "\n",
      "from llm.core.allocation import GeneralAllocation\n",
      "from llm.config import OPENAI_CONFIG\n",
      "from llm.core.platform.openai.entity import MODEL_VARIANT, ORG_KEYS\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class Allocation(GeneralAllocation):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def get_variant_org_key(self, model_id):\n",
      "        if not OPENAI_CONFIG.USE_ALLOCATION:\n",
      "            return OPENAI_CONFIG.DEFAULT_ORG_KEY\n",
      "\n",
      "        variant_model = MODEL_VARIANT.get(model_id)\n",
      "        variant_value = self.get_variant(variant_model)\n",
      "\n",
      "        if variant_value is None:\n",
      "            return OPENAI_CONFIG.DEFAULT_ORG_KEY\n",
      "\n",
      "        variant_org_key = ORG_KEYS.get(variant_value, OPENAI_CONFIG.DEFAULT_ORG_KEY)\n",
      "        logger.info(f\"use openai varian for model_id: {model_id}, with value {variant_value}:{variant_org_key}\")\n",
      "\n",
      "        return variant_org_key\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\entity.py\n",
      "\n",
      "from llm.config import OPENAI_CONFIG\n",
      "\n",
      "\n",
      "ORG_KEYS = {\n",
      "    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\n",
      "    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\n",
      "    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\n",
      "    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\n",
      "}\n",
      "\n",
      "MODEL_VARIANT = {\n",
      "    \"gpt-3.5-turbo\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\n",
      "    \"gpt-4\": \"gpt-4\",\n",
      "    \"gpt-4-turbo\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4o\": \"gpt-4o\"\n",
      "\n",
      "}\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.utils import generate_uuid\n",
      "from llm.client.openai.client import OpenAIClient\n",
      "from llm.client.openai.entity import OpenAIRequestSettings\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "from llm.core.platform.openai.allocation import Allocation\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.client = OpenAIClient()\n",
      "        self.require_model_validation = True\n",
      "        self.allocation = Allocation(platform_name=\"openai\")\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools,\n",
      "    ):\n",
      "        openai_config = OpenAIRequestSettings(\n",
      "            temperature=config.temperature,\n",
      "            max_tokens=config.max_token,\n",
      "            top_p=config.top_p\n",
      "        )\n",
      "\n",
      "        org_key = self.allocation.get_variant_org_key(model)\n",
      "\n",
      "        openai_response = await self.client.send_chat_request(\n",
      "            messages=messages,\n",
      "            model=model,\n",
      "            stream=False,\n",
      "            request_settings=openai_config,\n",
      "            response_format=response_format,\n",
      "            tools=tools,\n",
      "            client_org=org_key\n",
      "        )\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "choices = [choice.dict() for choice in openai_response.choices]\n",
      "        choices = [CompletionChoice(**choice) for choice in choices]\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=choices,\n",
      "            usage={\n",
      "                \"prompt_token\": openai_response.usage.prompt_tokens,\n",
      "                \"completion_token\": openai_response.usage.completion_tokens,\n",
      "                \"total_token\": openai_response.usage.total_tokens\n",
      "            }\n",
      "        )\n",
      "        return response\n",
      "\n",
      "    async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        uuid = generate_uuid()\n",
      "        openai_config = OpenAIRequestSettings(\n",
      "            temperature=config.temperature,\n",
      "            max_tokens=config.max_token,\n",
      "            top_p=config.top_p\n",
      "        )\n",
      "        org_key = self.allocation.get_variant_org_key(model)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "openai_response = await self.client.send_chat_request(\n",
      "            messages=messages,\n",
      "            model=model,\n",
      "            stream=True,\n",
      "            request_settings=openai_config,\n",
      "            response_format=response_format,\n",
      "            tools=tools,\n",
      "            client_org=org_key\n",
      "        )\n",
      "\n",
      "        async def post_process_stream(uuid, openai_response):\n",
      "            async for chunk in openai_response:\n",
      "                choices = []\n",
      "                for choice in chunk.choices:\n",
      "                    message_response = MessageDataResponse(\n",
      "                        content=choice.delta.content,\n",
      "                        role=choice.delta.role\n",
      "                    )\n",
      "                    _choices = CompletionChoice(\n",
      "                        index=choice.index, message=message_response, finish_reason=choice.finish_reason\n",
      "                    )\n",
      "                    choices.append(_choices)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\model.py\n",
      "\n",
      "if chunk.usage is not None:\n",
      "                    usage = {\n",
      "                        \"prompt_token\": chunk.usage.prompt_tokens,\n",
      "                        \"completion_token\": chunk.usage.completion_tokens,\n",
      "                        \"total_token\": chunk.usage.total_tokens\n",
      "                    }\n",
      "                else:\n",
      "                    usage = {\n",
      "                        \"prompt_token\": 0,\n",
      "                        \"completion_token\": 0,\n",
      "                        \"total_token\": 0\n",
      "                    }\n",
      "\n",
      "                response = ChatCompletionResponse(\n",
      "                    uuid=uuid,\n",
      "                    choices=choices,\n",
      "                    usage=usage\n",
      "\n",
      "                )\n",
      "\n",
      "                yield response\n",
      "\n",
      "        response = (row async for row in post_process_stream(uuid, openai_response))\n",
      "        return response\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\allocation.py\n",
      "\n",
      "from llm.core.allocation import GeneralAllocation\n",
      "from llm.config import VERTEXAI_CONFIG\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class Allocation(GeneralAllocation):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super().__init__(*args, **kwargs)\n",
      "\n",
      "    def get_variant_region(self):\n",
      "        if not VERTEXAI_CONFIG.USE_ALLOCATION:\n",
      "            return VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        variant_value = self.get_variant(\"region\")\n",
      "\n",
      "        if variant_value is None:\n",
      "            return VERTEXAI_CONFIG.REGION\n",
      "\n",
      "        logger.info(f\"use vertex ai region with value {variant_value}\")\n",
      "\n",
      "        return variant_value\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\entity.py\n",
      "\n",
      "REGION_VARIANT = [\n",
      "    \"us-central1\",\n",
      "    \"us-east4\",\n",
      "    \"us-west1\",\n",
      "    \"us-west4\",\n",
      "    \"northamerica-northeast1\",\n",
      "    \"europe-west9\",\n",
      "    \"europe-west2\",\n",
      "    \"europe-west3\",\n",
      "    \"europe-west4\",\n",
      "    \"europe-west9\",\n",
      "    \"asia-northeast1\",\n",
      "    \"asia-northeast3\",\n",
      "    \"asia-southeast1\"\n",
      "]\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "from llm.core.service.port import AbstractChatCompletionModel\n",
      "from vertexai.generative_models import Image, GenerationConfig\n",
      "from llm.client.vertexai.client import VertexAIClient\n",
      "from llm.client.vertexai.entity import UserContent\n",
      "from llm.core.platform.vertexai.allocation import Allocation\n",
      "from llm.utils import generate_uuid\n",
      "from llm.core.service.dto import (\n",
      "    MessageDataRequest,\n",
      "    MessageDataResponse,\n",
      "    ChatCompletionSetting,\n",
      "    CompletionChoice,\n",
      "    ChatCompletionResponse\n",
      ")\n",
      "from llm.core.platform.vertexai.entity import REGION_VARIANT\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "class Model(AbstractChatCompletionModel):\n",
      "    def __init__(self):\n",
      "        self.require_model_validation = True\n",
      "        self.allocation = Allocation(platform_name=\"vertexai\")\n",
      "        self.clients = self.__load_client()\n",
      "\n",
      "    def __load_client(self):\n",
      "        clients = {}\n",
      "        for region in REGION_VARIANT:\n",
      "            clients[region] = VertexAIClient(location=region)\n",
      "        return clients\n",
      "\n",
      "    def __get_client(self):\n",
      "        varian_region = self.allocation.get_variant_region()\n",
      "        return self.clients.get(varian_region)\n",
      "\n",
      "    async def generate_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        system_message = self._get_system_message(messages)\n",
      "        user_content = self._get_user_content(messages)\n",
      "        generation_config = self._construct_generation_config(config, response_format)\n",
      "\n",
      "        vertexai_client = self.__get_client()\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "# TODO implement tools in vertexai\n",
      "        vertexai_response = await vertexai_client.generate_response(\n",
      "            model=model,\n",
      "            user_content=user_content,\n",
      "            stream=False,\n",
      "            generation_config=generation_config,\n",
      "            system_message=system_message\n",
      "        )\n",
      "\n",
      "        choices = self._parse_choices(vertexai_response.candidates)\n",
      "\n",
      "        usage = {\n",
      "            \"prompt_token\": vertexai_response.usage_metadata.prompt_token_count,\n",
      "            \"completion_token\": vertexai_response.usage_metadata.candidates_token_count,\n",
      "            \"total_token\": vertexai_response.usage_metadata.total_token_count\n",
      "        }\n",
      "\n",
      "        response = ChatCompletionResponse(\n",
      "            uuid=generate_uuid(),\n",
      "            choices=choices,\n",
      "            usage=usage\n",
      "        )\n",
      "\n",
      "        return response\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "async def generate_stream_chat_completion(\n",
      "        self,\n",
      "        model: str,\n",
      "        messages: MessageDataRequest,\n",
      "        config: ChatCompletionSetting,\n",
      "        response_format,\n",
      "        tools\n",
      "    ):\n",
      "        uuid = generate_uuid()\n",
      "        system_message = self._get_system_message(messages)\n",
      "        user_content = self._get_user_content(messages)\n",
      "        generation_config = self._construct_generation_config(config, response_format)\n",
      "\n",
      "        vertexai_client = self.__get_client()\n",
      "\n",
      "        # TODO implement tools in vertexai\n",
      "        vertexai_response = await vertexai_client.generate_response(\n",
      "            model=model,\n",
      "            user_content=user_content,\n",
      "            stream=True,\n",
      "            generation_config=generation_config,\n",
      "            system_message=system_message\n",
      "        )\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "async def post_process_stream(uuid, vertexai_response):\n",
      "            async for chunk in vertexai_response:\n",
      "                if chunk is None:\n",
      "                    continue\n",
      "                choices = []\n",
      "                index = 0\n",
      "                for row in chunk.candidates:\n",
      "                    if len(row.content.parts) > 0:\n",
      "                        choice = CompletionChoice(\n",
      "                            index=index,\n",
      "                            message=MessageDataResponse(\n",
      "                                role=\"assistant\",\n",
      "                                content=row.content.parts[0].text\n",
      "                            ),\n",
      "                            finish_reason=row.finish_reason.name\n",
      "                        )\n",
      "                        choices.append(choice)\n",
      "                        index += 1\n",
      "                usage = {\n",
      "                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\n",
      "                    \"completion_token\": chunk.usage_metadata.candidates_token_count,\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "\"total_token\": chunk.usage_metadata.total_token_count\n",
      "                }\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "response = ChatCompletionResponse(\n",
      "                    uuid=uuid,\n",
      "                    choices=choices,\n",
      "                    usage=usage\n",
      "\n",
      "                )\n",
      "                yield response\n",
      "\n",
      "        response = (row async for row in post_process_stream(uuid, vertexai_response))\n",
      "\n",
      "        return response\n",
      "\n",
      "    def _get_system_message(self, messages):\n",
      "        first_message = messages[0]\n",
      "        if first_message.role == \"system\":\n",
      "            return first_message.content\n",
      "        else:\n",
      "            return None\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "def _get_user_content(self, messages):\n",
      "        # currently only use last user message as user content\n",
      "        last_message = messages[-1]\n",
      "        text = None\n",
      "        images = []\n",
      "        if last_message.role == \"user\":\n",
      "            content = last_message.content\n",
      "            if isinstance(content, list):\n",
      "                for row in content:\n",
      "                    if row.type == \"text\":\n",
      "                        text = row.text\n",
      "                    elif row.type == \"image_url\":\n",
      "                        image_url = row.image_url.get(\"url\")\n",
      "                        images.append(image_url)\n",
      "            else:\n",
      "                text = content\n",
      "\n",
      "        if len(images) == 0:\n",
      "            images = None\n",
      "\n",
      "        return UserContent(text=text, images=images)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "def _construct_generation_config(self, config: ChatCompletionSetting, response_format):\n",
      "        if response_format is not None:\n",
      "            if response_format.type == \"json_object\":\n",
      "                mime_type = \"application/json\"\n",
      "            else:\n",
      "                mime_type = \"text/plain\"\n",
      "        else:\n",
      "            mime_type = \"text/plain\"\n",
      "\n",
      "        generation_config = GenerationConfig(\n",
      "            temperature=config.temperature,\n",
      "            max_output_tokens=config.max_token,\n",
      "            response_mime_type=mime_type\n",
      "        )\n",
      "\n",
      "        return generation_config\n",
      "\n",
      "    def _parse_choices(self, response_candidates):\n",
      "        index = 0\n",
      "        choices = []\n",
      "        for row in response_candidates:\n",
      "            if len(row.content.parts)>0:\n",
      "                messages = MessageDataResponse(\n",
      "                    role=\"assistant\",\n",
      "                    content=row.content.parts[0].text\n",
      "                )\n",
      "            else:\n",
      "                messages = MessageDataResponse()\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "choice = CompletionChoice(\n",
      "                index=index,\n",
      "                message=messages,\n",
      "                finish_reason=row.finish_reason.name\n",
      "            )\n",
      "            choices.append(choice)\n",
      "            index += 1\n",
      "\n",
      "            return choices\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "from llm.core.service.dto import ChatCompletionRequest\n",
      "from importlib import import_module\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError\n",
      "from llm.core.entity import CHAT_COMPLETION_MODELS\n",
      "import copy\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "class ChatCompletionService:\n",
      "    def __init__(self, active_models, event_tracker):\n",
      "        self.active_models = active_models\n",
      "        self.event_tracker = event_tracker\n",
      "        self.__models = self.__load_model()\n",
      "\n",
      "    def __load_model(self):\n",
      "        models = {}\n",
      "        for platform in self.active_models:\n",
      "            Model = getattr(\n",
      "                import_module(\"llm.core.platform.{}.model\".format(platform)),\n",
      "                \"Model\",\n",
      "            )\n",
      "            models[platform] = Model()\n",
      "        return models\n",
      "\n",
      "    def __validate_platform_and_model(self, platform: str, model: str):\n",
      "        platform_model = self.__models.get(platform)\n",
      "        if not platform_model:\n",
      "            raise PlatformModelNotFoundError(\"platform\", platform)\n",
      "\n",
      "        if platform_model.require_model_validation:\n",
      "            if model not in CHAT_COMPLETION_MODELS.get(platform, []):\n",
      "                raise PlatformModelNotFoundError(\"model\", model)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "async def generate_chat_completion(self, request: ChatCompletionRequest):\n",
      "        model_id = request.model\n",
      "        message_data = request.messages\n",
      "        config = request.config\n",
      "        response_format = request.response_format\n",
      "        tools = request.tools\n",
      "\n",
      "        self.__validate_platform_and_model(request.platform, model_id)\n",
      "\n",
      "        if not request.stream:\n",
      "            completion_response = await self.__models[request.platform].generate_chat_completion(\n",
      "                model=model_id,\n",
      "                messages=message_data,\n",
      "                config=config,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "            await self.event_tracker.log_llm_chat_completion(request, completion_response)\n",
      "\n",
      "            return completion_response\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "else:\n",
      "            completion_response = await self.__models[request.platform].generate_stream_chat_completion(\n",
      "                model=model_id,\n",
      "                messages=message_data,\n",
      "                config=config,\n",
      "                response_format=response_format,\n",
      "                tools=tools\n",
      "            )\n",
      "\n",
      "            return (row async for row in self.__send_tracker_stream(request, completion_response))\n",
      "\n",
      "    async def __send_tracker_stream(self, request: ChatCompletionRequest, response):\n",
      "        full_completion_content = \"\"\n",
      "        full_completion_role = None\n",
      "        full_response = None\n",
      "        full_usage = None\n",
      "        async for row in response:\n",
      "            yield row\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\chat_completion.py\n",
      "\n",
      "if len(row.choices) > 0:\n",
      "                if full_response is None:\n",
      "                    full_response = copy.deepcopy(row)\n",
      "                if row.choices[0].message.content is not None:\n",
      "                    full_completion_content += row.choices[0].message.content\n",
      "                if row.choices[0].message.role is not None:\n",
      "                    full_completion_role = row.choices[0].message.role\n",
      "            if row.usage.prompt_token > 0:\n",
      "                full_usage = row.usage\n",
      "\n",
      "        full_response.choices[0].message.content = full_completion_content\n",
      "        full_response.choices[0].message.role = full_completion_role\n",
      "        full_response.usage = full_usage\n",
      "\n",
      "        await self.event_tracker.log_llm_chat_completion(request, full_response)\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "from pydantic import BaseModel\n",
      "from typing import Optional, List, Dict, Union, Any\n",
      "\n",
      "\n",
      "class ContentData(BaseModel):\n",
      "    type: str\n",
      "    text: Optional[str] = None\n",
      "    image_url: Optional[Dict[str, str]] = None\n",
      "\n",
      "\n",
      "class FunctionParameterData(BaseModel):\n",
      "    type: str\n",
      "    properties: Dict[Any, Any]\n",
      "    required: List[str]\n",
      "\n",
      "\n",
      "class FunctionData(BaseModel):\n",
      "    name: str\n",
      "    description: str\n",
      "    parameters: FunctionParameterData\n",
      "\n",
      "\n",
      "class ToolsDataRequest(BaseModel):\n",
      "    type: str\n",
      "    function: Optional[FunctionData]\n",
      "\n",
      "\n",
      "class MessageDataRequest(BaseModel):\n",
      "    role: str\n",
      "    content: Union[str, List[ContentData]]\n",
      "\n",
      "\n",
      "class ChatCompletionSetting(BaseModel):\n",
      "    max_token: Optional[int] = 256\n",
      "    temperature: Optional[float] = 0.7\n",
      "    top_p: Optional[float] = 1\n",
      "    top_k: Optional[float] = 1\n",
      "\n",
      "\n",
      "class ResponseFormatRequest(BaseModel):\n",
      "    type: Optional[str] = None\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "class ChatCompletionRequest(BaseModel):\n",
      "    platform: str\n",
      "    model: str\n",
      "    messages: List[MessageDataRequest]\n",
      "    tools: Optional[List[ToolsDataRequest]] = None\n",
      "    config: Optional[ChatCompletionSetting] = {}\n",
      "    stream: bool = False\n",
      "    task_name: Optional[str] = None\n",
      "    client_name: str\n",
      "    trace_id: str\n",
      "    session_id: Optional[str] = \"\"\n",
      "    response_format: Optional[ResponseFormatRequest] = None\n",
      "\n",
      "\n",
      "class TokenUsage(BaseModel):\n",
      "    prompt_token: Optional[int]\n",
      "    completion_token: Optional[int]\n",
      "    total_token: Optional[int]\n",
      "\n",
      "\n",
      "class FunctionResponse(BaseModel):\n",
      "    arguments: str\n",
      "    name: str\n",
      "\n",
      "\n",
      "class ToolCallResponse(BaseModel):\n",
      "    function: Optional[FunctionResponse] = None\n",
      "    type: str\n",
      "\n",
      "\n",
      "class MessageDataResponse(BaseModel):\n",
      "    role: Optional[str] = None\n",
      "    content: Optional[str] = None\n",
      "    tool_calls: Optional[List[ToolCallResponse]] = None\n",
      "\n",
      "\n",
      "class CompletionChoice(BaseModel):\n",
      "    index: int\n",
      "    message: MessageDataResponse\n",
      "    finish_reason: Optional[str] = None\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\dto.py\n",
      "\n",
      "class ChatCompletionResponse(BaseModel):\n",
      "    uuid: str\n",
      "    choices: Optional[List[CompletionChoice]] = []\n",
      "    usage: Optional[TokenUsage] = {}\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\exceptions.py\n",
      "\n",
      "class PlatformModelNotFoundError(Exception):\n",
      "    def __init__(self, field, value):\n",
      "        self.field = field\n",
      "        self.value = value\n",
      "\n",
      "\n",
      "class AIException(Exception):\n",
      "    def __init__(self, name, message, errors):\n",
      "        self.name = name\n",
      "        self.message = message\n",
      "        self.errors = errors\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\service\\port.py\n",
      "\n",
      "import abc\n",
      "\n",
      "\n",
      "class AbstractChatCompletionModel(abc.ABC):\n",
      "    def __init__(self) -> None:\n",
      "        pass\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def generate_chat_completion(self, model, messages, config, response_format, tools):\n",
      "        raise NotImplementedError\n",
      "\n",
      "    @abc.abstractmethod\n",
      "    def generate_stream_chat_completion(self, model, messages, config, response_format, tools):\n",
      "        raise NotImplementedError\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\conftest.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from pytest import fixture\n",
      "\n",
      "\n",
      "@pytest.fixture(scope=\"package\")\n",
      "def client():\n",
      "    from llm.api import app\n",
      "    with TestClient(app) as client:\n",
      "        yield client\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "from pprint import pprint\n",
      "import requests\n",
      "import json\n",
      "\n",
      "\n",
      "def test_chat_completion(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"dummy\",\n",
      "        \"model\": \"gpt-4-turbo\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_openai(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"openai\",\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_openai_with_tools(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"What's the weather like in San franciso, tokyo and paris\"}\n",
      "        ],\n",
      "        \"tools\": [{\n",
      "            \"type\": \"function\",\n",
      "            \"function\": {\n",
      "                \"name\": \"get_current_weather\",\n",
      "                \"description\": \"Get the current weather in a given location\",\n",
      "                \"parameters\": {\n",
      "                    \"type\": \"object\",\n",
      "                    \"properties\": {\n",
      "                        \"location\": {\n",
      "                            \"type\": \"string\",\n",
      "                            \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
      "                            },\n",
      "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
      "                    },\n",
      "                    \"required\": [\"location\"]\n",
      "                }\n",
      "            }\n",
      "        }],\n",
      "        \"platform\": \"openai\",\n",
      "        \"model\": \"gpt-3.5-turbo\",\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "\"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "    assert len(response_data[\"choices\"][0][\"message\"][\"tool_calls\"]) > 0\n",
      "\n",
      "Source : test_repo\\rg-llm\\tests\\integration_test\\test_chat_completion_api.py\n",
      "\n",
      "def test_chat_completion_vertexai(client):\n",
      "    data = {\n",
      "        \"trace_id\": \"abc\",\n",
      "        \"messages\": [\n",
      "            {\"role\": \"user\", \"content\": \"halo kak\"}\n",
      "        ],\n",
      "        \"platform\": \"vertexai\",\n",
      "        \"model\": \"gemini-1.5-flash-001\",\n",
      "        \"config\": {\n",
      "            \"max_token\": 1000,\n",
      "            \"temperature\": 0.5,\n",
      "            \"top_p\": 1,\n",
      "            \"top_k\": 1\n",
      "        },\n",
      "        \"client_name\": \"salesbot\",\n",
      "        \"task_name\": \"saya\",\n",
      "        \"stream\": False,\n",
      "        \"session_id\": \"session_abc\"\n",
      "    }\n",
      "\n",
      "    response = client.post(\"chat/completion\", json=data)\n",
      "    response_data = response.json().get(\"data\", {})\n",
      "\n",
      "    assert response.status_code == 200\n",
      "    assert response_data[\"choices\"][0][\"message\"][\"content\"] != \"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = \"\"\n",
    "\n",
    "for doc in texts:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata[\"source\"]\n",
    "    output += \"Source : \" + metadata + \"\\n\\n\" + page_content + \"\\n\\n\"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorDB\n",
    "embeddings=OpenAIEmbeddings(disallowed_special=())\n",
    "# vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"detect bug or optimize for the code\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import threading\\nimport sys\\nimport traceback\\nfrom logging.config import dictConfig\\nfrom config import GUNICORN_CONFIG\\n\\n\\nworkers = GUNICORN_CONFIG.NUM_WORKERS\\nthreads = GUNICORN_CONFIG.NUM_THREADS\\ntimeout = GUNICORN_CONFIG.TIMEOUT\\nworker_class = \"uvicorn.workers.UvicornWorker\"\\nbind = \\'0.0.0.0:80\\'', metadata={'language': 'python', 'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py'}),\n",
       " Document(page_content='def test_chat_completion_openai_with_tools(client):\\n    data = {\\n        \"trace_id\": \"abc\",\\n        \"messages\": [\\n            {\"role\": \"user\", \"content\": \"What\\'s the weather like in San franciso, tokyo and paris\"}\\n        ],\\n        \"tools\": [{\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": \"get_current_weather\",\\n                \"description\": \"Get the current weather in a given location\",\\n                \"parameters\": {\\n                    \"type\": \"object\",\\n                    \"properties\": {\\n                        \"location\": {\\n                            \"type\": \"string\",\\n                            \"description\": \"The city and state, e.g. San Francisco, CA\"\\n                            },\\n                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\\n                    },\\n                    \"required\": [\"location\"]\\n                }\\n            }\\n        }],\\n        \"platform\": \"openai\",\\n        \"model\": \"gpt-3.5-turbo\",', metadata={'language': 'python', 'source': 'test_repo\\\\rg-llm\\\\tests\\\\integration_test\\\\test_chat_completion_api.py'}),\n",
       " Document(page_content='REGION_VARIANT = [\\n    \"us-central1\",\\n    \"us-east4\",\\n    \"us-west1\",\\n    \"us-west4\",\\n    \"northamerica-northeast1\",\\n    \"europe-west9\",\\n    \"europe-west2\",\\n    \"europe-west3\",\\n    \"europe-west4\",\\n    \"europe-west9\",\\n    \"asia-northeast1\",\\n    \"asia-northeast3\",\\n    \"asia-southeast1\"\\n]', metadata={'language': 'python', 'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\entity.py'}),\n",
       " Document(page_content='import logging\\n\\n\\nclass RoguLogFilter(logging.Filter):\\n    \"\"\"\\n    Custom filter to exclude health check\\n    \"\"\"\\n\\n    def filter(self, record):\\n        try:\\n            uri = str(record.args[2])\\n        except Exception as e:\\n            uri = None\\n        return uri != \"/_health\"', metadata={'language': 'python', 'source': 'test_repo\\\\rg-llm\\\\custom_logging.py'}),\n",
       " Document(page_content='\"total_token\": chunk.usage_metadata.total_token_count\\n                }', metadata={'language': 'python', 'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\vertexai\\\\model.py'})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'}\\n\\nreturn variant_org_key\\n\\ncontents.extend(images)\\n\\nself.__models = self.__load_model()\\n\\ncode.append(\\'File: \"%s\", line %d, in %s\\' % (filename, lineno, name))\\n            if line:\\n                code.append(\"  %s\" % (line.strip()))\\n    worker.log.debug(\"\\\\n\".join(code))'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source : test_repo\\rg-llm\\llm\\api\\__init__.py\n",
      "\n",
      "from llm.api.factory import init_app\n",
      "\n",
      "\n",
      "app = init_app()\n",
      "\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\openai\\entity.py\n",
      "\n",
      "from llm.config import OPENAI_CONFIG\n",
      "\n",
      "\n",
      "ORG_KEYS = {\n",
      "    \"ruangguru\": OPENAI_CONFIG.ORG_KEY_1,\n",
      "    \"schoters\": OPENAI_CONFIG.ORG_KEY_2,\n",
      "    \"skillacademy\": OPENAI_CONFIG.ORG_KEY_3,\n",
      "    \"brainacademy\": OPENAI_CONFIG.ORG_KEY_4\n",
      "}\n",
      "\n",
      "MODEL_VARIANT = {\n",
      "    \"gpt-3.5-turbo\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-16k\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-1106\": \"gpt-3-5\",\n",
      "    \"gpt-3.5-turbo-0125\": \"gpt-3-5\",\n",
      "    \"gpt-4\": \"gpt-4\",\n",
      "    \"gpt-4-turbo\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-0125-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4-1106-preview\": \"gpt-4-turbo\",\n",
      "    \"gpt-4o\": \"gpt-4o\"\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\client\\common\\async_http.py\n",
      "\n",
      "_async_http_client = AsyncHttp()\n",
      "async_http_open = _async_http_client.open\n",
      "async_http_close = _async_http_client.close\n",
      "async_get = _async_http_client.get\n",
      "async_get_params = _async_http_client.get_params\n",
      "async_post = _async_http_client.post\n",
      "async_post_forget = _async_http_client.post_forget\n",
      "\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\core\\platform\\vertexai\\model.py\n",
      "\n",
      "async def post_process_stream(uuid, vertexai_response):\n",
      "            async for chunk in vertexai_response:\n",
      "                if chunk is None:\n",
      "                    continue\n",
      "                choices = []\n",
      "                index = 0\n",
      "                for row in chunk.candidates:\n",
      "                    if len(row.content.parts) > 0:\n",
      "                        choice = CompletionChoice(\n",
      "                            index=index,\n",
      "                            message=MessageDataResponse(\n",
      "                                role=\"assistant\",\n",
      "                                content=row.content.parts[0].text\n",
      "                            ),\n",
      "                            finish_reason=row.finish_reason.name\n",
      "                        )\n",
      "                        choices.append(choice)\n",
      "                        index += 1\n",
      "                usage = {\n",
      "                    \"prompt_token\": chunk.usage_metadata.prompt_token_count,\n",
      "                    \"completion_token\": chunk.usage_metadata.candidates_token_count,\n",
      "\n",
      "\n",
      "Source : test_repo\\rg-llm\\llm\\api\\views.py\n",
      "\n",
      "return JSONResponse(\n",
      "        dict(\n",
      "            status=status,\n",
      "            error=_camel_to_snake(name),\n",
      "            code=code,\n",
      "            error_code=error_message,\n",
      "            metadata={}\n",
      "        ),\n",
      "        status_code=code,\n",
      "    )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata[\"source\"]\n",
    "    print(\"Source : \" + metadata + \"\\n\\n\" + page_content +\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='}', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\entity.py'}),\n",
       " Document(page_content='return variant_org_key', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\platform\\\\openai\\\\allocation.py'}),\n",
       " Document(page_content='contents.extend(images)', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\client\\\\vertexai\\\\client.py'}),\n",
       " Document(page_content='self.__models = self.__load_model()', metadata={'source': 'test_repo\\\\rg-llm\\\\llm\\\\core\\\\service\\\\chat_completion.py'}),\n",
       " Document(page_content='code.append(\\'File: \"%s\", line %d, in %s\\' % (filename, lineno, name))\\n            if line:\\n                code.append(\"  %s\" % (line.strip()))\\n    worker.log.debug(\"\\\\n\".join(code))', metadata={'source': 'test_repo\\\\rg-llm\\\\gunicorn.config.py'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy  \n",
      "from random import randint, choice  \n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0: \n",
      "        return \"Invalid radius\"\n",
      "    area = math.pi * (radius ** 2\n",
      "    return area \n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return \"Invalid triangle sides\"\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimetr  \n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radious  \n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = r\"test_file\\test_file2.py\"\n",
    "\n",
    "with open(file_path, 'r') as rp:\n",
    "    code = rp.read()\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of Python files saved to python_files_content.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"test_repo\"\n",
    "output_file = \"python_files_content.txt\"\n",
    "\n",
    "# Initialize an empty list to store Python files\n",
    "python_files = []\n",
    "\n",
    "# Walk through all directories and subdirectories\n",
    "for root, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            python_files.append(file_path)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as out_file:\n",
    "    # Read and write the content of each Python file found\n",
    "    for file_path in python_files:\n",
    "        with open(file_path, 'r') as rp:\n",
    "            content = rp.read()\n",
    "            out_file.write(f\"Content of {file_path}:\\n{content}\\n\\n\")\n",
    "\n",
    "print(f\"Content of Python files saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of Python files saved to python_files_content.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"test_repo\"\n",
    "output_file = \"python_files_content.md\"\n",
    "\n",
    "# Initialize an empty list to store Python files\n",
    "python_files = []\n",
    "\n",
    "# Walk through all directories and subdirectories\n",
    "for root, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith('.py'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            python_files.append(file_path)\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w') as out_file:\n",
    "    # Write Markdown headers and content for each Python file found\n",
    "    for file_path in python_files:\n",
    "        with open(file_path, 'r') as rp:\n",
    "            content = rp.read()\n",
    "            out_file.write(f\"## Content of `{file_path}`:\\n\\n\")\n",
    "            out_file.write(\"```python\\n\")\n",
    "            out_file.write(content)\n",
    "            out_file.write(\"\\n```\\n\\n\")\n",
    "\n",
    "print(f\"Content of Python files saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Access variables from the environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class Localizer(BaseModel):\n",
    "\n",
    "    localizer: str = Field(\n",
    "        description=\"Explain the predicted the bug\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_localizer = llm.with_structured_output(Localizer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"**Role**: As a localizer, your task is to carefully identify any bug or issues in the provided code. If no issues are found, simply state that the code is bug-free. If there are issues, explain them clearly.\n",
    "**Requirement**\n",
    "{requirement}\n",
    "**predicted bug and its sources** :\n",
    "{code}\n",
    "\n",
    "Predicted Bug and Sources:\n",
    "Provide a detailed explanation of the bug, including why the code is incorrect or where the issue lies.\n",
    "\"\"\"\n",
    "\n",
    "localizer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Explain the bug from predicted code: \\n\\n {code} \\n\\n The requirement: {requirement}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "localizer_agent = localizer_prompt | structured_llm_localizer\n",
    "requirement = \"detect bug or optimize for the code\"\n",
    "docs = retriever.invoke(requirement)\n",
    "\n",
    "predicted_code = \"\"\n",
    "\n",
    "for doc in docs:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata[\"source\"]\n",
    "    predicted_code += \"Source : \" + metadata + \"\\n\\n\" + page_content + \"\\n\\n\"\n",
    "    \n",
    "# doc_txt = docs[1].requirement\n",
    "localizer = localizer_agent.invoke({\"code\": predicted_code, \"requirement\": requirement})\n",
    "# print(localizer_agent.invoke({\"code\": predicted_code, \"requirement\": requirement}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localizer='The bug in the provided code is in the gunicorn.config.py file. The issue lies in the import statement for GUNICORN_CONFIG. The code imports GUNICORN_CONFIG from a file named config, but the actual import should be from GUNICORN_CONFIG. This discrepancy in the import statement can lead to a ModuleNotFoundError as the correct path is not specified.'\n"
     ]
    }
   ],
   "source": [
    "print(localizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The bug in the provided code is in the gunicorn.config.py file. The issue '\n",
      " 'lies in the import statement for GUNICORN_CONFIG. The code imports '\n",
      " 'GUNICORN_CONFIG from a file named config, but the actual import should be '\n",
      " 'from GUNICORN_CONFIG. This discrepancy in the import statement can lead to a '\n",
      " 'ModuleNotFoundError as the correct path is not specified.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "pprint(localizer.localizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger using RAG with create_structured_output_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\CodeDebugger\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>.Please follow our extraction use case documentation for more guidelines on how to do information extraction with LLMs. <https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here: <https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Localizer(localizer='The code provided is bug-free.')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model\n",
    "class Localizer(BaseModel):\n",
    "\n",
    "    localizer: str = Field(\n",
    "        description=\"Explain the predicted the bug\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "localization_gen_prompt = ChatPromptTemplate.from_template(\"\"\"**Role**: As a localizer, your task is to carefully identify any bug or issues in the provided code. If no issues are found, simply state that the code is bug-free. If there are issues, explain them clearly.\n",
    "**Requirement**\n",
    "{requirement}\n",
    "**predicted bug and its sources** :\n",
    "{code}\n",
    "\n",
    "Provide a detailed explanation of the bug, including why the code is incorrect or where the issue lies.\"\"\")\n",
    "\n",
    "\n",
    "localizer_agent = create_structured_output_runnable(\n",
    "    Localizer, llm, localization_gen_prompt\n",
    ")\n",
    "\n",
    "requirement = \"detect bug or optimize for the code\"\n",
    "docs = retriever.invoke(requirement)\n",
    "\n",
    "predicted_code = \"\"\n",
    "\n",
    "for doc in docs:\n",
    "    page_content = doc.page_content\n",
    "    metadata = doc.metadata[\"source\"]\n",
    "    predicted_code += \"Source : \" + metadata + \"\\n\\n\" + page_content + \"\\n\\n\"\n",
    "\n",
    "localizer = localizer_agent.invoke({'requirement':requirement,'code':predicted_code})\n",
    "localizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The code provided is bug-free.'\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(localizer.localizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger using create_structure_output_runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\CodeDebugger\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>.Please follow our extraction use case documentation for more guidelines on how to do information extraction with LLMs. <https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here: <https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "class Localizer(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    Description: str = Field(\n",
    "        description=\"The explanation of the bug\"\n",
    "    )\n",
    "\n",
    "\n",
    "localization_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    '''**Role**: As a localizer, your task is to carefully identify any bug or issues in the provided code. If no issues are found, simply state that the code is bug-free. If there are issues, explain them clearly.\n",
    "**Requirement**\n",
    "{requirement}\n",
    "**Code**\n",
    "{code}\n",
    "'''\n",
    ")\n",
    "localizer_agent = create_structured_output_runnable(\n",
    "    Localizer, llm, localization_gen_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Localizer(Description='The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localizer = localizer_agent.invoke({'requirement':'Be careful to analyze the code','code':code})\n",
    "localizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5\")  # Change the model name as per your use case\n",
    "response_tokens = enc.encode(localizer.Description)\n",
    "\n",
    "# Count the tokens\n",
    "token_count = len(response_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 430\n",
      "\tPrompt Tokens: 304\n",
      "\tCompletion Tokens: 126\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00034100000000000005\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    localizer1 = localizer_agent.invoke({'requirement':'Be careful to analyze the code','code':code})\n",
    "\n",
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cb.total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.completion_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00034100000000000005"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb.total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description='The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.'\n"
     ]
    }
   ],
   "source": [
    "print(localizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localizer.Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The code provided has the following issues:\\n'\n",
      " '1. In the `calculate_area` function, there is a missing closing parenthesis '\n",
      " 'in the calculation of the area.\\n'\n",
      " '2. In the `calculate_perimeter` function, there is a typo in the return '\n",
      " 'statement where `perimeter` is misspelled as `perimetr`.\\n'\n",
      " '3. In the `Circle` class, there are typos in the `__init__` method where '\n",
      " '`radius` is misspelled as `radious` and in the `area` method where '\n",
      " '`calculate_area` is not correctly called.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(localizer.Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger using with_structured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Localizer(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    Description: str = Field(\n",
    "        description=\"The explanation of the bug\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(Localizer)\n",
    "\n",
    "system = '''**Role**: As a localizer, your task is to identify any issues in the provided code. If no issues are found, simply state that the code is bug-free. If there are issues, explain them clearly.\n",
    "\n",
    "**requirement**\n",
    "{requirement}\n",
    "**Code**\n",
    "{code}\n",
    "'''\n",
    "\n",
    "test_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Explain the bug\"), ## Tergantung prompt, jika prompt (\"As debugger, identify the code\" --> Gagal)\n",
    "    ]\n",
    ")\n",
    "retrieval_debug_agent = test_gen_prompt | structured_llm_grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Localizer(Description=\"The code has the following issues:\\n1. In the calculate_area function, there is a syntax error due to a missing closing parenthesis in the calculation of the area.\\n2. In the calculate_perimeter function, there is a typo in the return statement where 'perimetr' is misspelled instead of 'perimeter'.\\n3. In the Circle class, there are typos in the __init__ method where 'radius' is misspelled as 'radious' and in the area method where the calculate_area function is called with the incorrect parameter name.\\n4. The import statement for numpy is present but not used in the code, which is unnecessary.\")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_debug = retrieval_debug_agent.invoke({'requirement':'Be careful to analyze the code','code':code})\n",
    "retrieval_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The code has the following issues:\\n'\n",
      " '1. In the calculate_area function, there is a syntax error due to a missing '\n",
      " 'closing parenthesis in the calculation of the area.\\n'\n",
      " '2. In the calculate_perimeter function, there is a typo in the return '\n",
      " \"statement where 'perimetr' is misspelled instead of 'perimeter'.\\n\"\n",
      " '3. In the Circle class, there are typos in the __init__ method where '\n",
      " \"'radius' is misspelled as 'radious' and in the area method where the \"\n",
      " 'calculate_area function is called with the incorrect parameter name.\\n'\n",
      " '4. The import statement for numpy is present but not used in the code, which '\n",
      " 'is unnecessary.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(retrieval_debug.Description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Tester and Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutableCode(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    code: str = Field(\n",
    "        description=\"Detailed optmized error-free Python code with test cases assertion\"\n",
    "    )\n",
    "\n",
    "python_execution_gen = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You make a testing layer for the *Python Code* that can help to execute the code. You need to pass only Input as argument and validate if the Given Output is matched.\n",
    "*Instruction*:\n",
    "- Make sure to return the error if the assertion fails\n",
    "- Generate the code that can be execute\n",
    "Python Code to excecute:\n",
    "*Python Code*:{code}\n",
    "\n",
    "Input and Output For Code:\n",
    "*Input*: generate input to test\n",
    "*Expected Output*:generate output expected from input\n",
    "\n",
    "Explain the hyphotesis of error description of each test case.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "execution_agent = create_structured_output_runnable(\n",
    "    ExecutableCode, llm, python_execution_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_execute = execution_agent.invoke({\"code\":code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "# Test Cases\n",
      "\n",
      "def test_calculate_area():\n",
      "    # Test case 1: Negative radius\n",
      "    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid radius\n",
      "    assert calculate_area(5) == math.pi * 25, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_calculate_perimeter():\n",
      "    # Test case 1: Invalid triangle sides\n",
      "    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid triangle sides\n",
      "    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_circle():\n",
      "    # Test case 1: Circle area\n",
      "    circle = Circle(5)\n",
      "    assert circle.area() == math.pi * 25, 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Circle perimeter\n",
      "    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\n",
      "\n",
      "# Run the test cases\n",
      "try:\n",
      "    test_calculate_area()\n",
      "    test_calculate_perimeter()\n",
      "    test_circle()\n",
      "    print('All test cases passed!')\n",
      "except AssertionError as e:\n",
      "    print(f'Test case failed: {e}')\n"
     ]
    }
   ],
   "source": [
    "print(code_execute.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_execute = code + \"\\n\" + code_execute.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy  \n",
      "from random import randint, choice  \n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0: \n",
      "        return \"Invalid radius\"\n",
      "    area = math.pi * (radius ** 2\n",
      "    return area \n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return \"Invalid triangle sides\"\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimetr  \n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radious  \n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "# Test Cases\n",
      "\n",
      "def test_calculate_area():\n",
      "    # Test case 1: Negative radius\n",
      "    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid radius\n",
      "    assert calculate_area(5) == math.pi * 25, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_calculate_perimeter():\n",
      "    # Test case 1: Invalid triangle sides\n",
      "    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid triangle sides\n",
      "    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_circle():\n",
      "    # Test case 1: Circle area\n",
      "    circle = Circle(5)\n",
      "    assert circle.area() == math.pi * 25, 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Circle perimeter\n",
      "    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\n",
      "\n",
      "# Run the test cases\n",
      "try:\n",
      "    test_calculate_area()\n",
      "    test_calculate_perimeter()\n",
      "    test_circle()\n",
      "    print('All test cases passed!')\n",
      "except AssertionError as e:\n",
      "    print(f'Test case failed: {e}')\n"
     ]
    }
   ],
   "source": [
    "print(code + \"\\n\" +code_execute.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Eksekusi Kode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Exception : '(' was never closed (<string>, line 8)\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = None\n",
    "try:\n",
    "    exec(code_to_execute)\n",
    "except Exception as e:\n",
    "    error = f'Exception : {e}'\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugCode(BaseModel):\n",
    "\n",
    "    code: str = Field(\n",
    "        description=\"Optimized and Refined Python code to resolve the error\"\n",
    "    )\n",
    "    \n",
    "\n",
    "python_refine_gen = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are expert in Python Debugging. You have to analysis Given Code and Bug and generate code that handles the Bug\n",
    "    *Instructions*:\n",
    "    - Make sure to generate error free code\n",
    "    - Generated code is able to handle the bug\n",
    "    \n",
    "    *Code*: {code}\n",
    "    *Bug analyzed*:  {bug}\n",
    "    *Error detected*: {error}\n",
    "    \"\"\"\n",
    ")\n",
    "debug_code_agent = create_structured_output_runnable(\n",
    "    DebugCode, llm, python_refine_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_json = {\n",
    "    \"code\" : code,\n",
    "    \"bug\" : localizer.Description,\n",
    "    \"error\": error   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebugCode(code=\"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_code = debug_code_agent.invoke(dummy_json)\n",
    "debug_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n"
     ]
    }
   ],
   "source": [
    "print(debug_code.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n"
     ]
    }
   ],
   "source": [
    "print(debug_code.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_execute = debug_code.code + \"\\n\" + code_execute.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test case passed\n",
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "error = None\n",
    "try:\n",
    "    print(\"All test case passed\")\n",
    "    exec(code_to_execute)\n",
    "except Exception as e:\n",
    "    error = f'Exception : {e}'\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = debug_code.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefactorCode(BaseModel):\n",
    "\n",
    "    code: str = Field(\n",
    "        description=\"Refactored Python code\"\n",
    "    )\n",
    "\n",
    "# python_refactored_code = ChatPromptTemplate.from_template(\"\"\"You are an experienced developer tasked with programming for coding interview problems. Your task is to write correct, well-functioned {lang} code based on given specifications, also adhering to the required input/output format\"\"\"\n",
    "# )\n",
    "\n",
    "\n",
    "python_refactored_code = ChatPromptTemplate.from_template(\"\"\"Your task as an expert developer,  you are given the code that needs to be refactored and optimized for better performance, readability, and maintainability. Below are the details and the code snippet. Please refactor the code, provide comments where necessary/\n",
    "Details:\n",
    "\n",
    "- The code should follow {lang} best practices.\n",
    "- Aim for improved performance without sacrificing readability.\n",
    "- Add comments to explain complex sections of the code.\n",
    "- Optimize any redundant or inefficient parts of the code.\n",
    "- Follow naming conventions for variables and functions.\n",
    "\n",
    "Code Snippet:\n",
    "{code}\n",
    "\n",
    "Please provide the refactored and optimized version of the code along with any comment of explanations or suggestions for improvements.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "refactor_code_agent = create_structured_output_runnable(\n",
    "    RefactorCode, llm, python_refactored_code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "refactor_code = refactor_code_agent.invoke({\"lang\":\"python\", \"code\":code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "\n",
      "def calculate_area(radius):\n",
      "    '''\n",
      "    Calculate the area of a circle given the radius.\n",
      "    Args:\n",
      "        radius (float): The radius of the circle.\n",
      "    Returns:\n",
      "        float: The area of the circle.\n",
      "    '''\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    '''\n",
      "    Calculate the perimeter of a triangle given its sides.\n",
      "    Args:\n",
      "        side1 (float): Length of side 1.\n",
      "        side2 (float): Length of side 2.\n",
      "        side3 (float): Length of side 3.\n",
      "    Returns:\n",
      "        float: The perimeter of the triangle.\n",
      "    '''\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n"
     ]
    }
   ],
   "source": [
    "print(refactor_code.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_execute = refactor_code.code + \"\\n\" + code_execute.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = None\n",
    "try:\n",
    "    exec(code_to_execute)\n",
    "except Exception as e:\n",
    "    error = f'Exception : {e}'\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCoder(TypedDict):\n",
    "    requirement: str\n",
    "    code: str \n",
    "    testing_code: str\n",
    "    refactored_code:str\n",
    "    bug : Optional[str]\n",
    "    errors: Optional[str]\n",
    "    final_result: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_code(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Upload Code------')\n",
    "    code = state['code']\n",
    "    print(f\"Code : \\n{code}\")\n",
    "    return{'code':code }\n",
    "\n",
    "def localizer(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Localizer------')\n",
    "    requirement = state['requirement']\n",
    "    code = state['code']\n",
    "    localizer = localizer_agent.invoke({'code':code,'requirement':requirement})\n",
    "    print(f\"Bug analyzed : {localizer.Description}\")\n",
    "    return {'bug':localizer.Description}\n",
    "\n",
    "def executer(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Executer------')\n",
    "    code = state['code']\n",
    "    code_execute = execution_agent.invoke({\"code\":code})\n",
    "    code_to_execute = code + \"\\n\" + code_execute.code\n",
    "    print(f\"Testing code : \\n{code_execute.code}\")\n",
    "    bug=state['bug']\n",
    "    error = None\n",
    "    try:\n",
    "        exec(code_to_execute)\n",
    "        bug = None\n",
    "        print(\"Code Execution Successful\")\n",
    "    except Exception as e:\n",
    "        print('Found Error While Running')\n",
    "        error = f\"Execution Error : {e}\"\n",
    "        print(error)\n",
    "    return {'testing_code':code_execute.code,'errors':error, 'bug': bug}\n",
    "\n",
    "def debugger(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Debugger------')\n",
    "    errors = state['errors']\n",
    "    code = state['code']\n",
    "    execution_code = state['testing_code']\n",
    "    bug = state['bug']\n",
    "    debug_code = debug_code_agent.invoke({'code':code,'bug':bug,'error':errors, 'execution_code':execution_code})\n",
    "    print(\"The code after debug : \",debug_code.code )\n",
    "    return {'code':debug_code.code,'errors':None}\n",
    "\n",
    "def decide_to_developer(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Decide to Developer------')\n",
    "    if state['errors'] and state['bug']:\n",
    "        print(f'''There is error and bug. \n",
    "                The error : {state[\"errors\"]}\n",
    "                The bug : {state[\"bug\"]}'''\n",
    "            )\n",
    "        return 'debugger'\n",
    "    else:\n",
    "        print(\"There is no error, the code will be refactored\")\n",
    "        return 'developer'\n",
    "\n",
    "def developer(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Developer------')\n",
    "    code = state['code']\n",
    "    testing_code = state['testing_code']\n",
    "    refactored_code = refactor_code_agent.invoke({\"lang\":\"Python\", \"code\":code})\n",
    "    code_to_execute = refactored_code.code + \"\\n\" + testing_code\n",
    "    print(f\"The refactored code : \\n{refactored_code.code}\")\n",
    "    error = None\n",
    "    try:\n",
    "        exec(code_to_execute)\n",
    "        print(\"Developer Code Execution Successful\")\n",
    "    except Exception as e:\n",
    "        print('Found Error While Running')\n",
    "        error = f\"Execution Error : {e}\"\n",
    "    return {'refactored_code':refactored_code.code,'errors':error}\n",
    "    \n",
    "def decide_to_end(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(f'\\n------Entering in Decide to End------')\n",
    "    if state['errors']:\n",
    "        print(\"This code error when refactored, it will be return the debugger code\")\n",
    "        return 'the_end'\n",
    "    else:\n",
    "        print(\"----The code can be refactored----\")\n",
    "        return 'end'\n",
    "    \n",
    "def the_end(state):\n",
    "    print('\\n'+'----------'*20)\n",
    "    print(\"The code is failed to be refactored\")\n",
    "    code = state[\"code\"]\n",
    "    return {\"code\":code,\"final_result\":\"Can not use code refactor\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentCoder)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"upload_code\",upload_code)\n",
    "workflow.add_node(\"localizer\", localizer)  \n",
    "workflow.add_node(\"executer\", executer) \n",
    "workflow.add_node(\"debugger\", debugger) \n",
    "workflow.add_node(\"developer\",developer )\n",
    "workflow.add_node(\"the_end\",the_end)\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"upload_code\")\n",
    "workflow.add_edge(\"upload_code\", \"localizer\")\n",
    "workflow.add_edge(\"localizer\",\"executer\")\n",
    "workflow.add_edge(\"debugger\", \"executer\")\n",
    "# workflow.add_edge(\"developer\", \"decide_to_end\")\n",
    "workflow.add_edge(\"the_end\",END)\n",
    "#workflow.add_edge(\"executer\", \"decide_to_end\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    'executer',\n",
    "    decide_to_developer,\n",
    "    {\n",
    "        \"debugger\": \"debugger\",\n",
    "        \"developer\": \"developer\"\n",
    "        \n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"developer\",\n",
    "    decide_to_end,\n",
    "    {\n",
    "        \"end\" : END,\n",
    "        'the_end':'the_end'\n",
    "        \n",
    "    }\n",
    "    \n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "image_content = app.get_graph().draw_png()\n",
    "\n",
    "# Save the image to a file\n",
    "with open(\"graph_image.png\", \"wb\") as f:\n",
    "    f.write(image_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirement = \"Be careful to analyze the code\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "import numpy  \n",
      "from random import randint, choice  \n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0: \n",
      "        return \"Invalid radius\"\n",
      "    area = math.pi * (radius ** 2\n",
      "    return area \n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return \"Invalid triangle sides\"\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimetr  \n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radious  \n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "file_path = r\"test_file\\test_file2.py\"\n",
    "\n",
    "with open(file_path, 'r') as rp:\n",
    "    code = rp.read()\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Upload Code------\n",
      "Code : \n",
      "import math\n",
      "import numpy  \n",
      "from random import randint, choice  \n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0: \n",
      "        return \"Invalid radius\"\n",
      "    area = math.pi * (radius ** 2\n",
      "    return area \n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return \"Invalid triangle sides\"\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimetr  \n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radious  \n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "{'code': 'import math\\nimport numpy  \\nfrom random import randint, choice  \\n\\ndef calculate_area(radius):\\n    if radius < 0: \\n        return \"Invalid radius\"\\n    area = math.pi * (radius ** 2\\n    return area \\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return \"Invalid triangle sides\"\\n    perimeter = side1 + side2 + side3\\n    return perimetr  \\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radious  \\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n'}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Localizer------\n",
      "Bug analyzed : The code provided has the following issues:\n",
      "1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\n",
      "2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\n",
      "3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.\n",
      "4. The `numpy` library is imported but not used in the code.\n",
      "{'bug': 'The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.\\n4. The `numpy` library is imported but not used in the code.'}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Executer------\n",
      "Testing code : \n",
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "# Test Cases\n",
      "\n",
      "def test_calculate_area():\n",
      "    # Test case 1: Negative radius\n",
      "    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid radius\n",
      "    assert calculate_area(5) == math.pi * 25, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_calculate_perimeter():\n",
      "    # Test case 1: Invalid triangle sides\n",
      "    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid triangle sides\n",
      "    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_circle():\n",
      "    # Test case 1: Circle area\n",
      "    circle = Circle(5)\n",
      "    assert circle.area() == math.pi * 25, 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Circle perimeter\n",
      "    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\n",
      "\n",
      "# Run the test cases\n",
      "try:\n",
      "    test_calculate_area()\n",
      "    test_calculate_perimeter()\n",
      "    test_circle()\n",
      "    print('All test cases passed!')\n",
      "except AssertionError as e:\n",
      "    print(f'Test case failed: {e}')\n",
      "Found Error While Running\n",
      "Execution Error : '(' was never closed (<string>, line 8)\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Decide to Developer------\n",
      "There is error and bug. \n",
      "                The error : Execution Error : '(' was never closed (<string>, line 8)\n",
      "                The bug : The code provided has the following issues:\n",
      "1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\n",
      "2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\n",
      "3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.\n",
      "4. The `numpy` library is imported but not used in the code.\n",
      "{'testing_code': \"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n\\n# Test Cases\\n\\ndef test_calculate_area():\\n    # Test case 1: Negative radius\\n    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\\n    \\n    # Test case 2: Valid radius\\n    assert calculate_area(5) == math.pi * 25, 'Test case 2 failed'\\n\\n\\ndef test_calculate_perimeter():\\n    # Test case 1: Invalid triangle sides\\n    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\\n    \\n    # Test case 2: Valid triangle sides\\n    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\\n\\n\\ndef test_circle():\\n    # Test case 1: Circle area\\n    circle = Circle(5)\\n    assert circle.area() == math.pi * 25, 'Test case 1 failed'\\n    \\n    # Test case 2: Circle perimeter\\n    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\\n\\n# Run the test cases\\ntry:\\n    test_calculate_area()\\n    test_calculate_perimeter()\\n    test_circle()\\n    print('All test cases passed!')\\nexcept AssertionError as e:\\n    print(f'Test case failed: {e}')\", 'bug': 'The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.\\n4. The `numpy` library is imported but not used in the code.', 'errors': \"Execution Error : '(' was never closed (<string>, line 8)\"}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Debugger------\n",
      "The code after debug :  import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "# Code has been corrected to handle the bug\n",
      "{'code': \"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n\\n# Code has been corrected to handle the bug\", 'errors': None}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Executer------\n",
      "Testing code : \n",
      "import math\n",
      "import numpy\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "# Test Cases\n",
      "\n",
      "def test_calculate_area():\n",
      "    # Test case 1: Negative radius\n",
      "    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Positive radius\n",
      "    assert calculate_area(5) == math.pi * (5 ** 2), 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_calculate_perimeter():\n",
      "    # Test case 1: Invalid triangle sides\n",
      "    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Valid triangle sides\n",
      "    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\n",
      "\n",
      "\n",
      "def test_circle():\n",
      "    # Test case 1: Area calculation\n",
      "    circle = Circle(5)\n",
      "    assert circle.area() == math.pi * (5 ** 2), 'Test case 1 failed'\n",
      "    \n",
      "    # Test case 2: Perimeter calculation\n",
      "    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\n",
      "\n",
      "# Run the test cases\n",
      "try:\n",
      "    test_calculate_area()\n",
      "    test_calculate_perimeter()\n",
      "    test_circle()\n",
      "    print('All test cases passed!')\n",
      "except AssertionError as e:\n",
      "    print(f'Test case failed: {e}')\n",
      "All test cases passed!\n",
      "Code Execution Successful\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Decide to Developer------\n",
      "There is no error, the code will be refactored\n",
      "{'testing_code': \"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n\\n# Test Cases\\n\\ndef test_calculate_area():\\n    # Test case 1: Negative radius\\n    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\\n    \\n    # Test case 2: Positive radius\\n    assert calculate_area(5) == math.pi * (5 ** 2), 'Test case 2 failed'\\n\\n\\ndef test_calculate_perimeter():\\n    # Test case 1: Invalid triangle sides\\n    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\\n    \\n    # Test case 2: Valid triangle sides\\n    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\\n\\n\\ndef test_circle():\\n    # Test case 1: Area calculation\\n    circle = Circle(5)\\n    assert circle.area() == math.pi * (5 ** 2), 'Test case 1 failed'\\n    \\n    # Test case 2: Perimeter calculation\\n    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\\n\\n# Run the test cases\\ntry:\\n    test_calculate_area()\\n    test_calculate_perimeter()\\n    test_circle()\\n    print('All test cases passed!')\\nexcept AssertionError as e:\\n    print(f'Test case failed: {e}')\", 'bug': None, 'errors': None}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Developer------\n",
      "The refactored code : \n",
      "import math\n",
      "\n",
      "\n",
      "def calculate_area(radius):\n",
      "    '''\n",
      "    Calculate the area of a circle given the radius.\n",
      "    Args:\n",
      "        radius (float): The radius of the circle.\n",
      "    Returns:\n",
      "        float: The area of the circle.\n",
      "    '''\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    '''\n",
      "    Calculate the perimeter of a triangle given its sides.\n",
      "    Args:\n",
      "        side1 (float): Length of side 1.\n",
      "        side2 (float): Length of side 2.\n",
      "        side3 (float): Length of side 3.\n",
      "    Returns:\n",
      "        float: The perimeter of the triangle.\n",
      "    '''\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        '''\n",
      "        Calculate the area of the circle using the calculate_area function.\n",
      "        Returns:\n",
      "            float: The area of the circle.\n",
      "        '''\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        '''\n",
      "        Calculate the perimeter of the circle.\n",
      "        Returns:\n",
      "            float: The perimeter of the circle.\n",
      "        '''\n",
      "        return 2 * math.pi * self.radius\n",
      "\n",
      "All test cases passed!\n",
      "Developer Code Execution Successful\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "------Entering in Decide to End------\n",
      "----The code can be refactored----\n",
      "{'refactored_code': \"import math\\n\\n\\ndef calculate_area(radius):\\n    '''\\n    Calculate the area of a circle given the radius.\\n    Args:\\n        radius (float): The radius of the circle.\\n    Returns:\\n        float: The area of the circle.\\n    '''\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    '''\\n    Calculate the perimeter of a triangle given its sides.\\n    Args:\\n        side1 (float): Length of side 1.\\n        side2 (float): Length of side 2.\\n        side3 (float): Length of side 3.\\n    Returns:\\n        float: The perimeter of the triangle.\\n    '''\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        '''\\n        Calculate the area of the circle using the calculate_area function.\\n        Returns:\\n            float: The area of the circle.\\n        '''\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        '''\\n        Calculate the perimeter of the circle.\\n        Returns:\\n            float: The perimeter of the circle.\\n        '''\\n        return 2 * math.pi * self.radius\\n\", 'errors': None}\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"recursion_limit\": 15}\n",
    "inputs = {\"requirement\": requirement,\"code\" : code}\n",
    "running_dict = {}\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        running_dict[k] = v\n",
    "        if k != \"__end__\":\n",
    "            print(v)\n",
    "            print('----------'*20+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"refactored_code\": \"import math\\n\\n\\ndef calculate_area(radius):\\n    '''\\n    Calculate the area of a circle given the radius.\\n    Args:\\n        radius (float): The radius of the circle.\\n    Returns:\\n        float: The area of the circle.\\n    '''\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    '''\\n    Calculate the perimeter of a triangle given its sides.\\n    Args:\\n        side1 (float): Length of side 1.\\n        side2 (float): Length of side 2.\\n        side3 (float): Length of side 3.\\n    Returns:\\n        float: The perimeter of the triangle.\\n    '''\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        '''\\n        Calculate the area of the circle using the calculate_area function.\\n        Returns:\\n            float: The area of the circle.\\n        '''\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        '''\\n        Calculate the perimeter of the circle.\\n        Returns:\\n            float: The perimeter of the circle.\\n        '''\\n        return 2 * math.pi * self.radius\\n\",\n",
      "    \"errors\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "json_data = json.dumps(v, indent=4)\n",
    "print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "\n",
      "\n",
      "def calculate_area(radius):\n",
      "    '''\n",
      "    Calculate the area of a circle given the radius.\n",
      "    Args:\n",
      "        radius (float): The radius of the circle.\n",
      "    Returns:\n",
      "        float: The area of the circle.\n",
      "    '''\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    '''\n",
      "    Calculate the perimeter of a triangle given its sides.\n",
      "    Args:\n",
      "        side1 (float): Length of side 1.\n",
      "        side2 (float): Length of side 2.\n",
      "        side3 (float): Length of side 3.\n",
      "    Returns:\n",
      "        float: The perimeter of the triangle.\n",
      "    '''\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        '''\n",
      "        Calculate the area of the circle using the calculate_area function.\n",
      "        Returns:\n",
      "            float: The area of the circle.\n",
      "        '''\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        '''\n",
      "        Calculate the perimeter of the circle.\n",
      "        Returns:\n",
      "            float: The perimeter of the circle.\n",
      "        '''\n",
      "        return 2 * math.pi * self.radius\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(v[\"refactored_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upload_code': {'code': 'import math\\nimport numpy  \\nfrom random import randint, choice  \\n\\ndef calculate_area(radius):\\n    if radius < 0: \\n        return \"Invalid radius\"\\n    area = math.pi * (radius ** 2\\n    return area \\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return \"Invalid triangle sides\"\\n    perimeter = side1 + side2 + side3\\n    return perimetr  \\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radious  \\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n'},\n",
       " 'localizer': {'bug': 'The code provided has the following issues:\\n1. In the `calculate_area` function, there is a missing closing parenthesis in the calculation of the area.\\n2. In the `calculate_perimeter` function, there is a typo in the return statement where `perimeter` is misspelled as `perimetr`.\\n3. In the `Circle` class, there are typos in the `__init__` method where `radius` is misspelled as `radious` and in the `area` method where `calculate_area` is not correctly called.\\n4. The `numpy` library is imported but not used in the code.'},\n",
       " 'executer': {'testing_code': \"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n\\n# Test Cases\\n\\ndef test_calculate_area():\\n    # Test case 1: Negative radius\\n    assert calculate_area(-5) == 'Invalid radius', 'Test case 1 failed'\\n    \\n    # Test case 2: Positive radius\\n    assert calculate_area(5) == math.pi * (5 ** 2), 'Test case 2 failed'\\n\\n\\ndef test_calculate_perimeter():\\n    # Test case 1: Invalid triangle sides\\n    assert calculate_perimeter(1, 2, 5) == 'Invalid triangle sides', 'Test case 1 failed'\\n    \\n    # Test case 2: Valid triangle sides\\n    assert calculate_perimeter(3, 4, 5) == 12, 'Test case 2 failed'\\n\\n\\ndef test_circle():\\n    # Test case 1: Area calculation\\n    circle = Circle(5)\\n    assert circle.area() == math.pi * (5 ** 2), 'Test case 1 failed'\\n    \\n    # Test case 2: Perimeter calculation\\n    assert circle.perimeter() == 2 * math.pi * 5, 'Test case 2 failed'\\n\\n# Run the test cases\\ntry:\\n    test_calculate_area()\\n    test_calculate_perimeter()\\n    test_circle()\\n    print('All test cases passed!')\\nexcept AssertionError as e:\\n    print(f'Test case failed: {e}')\",\n",
       "  'bug': None,\n",
       "  'errors': None},\n",
       " 'debugger': {'code': \"import math\\nimport numpy\\nfrom random import randint, choice\\n\\ndef calculate_area(radius):\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        return 2 * math.pi * self.radius\\n\\n# Code has been corrected to handle the bug\",\n",
       "  'errors': None},\n",
       " 'developer': {'refactored_code': \"import math\\n\\n\\ndef calculate_area(radius):\\n    '''\\n    Calculate the area of a circle given the radius.\\n    Args:\\n        radius (float): The radius of the circle.\\n    Returns:\\n        float: The area of the circle.\\n    '''\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    '''\\n    Calculate the perimeter of a triangle given its sides.\\n    Args:\\n        side1 (float): Length of side 1.\\n        side2 (float): Length of side 2.\\n        side3 (float): Length of side 3.\\n    Returns:\\n        float: The perimeter of the triangle.\\n    '''\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        '''\\n        Calculate the area of the circle using the calculate_area function.\\n        Returns:\\n            float: The area of the circle.\\n        '''\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        '''\\n        Calculate the perimeter of the circle.\\n        Returns:\\n            float: The perimeter of the circle.\\n        '''\\n        return 2 * math.pi * self.radius\\n\",\n",
       "  'errors': None}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import math\\n\\n\\ndef calculate_area(radius):\\n    '''\\n    Calculate the area of a circle given the radius.\\n    Args:\\n        radius (float): The radius of the circle.\\n    Returns:\\n        float: The area of the circle.\\n    '''\\n    if radius < 0:\\n        return 'Invalid radius'\\n    area = math.pi * (radius ** 2)\\n    return area\\n\\n\\ndef calculate_perimeter(side1, side2, side3):\\n    '''\\n    Calculate the perimeter of a triangle given its sides.\\n    Args:\\n        side1 (float): Length of side 1.\\n        side2 (float): Length of side 2.\\n        side3 (float): Length of side 3.\\n    Returns:\\n        float: The perimeter of the triangle.\\n    '''\\n    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\\n        return 'Invalid triangle sides'\\n    perimeter = side1 + side2 + side3\\n    return perimeter\\n\\n\\nclass Circle:\\n    def __init__(self, radius):\\n        self.radius = radius\\n\\n    def area(self):\\n        '''\\n        Calculate the area of the circle using the calculate_area function.\\n        Returns:\\n            float: The area of the circle.\\n        '''\\n        return calculate_area(self.radius)\\n\\n    def perimeter(self):\\n        '''\\n        Calculate the perimeter of the circle.\\n        Returns:\\n            float: The perimeter of the circle.\\n        '''\\n        return 2 * math.pi * self.radius\\n\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(running_dict.items())[-1][1]['refactored_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "from random import randint, choice\n",
      "\n",
      "def calculate_area(radius):\n",
      "    if radius < 0:\n",
      "        return 'Invalid radius'\n",
      "    area = math.pi * (radius ** 2)\n",
      "    return area\n",
      "\n",
      "def calculate_perimeter(side1, side2, side3):\n",
      "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
      "        return 'Invalid triangle sides'\n",
      "    perimeter = side1 + side2 + side3\n",
      "    return perimeter\n",
      "\n",
      "class Circle:\n",
      "    def __init__(self, radius):\n",
      "        self.radius = radius\n",
      "\n",
      "    def area(self):\n",
      "        return calculate_area(self.radius)\n",
      "\n",
      "    def perimeter(self):\n",
      "        return 2 * math.pi * self.radius\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(running_dict[\"debugger\"][\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases for calculate_area passed successfully!\n",
      "All test cases for calculate_perimeter passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "from random import randint, choice\n",
    "\n",
    "def calculate_area(radius):\n",
    "    if radius < 0:\n",
    "        return 'Invalid radius'\n",
    "    area = math.pi * (radius ** 2)\n",
    "    return area\n",
    "\n",
    "def calculate_perimeter(side1, side2, side3):\n",
    "    if side1 + side2 <= side3 or side1 + side3 <= side2 or side2 + side3 <= side1:\n",
    "        return 'Invalid triangle sides'\n",
    "    perimeter = side1 + side2 + side3\n",
    "    return perimeter\n",
    "\n",
    "class Circle:\n",
    "    def __init__(self, radius):\n",
    "        self.radius = radius\n",
    "\n",
    "    def area(self):\n",
    "        return calculate_area(self.radius)\n",
    "\n",
    "    def perimeter(self):\n",
    "        return 2 * math.pi * self.radius\n",
    "\n",
    "def test_calculate_area():\n",
    "    assert calculate_area(5) == 78.53981633974483, 'Test Case 1 Failed'\n",
    "    assert calculate_area(0) == 0.0, 'Test Case 2 Failed'\n",
    "    assert calculate_area(-5) == 'Invalid radius', 'Test Case 3 Failed'\n",
    "    print('All test cases for calculate_area passed successfully!')\n",
    "\n",
    "def test_calculate_perimeter():\n",
    "    assert calculate_perimeter(3, 4, 5) == 12, 'Test Case 1 Failed'\n",
    "    assert calculate_perimeter(2, 2, 5) == 'Invalid triangle sides', 'Test Case 2 Failed'\n",
    "    assert calculate_perimeter(1, 1, 1) == 3, 'Test Case 3 Failed'\n",
    "    print('All test cases for calculate_perimeter passed successfully!')\n",
    "    \n",
    "test_calculate_area()\n",
    "test_calculate_perimeter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
