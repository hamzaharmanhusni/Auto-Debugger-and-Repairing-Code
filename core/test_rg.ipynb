{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain_community.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = r\"D:\\project-ruang-guru\\Debugging\\Code\\Source-Code-Analysis-using-GenAI\\research\\test_repo\\rg-llm\\llm\\api\\error_handlers.py\"\n",
    "\n",
    "with open(file_path, 'r') as rp:\n",
    "    code = rp.read()\n",
    "    \n",
    "code = \"Sources : \" + file_path +\"\\n\\n\" +\"**Code **: \\n\"+ code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources : D:\\project-ruang-guru\\Debugging\\Code\\Source-Code-Analysis-using-GenAI\\research\\test_repo\\rg-llm\\llm\\api\\error_handlers.py\n",
      "\n",
      "**Code **: \n",
      "from fastapi import Request\n",
      "from fastapi.applications import FastAPI\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError, AIException\n",
      "from llm.api.views import error_response\n",
      "from typing import Union\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "async def request_validation_handler(_: Request, error: RequestValidationError):\n",
      "    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\n",
      "                         for e in error.errors()]\n",
      "    return error_response(error, 400, validation_errors)\n",
      "\n",
      "\n",
      "async def http_exception_handler(_, error: StarletteHTTPException):\n",
      "    return error_response(error)\n",
      "\n",
      "\n",
      "async def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\n",
      "    logger.error(f\"Not Found for {error.field}: {error.value}\")\n",
      "    return error_response(error, 400)\n",
      "\n",
      "\n",
      "async def ai_exception_handler(_, error):\n",
      "    logger.error(f\"LLM error with detail: {error}\")\n",
      "    return error_response(error, 500)\n",
      "\n",
      "\n",
      "def register_error_handlers(app: FastAPI) -> FastAPI:\n",
      "    app.exception_handler(RequestValidationError)(request_validation_handler)\n",
      "    app.exception_handler(StarletteHTTPException)(http_exception_handler)\n",
      "    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\n",
      "    app.exception_handler(AIException)(ai_exception_handler)\n",
      "    return app\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langgraph Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Localizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\anaconda3\\envs\\CodeDebugger\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. You can read more about the method here: <https://python.langchain.com/docs/modules/model_io/chat/structured_output/>.Please follow our extraction use case documentation for more guidelines on how to do information extraction with LLMs. <https://python.langchain.com/docs/use_cases/extraction/>. If you notice other issues, please provide feedback here: <https://github.com/langchain-ai/langchain/discussions/18154>\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "class Localizer(BaseModel):\n",
    "    predicted_bug: str = Field(\n",
    "        description=\"A description of the predicted bug in the code.\"\n",
    "    )\n",
    "    \n",
    "def localizer_invoke(requirement, code):    \n",
    "    localization_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    **Role**: As a localizer, your task is to review the following code snippet and identify any potential bugs. Ignore any issues related to missing dependencies or packages. Focus on syntax errors, logical errors, and potential runtime issues within the given code. Provide a detailed explanation of each identified bug and suggest possible fixes. If no issues are found, simply state that the code is bug-free. If there are issues, explain them clearly.\n",
    "\n",
    "    **Requirement**:\n",
    "    {requirement}\n",
    "\n",
    "    **Provided resources**:\n",
    "    {code}\n",
    "\n",
    "    The first row is counted after **Code**\n",
    "    \n",
    "    So it is like : \n",
    "    Source : path/to/file\n",
    "    \n",
    "    **Code** : \n",
    "    Row 1\n",
    "    Row 2\n",
    "    Row 3 \n",
    "    And so on\n",
    "    \n",
    "    Your output must be in the following format:\n",
    "\n",
    "    \n",
    "    ```\n",
    "    Predicted error code 1 -- source : path/to/file -- Row -- Explanation of the error and why it occurs\n",
    "    Predicted error code 2 -- source : path/to/file -- Row -- Explanation of the error and why it occurs\n",
    "    ```\n",
    "\n",
    "    Please review the code accordingly.\n",
    "    '''    \n",
    "    )\n",
    "    localizer_agent = create_structured_output_runnable(\n",
    "        Localizer, llm, localization_gen_prompt\n",
    "    ) \n",
    "    with get_openai_callback() as localizer_info:\n",
    "        localizer_invoke = localizer_agent.invoke({'code':code,'requirement':requirement})\n",
    "    return localizer_invoke, localizer_info\n",
    "requirement = \"Identify the possible bugs\"\n",
    "code = code\n",
    "localizer_invoke, localizer_info = localizer_invoke(requirement, code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localizer Invoke : \n",
      "\n",
      "The code is bug-free.\n",
      "\n",
      "Localizer Info:\n",
      "\n",
      "Tokens Used: 662\n",
      "\tPrompt Tokens: 649\n",
      "\tCompletion Tokens: 13\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.000344\n"
     ]
    }
   ],
   "source": [
    "print(f\"Localizer Invoke : \\n\\n{localizer_invoke.predicted_bug}\\n\\nLocalizer Info:\\n\\n{localizer_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Localizer(predicted_bug='The code is bug-free.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(localizer_invoke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explanation(BaseModel):\n",
    "    function_explanation: str = Field(\n",
    "        description=\"The explanation of the code.\"\n",
    "    )\n",
    "\n",
    "def explanation_invoke(code):\n",
    "    expalantion_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    **Role** : As an identifier, you will provide an explanation of the code given. Then, You should specify the workflow or processing steps within the code. \n",
    "    \n",
    "    **Given code** : \n",
    "    {code}\n",
    "    \n",
    "    Analyze the code to identify the function and process on the code.\n",
    "    \n",
    "    Follow this structured format:\n",
    "\n",
    "    1. **Class 1**: Provide a summarized overview of Class 1.\n",
    "        - a) **Method or Async Method 1 of Class 1**: Describe the function and purpose of Method 1.\n",
    "        - b) **Method Async Method 2 of Class 1**: Describe the function and purpose of Method 2.\n",
    "        - Continue this pattern for all methods in Class 1.\n",
    "    2. **Class 2**: Provide a summarized overview of Class 2.\n",
    "        - a) **Method or Async Method 1 of Class 2**: Describe the function and purpose of Method 1.\n",
    "        - b) **Method or Async Method 2 of Class 2**: Describe the function and purpose of Method 2.\n",
    "        - Continue this pattern for all methods in Class 2.\n",
    "        \n",
    "    - Continue this pattern for all remaining classes.\n",
    "\n",
    "    After detailing the functions of each class and method, provide an overall explanation of the code's workflow. \n",
    "\n",
    "    Explain the process of the given code, predicting the overall function. If the code involves any dependencies or packages that are not explicitly stated, please predict their function as well.\n",
    "    \"\"\"    \n",
    "    )\n",
    "    \n",
    "    explanation_agent = create_structured_output_runnable(\n",
    "        Explanation, llm, expalantion_gen_prompt\n",
    "    )\n",
    "    with get_openai_callback() as expalanation_info:\n",
    "        explanation_invoke = explanation_agent.invoke({'code':code})\n",
    "    return explanation_invoke, expalanation_info\n",
    "code = code\n",
    "explanation_invoke, explanation_info = explanation_invoke(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation Invoke : \n",
      "\n",
      "Sure! Let's analyze the provided code to identify the functions and processes within it.\n",
      "\n",
      "Identifier Info:\n",
      "\n",
      "Tokens Used: 737\n",
      "\tPrompt Tokens: 712\n",
      "\tCompletion Tokens: 25\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00039349999999999997\n"
     ]
    }
   ],
   "source": [
    "print(f\"Explanation Invoke : \\n\\n{explanation_invoke.function_explanation}\\n\\nIdentifier Info:\\n\\n{explanation_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Repairer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Repairer(BaseModel):\n",
    "    repair_description: str = Field(\n",
    "        description=\"A description of the code that needs to be repaired.\"\n",
    "    )\n",
    "\n",
    "def repairer_invoke(code, explanation, bug):        \n",
    "    repairer_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    **Role** : As a code repairer, your task is to provide the corrected version based on any predicted bugs and the explanation of the code. \n",
    "\n",
    "    You will be given the predicted bug(s). \n",
    "    \n",
    "    **Code** : \n",
    "    {code}\n",
    "    \n",
    "    **The explanation of the code**\n",
    "    {explanation}\n",
    "    \n",
    "    **Predicted bug** \n",
    "    {bug}\n",
    "\n",
    "    For each bug, follow this format:\n",
    "    \n",
    "    Error 1:\n",
    "    the code with error 1 -- specify the row(s)  \n",
    "    \n",
    "    After repair 1:\n",
    "    `the code after repairing error 1`  \n",
    "    Explanation: Explain what you have repaired.\n",
    "    \\n\\n\n",
    "    Error 2:\n",
    "    the code with error 2 -- specify the row(s)  \n",
    "    After repair 2:\n",
    "    `the code after repairing error 2`  \n",
    "    Explanation: Explain what you have repaired.\n",
    "    \\n\\n\n",
    "    Continue this pattern for any additional errors.\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "    repairer_agent = create_structured_output_runnable(\n",
    "        Repairer, llm, repairer_gen_prompt\n",
    "    )  \n",
    "    with get_openai_callback() as repairer_info:\n",
    "        repairer_invoke= repairer_agent.invoke({\"code\":code, \"explanation\":explanation, \"bug\":bug})\n",
    "    return repairer_invoke, repairer_info\n",
    "\n",
    "code = code\n",
    "explanation = explanation_invoke.function_explanation\n",
    "bug = localizer_invoke.predicted_bug\n",
    "\n",
    "repairer_invoke, repairer_info = repairer_invoke(code, explanation, bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repairer Invoke : \n",
      "\n",
      "The code is bug-free.\n",
      "\n",
      "Repairer Info:\n",
      "\n",
      "Tokens Used: 634\n",
      "\tPrompt Tokens: 621\n",
      "\tCompletion Tokens: 13\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00033\n"
     ]
    }
   ],
   "source": [
    "print(f\"Repairer Invoke : \\n\\n{repairer_invoke.repair_description}\\n\\nRepairer Info:\\n\\n{repairer_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Crafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crafter(BaseModel):\n",
    "    code: str = Field(\n",
    "        description=\"The code after reviewed\"\n",
    "    )\n",
    "    \n",
    "def crafter_invoke(code, repaired_description):\n",
    "    crafter_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    **Role**: As a crafter, your task is to merge the corrected code into the original code if it should be corrected.\n",
    "    **The given code:**\n",
    "    {code}\n",
    "    \n",
    "    The first row is counted after **Code**\n",
    "    \n",
    "    So it is like : \n",
    "    Source : path/to/file\n",
    "    \n",
    "    **Code** : \n",
    "    Row 1\n",
    "    Row 2\n",
    "    Row 3 \n",
    "    And so on\n",
    "\n",
    "    **The description of the code that should be repaired:**\n",
    "    {repaired_description}\n",
    "    \"\"\"\n",
    "    )\n",
    "    crafter_agent = create_structured_output_runnable(\n",
    "        Crafter, llm, crafter_gen_prompt\n",
    "    )\n",
    "    with get_openai_callback() as crafter_info:\n",
    "        crafter_invoke=crafter_agent.invoke({'code':code,'repaired_description':repaired_description})   \n",
    "    \n",
    "    return crafter_invoke,crafter_info\n",
    "\n",
    "code = code\n",
    "repaired_description = repairer_invoke.repair_description\n",
    "crafter_invoke,crafter_info = crafter_invoke(code, repaired_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crafter Invoke : \n",
      "\n",
      "from fastapi import Request\n",
      "from fastapi.applications import FastAPI\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError, AIException\n",
      "from llm.api.views import error_response\n",
      "from typing import Union\n",
      "\n",
      "import logging\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "async def request_validation_handler(_: Request, error: RequestValidationError):\n",
      "    validation_errors = [(\".\".join(str(x) for x in e[\"loc\"]), e[\"type\"])\n",
      "                         for e in error.errors()]\n",
      "    return error_response(error, 400, validation_errors)\n",
      "\n",
      "\n",
      "async def http_exception_handler(_, error: StarletteHTTPException):\n",
      "    return error_response(error)\n",
      "\n",
      "\n",
      "async def platform_model_exception_handler(request: Request, error: PlatformModelNotFoundError):\n",
      "    logger.error(f\"Not Found for {error.field}: {error.value}\")\n",
      "    return error_response(error, 400)\n",
      "\n",
      "\n",
      "async def ai_exception_handler(_, error):\n",
      "    logger.error(f\"LLM error with detail: {error}\")\n",
      "    return error_response(error, 500)\n",
      "\n",
      "\n",
      "def register_error_handlers(app: FastAPI) -> FastAPI:\n",
      "    app.exception_handler(RequestValidationError)(request_validation_handler)\n",
      "    app.exception_handler(StarletteHTTPException)(http_exception_handler)\n",
      "    app.exception_handler(PlatformModelNotFoundError)(platform_model_exception_handler)\n",
      "    app.exception_handler(AIException)(ai_exception_handler)\n",
      "    return app\n",
      "\n",
      "\n",
      "Crafter Info:\n",
      "\n",
      "Tokens Used: 853\n",
      "\tPrompt Tokens: 518\n",
      "\tCompletion Tokens: 335\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0007615\n"
     ]
    }
   ],
   "source": [
    "print(f\"Crafter Invoke : \\n\\n{crafter_invoke.code}\\n\\nCrafter Info:\\n\\n{crafter_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Developer(BaseModel):\n",
    "    refactored_code: str = Field(\n",
    "        description=\"The refactored and optimized Python code.\"\n",
    "    )\n",
    "\n",
    "def developer_invoke(repaired_code : str, lang:str):\n",
    "    developer_gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Your task as an expert developer,  you are given the code that needs to be refactored and optimized for better performance, readability, and maintainability. Below are the details and the code snippet. Please refactor the code, provide comments where necessary, and suggest any improvements or best practices that can be applied.\n",
    "\n",
    "    Details:\n",
    "\n",
    "    - The code should follow {lang} best practices.\n",
    "    - Ignoring the missing depedencies or packages\n",
    "    - Aim for improved performance without sacrificing readability.\n",
    "    - Ensure that the code is modular and easy to maintain.\n",
    "    - Add comments to explain complex sections of the code.\n",
    "    - Optimize any redundant or inefficient parts of the code.\n",
    "    - Ensure proper error handling and input validation.\n",
    "    - Follow naming conventions for variables and functions.\n",
    "\n",
    "    Code Snippet:\n",
    "    {code}\n",
    "\n",
    "    Please provide the refactored and optimized version of the code along with any comment of explanations or suggestions for improvements.\"\"\"\n",
    "    )\n",
    "    developer_agent = create_structured_output_runnable(\n",
    "        Developer, llm, developer_gen_prompt\n",
    "    )\n",
    "    with get_openai_callback() as developer_info:\n",
    "        developer_invoke=developer_agent.invoke({\"lang\":lang,\"code\":repaired_code})\n",
    "    return developer_invoke,developer_info\n",
    "\n",
    "repaired_code = crafter_invoke.improved_code\n",
    "lang = \"Python\"\n",
    "developer_invoke,developer_info = developer_invoke(repaired_code, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developer Invoke : \n",
      "\n",
      "from fastapi import Request, FastAPI\n",
      "from fastapi.exceptions import RequestValidationError\n",
      "from starlette.exceptions import HTTPException as StarletteHTTPException\n",
      "from llm.core.service.exceptions import PlatformModelNotFoundError, AIException\n",
      "from llm.api.views import error_response\n",
      "from typing import Union\n",
      "import logging\n",
      "\n",
      "# Initialize logger\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "# Handler for request validation errors\n",
      "async def request_validation_handler(_: Request, error: RequestValidationError):\n",
      "    validation_errors = [\n",
      "        (\".\".join(str(x) for x in e.get(\"loc\", [])), e.get(\"type\", \"unknown\"))\n",
      "        for e in error.errors()\n",
      "    ]\n",
      "    return error_response(error, 400, validation_errors)\n",
      "\n",
      "# Handler for HTTP exceptions\n",
      "async def http_exception_handler(_: Request, error: StarletteHTTPException):\n",
      "    return error_response(error, 500)\n",
      "\n",
      "# Handler for platform model not found errors\n",
      "async def platform_model_exception_handler(_: Request, error: PlatformModelNotFoundError):\n",
      "    logger.error(f\"Not Found for {getattr(error, 'field', 'unknown')}: {getattr(error, 'value', 'unknown')}\")\n",
      "    return error_response(error, 400)\n",
      "\n",
      "# Handler for AI exceptions\n",
      "async def ai_exception_handler(_: Request, error: AIException):\n",
      "    logger.error(f\"LLM error with detail: {error}\")\n",
      "    return error_response(error, 500)\n",
      "\n",
      "# Function to register all error handlers\n",
      "def register_error_handlers(app: FastAPI) -> FastAPI:\n",
      "    app.add_exception_handler(RequestValidationError, request_validation_handler)\n",
      "    app.add_exception_handler(StarletteHTTPException, http_exception_handler)\n",
      "    app.add_exception_handler(PlatformModelNotFoundError, platform_model_exception_handler)\n",
      "    app.add_exception_handler(AIException, ai_exception_handler)\n",
      "    return app\n",
      "\n",
      "\n",
      "Crafter Info:\n",
      "\n",
      "Tokens Used: 1011\n",
      "\tPrompt Tokens: 591\n",
      "\tCompletion Tokens: 420\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.009255\n"
     ]
    }
   ],
   "source": [
    "print(f\"Developer Invoke : \\n\\n{developer_invoke.refactored_code}\\n\\Developer Info:\\n\\n{developer_info}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CodeDebugger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
